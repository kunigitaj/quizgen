
Developing Data Models with SAP HANA Cloud

...

Unit 1
Preparing the Modeling Environment
After completing this unit, you will be able to:

Describe SAP Business Application Studio and how it is used for development in SAP HANA Cloud.
Create a Development Space for your modeling content in SAP Business Application Studio.
Create a modeling project in SAP Business Application Studio.
Import a Project into your Workspace of SAP Business Application Studio.
Describe the layout and features of SAP Business Application Studio that are relevant to modeling.
Getting Started with SAP Business Application Studio
Working with Development Spaces
Creating a New Project in SAP Business Application Studio
Importing an Existing Project in SAP Business Application Studio
Navigating SAP Business Application Studio

...

Getting Started with SAP Business Application Studio
Objective

After completing this lesson, you will be able to describe SAP Business Application Studio and how it is used for development in SAP HANA Cloud.
SAP Business Application Studio
Before we get started developing data models in SAP HANA Cloud, we should introduce the key tool we will use, SAP Business Application Studio.

SAP Business Application Studio is a very powerful, web-based Integrated Development Environment (IDE) that is used to develop all components of a modern, full-stack application including the interface, the application and the database layer.

SAP Business Application Studio supports development of cloud, on-premise and hybrid applications that integrate SAP and non-SAP technologies.

SAP Business Application Studio is probably the only development tool that you will ever need as it includes a huge number of features to support many development scenarios. One of those scenarios is data modeling in SAP HANA Cloud. This is the scenario we focus on in this learning journey.

A screenshot of SAP Business Application Studio, showing the Action bar, the Explorer Pane, and the Welcome tab with the some of the key startup actions.
SAP Business Application Studio is based on Visual Studio Code, which is an open-source software project started by Microsoft. It is widely adopted by the development community and is popular choice for developers because it allows them to plug-in their favorite extensions that provide additional tooling for their development projects. SAP and third parties provide many plug-ins for different types of SAP development projects to increase developer productivity.

SAP Business Application Studio provides text and graphical editors to create development artifacts, plus a command line interface (CLI).

A key feature of SAP Business Application Studio is the native Git support for source file version management. Git controls are embedded into the Business Application Studio.

Note

Web IDE for SAP HANA can also be used for modeling in SAP HANA Cloud, but is not recommended as it misses many additional productivity aids and features that supports the modeler. Web IDE will not be developed further and Business Application Studio will be the tool that receives all new features. SAP Business Application Studio is the recommended tool for data modeling in SAP HANA Cloud.

Getting Access to SAP Business Application Studio
SAP Business Application Studio is provided as a web-based application, with the back-end running on SAP Business Technology Platform (BTP). You subscribe to SAP Business Application Studio service at the BTP sub-account level. You do this in the BTP Cockpit.

Before you can launch SAP Business Application Studio from your browser, the following steps are required. Some relate to the BTP account or sub-account, others are specific to the user that will use SAP Business Application Studio.

A table showing the required steps in SAP BTP before you can work in SAP Business Application Studio. The steps are separated beetween BTP Account and Subaccount step, and User Membership and authorization setup.
When these steps are complete, you are ready to launch SAP Business Application Studio.

Watch this video to learn about steps to launch SAP Business Application Studio.

...

Working with Development Spaces
Objective

After completing this lesson, you will be able to create a Development Space for your modeling content in SAP Business Application Studio.
Development Spaces in SAP Business Application Studio
In SAP Business Application Studio, each developer has their own environment. This environment is made up of one or more Development Spaces.

Be careful not to confuse development spaces with Cloud Foundry spaces.
When you create a development space, you will be asked to choose tools, known as extensions, that you require. For modeling in SAP HANA Cloud, it is essential that you include the SAP HANA Calculation View Editor extension in your development space. This is a key tool that you will use.

Watch the video, Configuring a Development Space, following this figure to learn about configuring a Development Space and choosing extensions.

Managing Development Spaces
The landing page of SAP Business Application Studio contains the list of Development Spaces. You may have access to more than one. This list Includes the following:

The name of the Development Space
The kind of application
The status: STOPPED, STARTING, RUNNING, and so forth
Creation date
ID
A Development Space must be running before you can work in it. You start it up from the landing page. When it is running, the name of the Development Space "becomes" a hyperlink that you use to enter your Development Space.

Caution

Any Development Space automatically stops, after some idle time, to preserve resources. Therefore, be sure to always check the status of a development space in the landing page before connecting to that space. This is especially true if you bookmark the URL of a development space, which includes the space ID.
Creating Several Development Spaces
A diagram showing three different development spaces of different kinds created by a user. Two of them are running, the other one is stopped.
Depending on the SAP BTP subscription, there might be some limitations to the maximum number of spaces that you can create, and the maximum number of spaces that can be running in parallel.

Note

In an SAP BTP Trial account, you can define up to two Development Spaces and only one of these can be running at one time. If you need to swap development spaces, you must first stop the one you are using.
Maintenance of Development Spaces
When a Development Space is created, its name and kind of application cannot be changed. However, you can add optional extensions to the development space. Before you can do that, the Development Space must be stopped.

Caution

To preserve the existing projects of your development space, it is not possible to remove an extension that is already assigned to it.
It is also possible to export the entire content of a Development Space as a compressed .tar.gz archive. Before you can do that, the Development Space must be running.

At the end of the Development Space export, you are given detailed information on how to import the Development Space content into another one. Generally, the target Development Space should be an empty one.

Create a Development Space
Learn how to create a development space.

Demo

...

Creating a New Project in SAP Business Application Studio
Objective

After completing this lesson, you will be able to create a modeling project in SAP Business Application Studio.
Projects in SAP Business Application Studio
The File Structure in the Repository of SAP Business Application Studio
In SAP Business Application Studio, projects are created in a folder structure that is specific to your user and your development space.

Watch this video to learn about file structure in the repository of SAP Business Application Studio.

When a project is opened in the Explorer view, you can navigate its content (sub-folders and files), open files with the editor, delete or move files, and so on.

This slide show the file structure of a project in SAP Business Application Studio.
The ribbon at the top of the Explorer view shows the name of the folder (capitalized). It is opened as a workspace, meaning that specific Business Application Studio settings can be associated to the folder.

Creating a Project from a Template
If you are starting from scratch and not working on an existing project, you use the Start from Template button on the Welcome tab. This wizard creates an near-empty project with only the base files provided to get you started.

Create a New Project from a Template
Demo

...

Importing an Existing Project in SAP Business Application Studio
Objective

After completing this lesson, you will be able to import a Project into your Workspace of SAP Business Application Studio.
Import a Project
Sharing Development Content using Git
Git is the preferred method for sharing development content in an SAP HANA Cloud project. Sharing means importing and exporting content with other developers.

Git is used to manage the source code in an application development environment. It provides tools for developers who need to collaborate on projects and work in parallel. Git provides sophisticated features to manage the source code, especially in large projects where multiple team members need to work together and share objects without overwriting each others' work.

SAP Business Application Studio includes Git allowing you to manage the lifecycle of your source code locally, but also to connect to a remote corporate/public Git repository to interact with other developers, share code, and so on.

Git operations can also be invoked from the terminal, which is suitable for experienced Git users, or from the Git view that gives you access to the base Git functionality in a more guided and visual approach.

Note

A dedicated lesson covers using Git in SAP Business Application Studio later in this course.
Importing Development Content without Git
Although Git is recommended, you could also use the simpler import features of SAP SAP Business Application Studio to bring content into your workspace. You can copy a complete project or just parts of a project to your existing workspace or start a new workspace. Be careful when you are using these tools as there is no tracking and version controls as you have with Git.

Import
Import Options	Description
Import (Welcome page)
Import project (right-click on blank area)

Imports exclusively .tar or .zip archives and also extracts them at the same time.
After importing a project, the new folder can be opened in a new workspace.

File → Upload Files	Uploads archives (.tar or .zip) or single files
This is useful when you need to add individual files into your project structure instead of an entire project. If you upload an archive file, you must then extract its contents with the command line (tar / unzip).

Drag and drop from your file explorer	With drag and drop, you can copy files, folders, or both, from your Windows File Explorer to any folder visible in the Explorer view of Business Application Studio.
Caution

When you are using File → Upload Files and drag and drop from your Windows File Explorer, the uploaded content might overwrite existing content (same file name and same target folder). A warning is shown for each conflicting file.
Exporting Development Content without Git
You can easily export development content using the simple download option of SAP Business Application Studio.

Export Options	Description
Download	Downloads files and/or folders from SAP Business Application Studio perhaps to pass to other colleagues. Multiple objects are archived as a single .tar file. Simply right-click on any file or folder in the Explorer view of Business Application Studio.
To download the entire root folder (perhaps to include multiple projects), right-click the blank area below the file structure.

Import a project
In this demonstration, you will learn how to import a project into SAP Business Application Studio from a .zip or .tar archive.

Demo


...

Navigating SAP Business Application Studio
Objective

After completing this lesson, you will be able to describe the layout and features of SAP Business Application Studio that are relevant to modeling.
Interface of SAP Business Application Studio
Watch this video to learn about SAP Business Application Studio Interface.

SAP Business Application Studio offers a lot of different ways to perform the same action: menu commands, keyboard shortcuts, quick commands, and so on. The different views on the left are there to cover your need as a developer. Explore the file structure to create/modify/move/delete files, search/replace within file content, connect your workspace to Git, Debug, and so on.

The list of available views depend on the extensions that are included by default (depending on the kind of application your development space was created for) or added as options.

Some of the views are added automatically as icons to the very left of the screen, others are not. However, you can open any existing view by choosing Choose → Open View.

Some views have their own left pane, while others appear as a sub-pane of another one. For example, the SAP HANA Projects and Java Projects views are presented in the same Explorer view.

To get a neat workplace, you can easily hide the icons of the views that you do not need. You can also hide any additional extension-related pane.

SAP Business Application Studio Editors
Watch this video to learn about the text editor and running commands.

This slide show how to use the Running Quick Command function.
The Quick Command feature is a powerful way to execute repetitive or one-time actions. The available commands come under different categories, such as Cloud Foundry, CDS, graphical modeling, and so on. Exactly like the views, the quick commands depend on the extensions installed in your development space.

This slide show the available status bar features.
The status bar of SAP Business Application Studio provides several types of information. Among these are notifications and information on problems (warnings or errors) in your code.

Another important piece of information if you work in Cloud Foundry is the Cloud Foundry status and, if applicable, the Cloud Foundry target. It also provides Git-related information, when a project is Git-enabled, such as the name of the checked-out branch.

Work with Basic Features of SAP Business Application Studio
In this demonstration, you will learn how to use the basic features of SAP Business Application Studio.

Demo

...

Knowledge quiz
It's time to put what you've learned to the test, get 4 right to pass this unit.

1.
When you need to collaborate with others on large projects, which is the best option for bringing development content into your workspace?
Choose the correct answer.

Upload from File

Import

Clone from Git
2.
Which option do you use if you want to create a brand new project?
Choose the correct answer.

Clone from Git

Import

Start from Template
3.
Which extension is essential to include in your development space in order to begin modeling?
Choose the correct answer.

CDS Graphical Editor

SAP HANA Calculation View Editor

Flowgraph Editor
4.
In SAP HANA Cloud, which tool is recommended for data modeling?
Choose the correct answer.

Database Explorer

SAP Business Application Studio

Web IDE
5.
What can you do in the Explorer view of SAP Business Application Studio?
There are two correct answers.

Create a project from a template

List files and folders

Manage SAP HANA artifacts deployment.

......

Unit 2
Creating Calculation Views
After completing this unit, you will be able to:

Understand modeling terminology of SAP HANA Cloud so that you are ready to begin modeling.
Create a dimension calculation view using the graphical calculation view editor.
Create a cube calculation view using the graphical calculation view editor.
Create a time-based dimension calculation view using the graphical calculation view editor.
Understand which data sources are supported by calculation views.
Check the output of a calculation view to ensure correct results are generated.
Describe features that are common to all types of calculation view.
Describe the function of the top view node.
Understanding Basic Concepts and Terminology
Creating Dimension Calculation Views
Creating Cube Calculation Views
Creating Time-Based Dimension Calculation Views
Choosing a Data Source for a Calculation View
Checking the Output of a Calculation View
Working with Common Features of Calculation Views
Top View Node

...

Understanding Basic Concepts and Terminology
Objective

After completing this lesson, you will be able to understand modeling terminology of SAP HANA Cloud so that you are ready to begin modeling.
Key Concepts of Data Modeling
Before introducing calculation views in SAP HANA Cloud, you should become familiar with some key modeling concepts and the terms that are frequently used.

Measure and Attribute
A data model usually contains a selection of measures and attributes.

Measure Versus Attribute
 	Measure	Attribute
What is it?	A numeric value, such as a price or quantity, on which you can process arithmetic or statistics operations, such as sum, average, top N values, and calculations.	A descriptive element such as 'country' that is used the provide context for a measure. For example 'Sales Revenue by France'.
Examples	
Quantity

Sales Revenue

Product

Customer

Country

Year

Attributes are used to filter or aggregate the measures, in order to answer questions such as the following:

What are the total sales originating from Hamburg?

What is the sales revenue for Cars in 2022?

Dimension
A dimension is a group of related attributes. In the example below, we see a dimension called Product. This dimension includes various related attributes including product name, category, supplier.

Dimensions are re-useable objects. When they have been created, they can are shared by developers and used to develop star schemas.

A dimension contains key, name, and attributes. Use different dimensions for different entities, like Product or Sales Org. Product has attributes Product Category and Supplier. Sales Org has attributes Country and Region.
Star Schema
A star schema is a very powerful type of data model that we can create in SAP HANA Cloud. A star schema invokes a powerful OLAP engine to provide fast slice-and-dice across data.

Watch this video to learn about the Star Schema.

Note

A fact table can be one table but it is often defined from a combination of tables to produce a view of a transaction, such as a sales order. Dimensions are then connected to the fact table to provide additional, descriptive information about the transaction, such as, the country of the customer.
Hierarchy
A hierarchy is a structured representation of an organization, a list of products, the time dimension, and so on, using levels.

It is often used to provide the easy navigation of a large data set in a drill-down. You can define one or more hierarchies in calculation views that provide the end user with a convenient and natural way to navigate their data.

Watch this video to learn about the hierarchy concept.

Semantics
The term semantics is used to describe the meaning of an attribute or a measure, for example:

A monetary value

For example, the total amount sold would need to indicate the currency (for example USD, EUR, or GBP). An amount without a currency is meaningless.

A quantity, weight, volume, or distance

For example, quantity would need to specify the unit of measurement in which the data is expressed. Again, a quantity without a unit of measurement is meaningless.

In SAP HANA Cloud, we can add semantics to measures, attributes, or an entire calculation view. Semantic information is essential in the final report to provide the business user with important information about the data. This information is often not available in the original source tables but can easily be added by the developer in the calculation view.

Calculation Views in SAP HANA
A calculation view is the key modeling object of SAP HANA Cloud.

Calculation Views are built on top of physical tables in the database layer of SAP HANA platform. They are consumed in operational reporting and analytics applications.
The purpose of calculation views is to project data from the individual database tables, and to perform a variety of data calculations in order to generate a meaningful result set to answer a specific business question.

Benefits of Calculation Views

Design-Time Versus Runtime Calculation Views

Deploying Calculation Views
Relationship between design-time and run-time objects. Objects are designed in the HDB module in your project inside your workspace. When you deploy the object, a run-time object, such as a column view, is created in your HDI container.
When you deploy a design-time file with SAP Business Application Studio, SAP HANA Cloud generates the corresponding runtime object in a database container. A container is where all related runtime objects are stored in the database. An SAP HANA Cloud database can have many containers. We will cover containers later.

Analytical Versus Transactional Requirements
Physical tables are updated and read by transactional applications via the application server. Calculation views are read directly by analytical or (less often) transactional applications. For details, see the following text.
In transactional applications, such as SAP ERP, the underlying data (stored in physical tables) is generally handled by the application server. This layer is necessary to handle the business process logic. The application usually reads and writes directly to the database tables. Calculation views are not usually needed, however it is technically possible for the application to read a calculation view if this benefits the application. For example, it is possible to consume a calculation view from ABAP.

Calculation views are read only, and cannot change data in the SAP HANA Cloud database.

Analytical applications that do not usually need to write to the database so they can bypass the application server and directly query the calculation views, where data is calculated on-the-fly, in the SAP HANA Cloud in-memory database.

...

Creating Dimension Calculation Views
Objective

After completing this lesson, you will be able to create a dimension calculation view using the graphical calculation view editor.
Dimension Calculation Views
Purpose of a Dimension Calculation View
A dimension calculation view is used to expose master data from source tables. You can combine multiple data sources, define filters, calculate additional attributes, and create hierarchies to provide a meaningful view of master data. A dimension calculation view is a highly-reusable, centralized object that provides consistent data. You would typically define a dimension calculation view and share it with other colleagues who would consume it in their own calculation views.

An example of a dimension view for customer that accesses columns from different database tables. ID, Name, and Country are accessed from the Customer Table. Contact Frequency is accessed from the Contact Preference Table.
Dimension calculation views are only built from attributes. They do not include measures. Every column from the data source is treated as an attribute. This means that any numerical column in the source record, such as price or salary, is treated as an attribute and not a measure. This means when you query a dimension calculation view, a complete list of every single individual value in the source is produced in the output. Even the numerical values such as quantities are listed individually and not aggregated. If you want to sum values then you need a cube calculation view where it is possible to define measures.

If you would like to produce only a distinct list of attribute values, you can specify aggregation on any column. Then each unique value appears only once in the result.

A dimension calculation view does not need to be based only on a single table. You can combine master data tables in a dimension. For example, you could join the customer table to the customer contacts table to generate a comprehensive view of customer information.

Although dimension calculation views can be directly consumed by most reporting tools, it is more likely that your dimension calculation views will be consumed in cube calculation views.

Sharing Dimension Calculation Views
Dimension calculation views are reusable objects and can be shared between several cube calculation views.

One Product DIMENSION Calculation View can be used in several CUBE Calculation Views. For details, see the following text.
For example, the product dimension can be used in a purchase order cube calculation view and also in a sales order cube calculation view. Both cube calculation views require information about products. The dimension calculation view provides that shared information.

...

Creating Cube Calculation Views
Objective

After completing this lesson, you will be able to create a cube calculation view using the graphical calculation view editor.
Cube Calculation Views
When you would like to create a calculation view that includes measures, you use a calculation view of the following type: cube.

An example of a cube calculation view for orders that accesses columns from different database tables. Order ID, Customer ID, and Quantity Sold are accessed from the Sales Table. Quantity Returned is accessed from the Returns Table.
By default, all the measures in this type of calculation view will always be aggregated by the attributes requested by the query. Consequently, even though the calculation view may be able to provide many attributes, the measures are always automatically aggregated only by the attributes that were requested by the query.

For example, your cube calculation view provides the measure: revenue, and the attributes: country and city. The query requests only country, and so revenue is summed by country and not by city. If the next query requests the measure: revenue, and the attributes: country and city, then the revenue would be summed by city and also country. This means that you will have two levels of aggregation.

This type of calculation view is optimized for ad-hoc analysis, where unpredictable slice-and-dice is required over the measures by any combination of attributes within the model.

Create a Simple CUBE Calculation View
In this demonstration, you will learn how to create a simple CUBE calculation view.

Demo
Start Demo
Cube with Star Join Calculation View
An extension to the cube calculation view is the cube with star join.

An example of a cube with star join Calculation View for sales. In addition to sales and returns data from tables, customer name and contact frequency are accessed from the customer dimension Calculation View.
In addition to the capabilities of the cube type of calculation view, a cube with star join calculation view allows you to connect dimension calculation views so that you significantly expand the capabilities for analysis by providing additional attributes. For example, if you create a sales cube calculation view, which provides only limited attributes such as a product number, you could then join the product dimension calculation view to provide many more product-related attributes such as product description, supplier, color, and price. You could then aggregate the sales revenues by product color, supplier, and so on.

Cube with star join calculation views are the most advanced type of calculation view and are popular in analytical scenarios where ad-hoc analysis is required. Cube with star join calculation views are processing by a dedicated SAP HANA Cloud OLAP engine to provide high performance.

The type of joins between the fact and dimension tables within the star schema can be defined in the Star Join node. The available joins are as follows:

Referential Join

Inner Join

Left Outer Join

Right Outer Join

Full Outer Join, with some specific restrictions (see previous information)

Text Join

Screen capture of a definition of a Calculation View containing first a Join node, then a Star Join node, then a Semantics node on top. The lower level Join acts as fact table. The Star Join node above includes several dimension Views.
Shared Columns from DIMENSION Calculation Views
In a cube with star join calculation view, the Columns tab of the Semantics node separates columns into two categories:

Private
Private columns are columns that are defined inside the calculation view itself. These can be measures or attributes. You have full control over these columns.

Shared
Shared columns are columns that are defined "externally", in one or more dimension calculation views that are referenced by your cube with star join calculation view. On these columns, you have logically less control, because they are potentially "shared" with another cube with a star join calculation view. Still, you can hide some of these columns to only keep the ones that you need.

Regarding the shared columns, their Name and Label properties cannot be changed, compared with a private column, but you can define an Alias Name and an Alias Label. Moreover, providing Alias Names is mandatory if column names from the underlying dimension calculation views conflict with each other or conflict with the private column names.

Create a CUBE with Star Join Calculation View
In this demonstration, you will learn how to create a CUBE with a star join calculation view.

Demo


...

Creating Time-Based Dimension Calculation Views
Objective

After completing this lesson, you will be able to create a time-based dimension calculation view using the graphical calculation view editor.
Time-Based Dimension Calculation Views
There are two types of dimension calculation view: Standard and Time.

You use time-based dimension calculation views to automatically generate various date-related attributes from a base date. For example, from the base date dd-mm-yy, the time-based dimension calculation view automatically provides the following:

The number of the day in the week. For example, Wednesday = day 3
The week number in the year. For example, 19 Feb = week 7
The quarter. For example, 12th December = Q4
The half-year. For example, 12th December = second half
The financial period. For example, 27th April 2022 = fiscal period 01/22
This means that you do not have to provide all possible time attributes in the source record or create complicated SQL functions to generate the additional date-related attributes. All that is needed is the base date in the source record. From that we can derive all possible time attributes automatically.

The main use case for time-based dimension calculation views is to allow a business user to aggregate measures of a cube by any date-relates attribute. The modeler can even define a time hierarchy in the dimension calculation view so that a drill-down through time is possible. For example, drill from year to half-year, then to quarter, then to month, then to week, and finally to day. You can even go further and drill right down to hour, minute, and seconds.

Screen capture of creating a new Calculation View for time attributes. First, define its type TIME. Use the underlying table HA300::M_TIME_DIMENSION. Then, choose Create. A View with columns such as CALQUARTER, CALMONTH, CALWEEK is generated.
When you create a time-based dimension calculation view, a table is also generated that is automatically filled with records that represent the data attributes for a range of dates that you specify. For example, you can choose to generate records between 2020 - 2024. You can regenerate the data at any time to keep the table up-to-date. The generated time table is used as the data source to the time-based dimension calculation view. You then consume the time-based dimension calculation view in your cube calculation view.

Different Calendar Types in Time Calculation View


...

Choosing a Data Source for a Calculation View
Objective

After completing this lesson, you will be able to understand which data sources are supported by calculation views.
Supported Data Source Types in Graphical Calculation Views
All calculation views have a data source assigned. This provides the input data that is processed by the calculation view.

The following is a list of the data source types that are supported in SAP HANA Cloud calculation views:

Row Table
Column Table
Virtual Tables
Calculation Views
SQL Views
Table Functions
Row or Column Tables
SAP HANA Cloud supports traditional row tables. It also supports SAP HANA Cloud optimized column tables. It makes no difference to a modeler whether the table is row or column and all modeling functions are available with both types. Column tables are optimized to provide you with a very fast read-performance, which makes it the most popular type of table use with calculation views.

Virtual Tables
A virtual table is a table that is part of the SAP HANA Cloud database and is mapped to a remote table outside of SAP HANA Cloud.

By including a virtual table in your calculation view, you reach external data sources from any location in your landscape. The data sources can be database tables, flat files, spreadsheets and more. You should always keep performance in mind when using virtual tables as their use can significantly impact response time.

Calculation Views
Calculation views are a popular data source for other calculation views. Developing data models using layers is recommended break up complex models. In addition, a calculation view can be used by other calculation views, so it makes sense to capture the data processing logic once and reuse many times to avoid redundancy.

SQL Views
SQL views have been around for a very long time and are created using the SQL language. These views might be present in your SAP HANA Cloud database as they are still popular with developers who may not have the skills to develop calculation views. They are a much simpler type of view compared to a calculation view but still provide the ability to combine data sources and add calculated columns.

Table Functions
Table functions can be used to define complex data sources using SQL Script. These data sources can then be consumed by a calculation view.

As a general rule, you should always try to use the standard functionality of the calculation view using the graphical editors. However, sometimes you might need to revert to SQL code to produce the desired outcome.

...

Checking the Output of a Calculation View
Objective

After completing this lesson, you will be able to check the output of a calculation view to ensure correct results are generated.
Checking the Output of a Calculation View
When you are building a calculation view, SAP Business Application Studio allows you to use a feature called Data Preview to check the results.

There are two ways to display a data preview:

Right-click the top node of an open calculation view and choose Data Preview.
Right-click any deployed calculation view in the Explorer pane and choose Data Preview.
This Is particularly useful to preview data from a calculation view that is not currently open.

The embedded data preview in SAP Business Application Studio mentioned above is an equivalent to the data preview included in another SAP HANA Cloud tool: Database Explorer. This tool can be launched from the Explorer window, in the SAP HANA Projects pane. Choose the root element (this is the database module, for example HC300/db), and choose Open HDI Container.

In the data preview, you can view the SQL code that generated the results. This is a simple SQL query that selects all columns of the calculation view, with no filters. The statement includes all aggregate functions defined In the top view node, if any. You can edit the query to remove columns or add filters, and so on, in order to test the calculation view against different query conditions.

Watch this video to learn about the Standard Preview or Custom SQL Query.

Standard Data Preview Features
Even though the Database Explorer is not a reporting tool, it still offers analysis functionality that can be useful during modeling or troubleshooting. It is comprised of the following tabs, each offering specific capabilities:

Database Explorer Tabs
Tab	Displays	Use Case
Raw Data	All data	Basic display of contents
Hierarchies	Data for a Selected SQL Hierarchy	Preview of a modeled SQL hierarchy
Analysis	Selected attributes and measures in tables or graphs	Profiling and analysis
Choosing the User who Executes the Data Preview
Up to SAP HANA Cloud QRC 3/2021, the data preview triggered from within the Calculation View editor (by right-clicking the up-most node, below the Semantics node) or right-clicking a design-time object in the DB module structure, was executed with a technical user ..._RT who has, by default, extensive privileges on the container's objects plus all the privileges granted to the application user in case an external schema or container is accessed.

Since QRC 4/2021, from the Calculation View editor, it is possible to choose whether you want to trigger the data preview with this ..._RT user (Data Preview) or another user (Data Preview With Other Database User). This allows you to apply session contexts (client values, analytic privileges, and so on).

Screen capture of the context menu on an Aggregation node with entry Data Preview with Other Database User. See text before and after the image for details.
You must choose the other database user among the users referenced in connections defined in the Database Explorer. In particular, to query a calculation view with a standard database user, this user must be assigned to an SAP HANA Database (it is not possible to add an HDI container with another user than the ..._RT user).

You can find more information about users and security in the section, Implementing Security in SAP HANA Modeling, of this course.

Note

To avoid Data Preview delay, the list of Database Explorer connection is not filtered. Make sure to choose a relevant connection, that is, one made with a user who has access to the HDI Container schema.
Manual vs. Automatic Data Preview Query Execution
Up to SAP HANA Cloud QRC 4/2021, upon data preview, the default query is automatically executed.

Starting with SAP HANA Cloud QRC 1/2022, a general option in SAP Business Application Studio allows you to decide whether the query is executed automatically, or needs to be triggered manually. This new option allows you to avoid the automatic execution of an expensive query during data preview, especially when you want to adapt the query before executing it.

Hint

To find the setting easily, in the preferences of SAP Business Application Studio, search for Manual Refresh. Note that you can have different settings in each development space, and you can also specify the option at the User or Workspace level.
Data Preview: SQL Hierarchies
SAP HANA Cloud QRC 1/2022 brings in a new capability for Data Preview: SQL Hierarchies modeled in a Calculation View can be visualized within SAP Business Application Studio Data Preview.

Screen capture of the preview of a geographical hierarchy structure and its data. The root node (all) is expanded to regions (Europe, North America), to countries (DE, ES, FR, GB) and to provinces). A button SQL at the top right allows you to display the SQL hierarchy query statement.
It allows you to easily check the modeled hierarchies without using an external and / or third-party tool. You can also display and copy the SQL statement used to navigate the hierarchy.

...

Working with Common Features of Calculation Views
Objective

After completing this lesson, you will be able to describe features that are common to all types of calculation view.
Defining Column Semantics
Every calculation view has a Semantics node. You do not add this, it is already present and will always be the top node (regardless of the type of calculation view). In this node, you assign semantics to each column in order to define its behavior and meaning. This is important information used by consuming clients so that they are able to display and process the columns appropriately.

Screen capture of details of the Semantics node of a calculation view. As Semantics, you can specify that a field is Quantity with Unit Of Measure or Amount with Currency Code or Date. For the reason to do so, see the following text.
One of the most important and mandatory settings for each column is its column type. You can choose between attribute or measure. This defines the basic role of the column.

In the semantic node, you can also optionally assign a semantic type to each column. A semantic type describes the specific meaning of each column and this can be helpful to any client that consumes the calculation view by enabling it to then represent the columns in the appropriate format. For example, if you define a column as a date semantic type, the front-end application can format the value with separators rather that a simple string. The key point is that it is the responsibility of the front-end application or consuming client to make use of this additional information provided by the semantic type.

Assigning a description column to another column - for example, assigning the product id column to a product description column so that a user sees a value which is more meaningful.

Hiding a column - This can be used if a column is only used in a calculation, or is an internal value that should not be shown, for example, hiding the unhelpful product id when we have assigned a description column that should be shown in its place.

Assigning a variable - This enables a user to select a filter value at runtime for the attribute.

Apart from the semantic type, there are other important settings that can be defined for each column such as the following:

One of the most frequently maintained values in the Semantics node is the name and label of the column. It is possible to define an alternative name and label to any column so that it will make more sense to a user than what was originally proposed from the data source.
For example, who wants to see the word MATNR in a business report column heading when we really should be seeing the words Material Number?
Data Source Aliases
In some cases, you need to use the same data source more than once in a calculation view. Although it is possible to include the same data source multiple times in the same calculation view, each instance needs a unique name to identify it.

When adding multiple instances of thee same table to a view, an alias will be proposed. It is possible to modify the proposed alias name, such as HA300::SNWD_BP_1, in the data source properties of the Mapping tab.
In order to support this scenario, you define an alias for any additional instance of the same data source. The alias is found under Properties when you click on the header of the data source.

Note

SAP Business Application Studio automatically suggests an alias by adding an incremental number to the end of the original data source name. We recommend choosing your own alias name to provide something more meaningful, by overwriting the proposal.
Hiding Columns
Sometimes columns are used within calculations but must not be exposed to the business user through a front-end tool. You can choose to hide columns that should not be exposed.

Screen capture of the columns settings for the Semantics node. To hide a column, select the Hidden check-box. The column will not be exposed to reporting tools, but can be used within the calculation view itself, for example in calculated columns.
In the example shown in the figure, Hidden Columns, we see that FIRST_NAME and LAST_NAME are not needed as they are hidden. The scenario that we have created here is a new calculated column FULL_NAME, which combines FIRST_NAME and LAST_NAME so that these are not required in the result.

Label Columns and Hidden Attributes
You can also define a label column to provide a more meaningful description to the end user in the report compared to displaying the technical name - for example, Employee Name vs. Employee ID.

Screen capture of the columns details for the Semantics node. You can define a label column next to a base attribute. When reporting on an information model with a tool that supports this feature, the label column is displayed as Text and the base column is displayed as Key.
To achieve this, you simply associate the column that contains the technical name with the column that contains the description. The front-end tool usually provides a feature that enables the end user to toggle between the two.

Watch this video to learn how to Hide Columns Set Label Columns.

Sorting the Data Set
SAP HANA Cloud has the possibility to sort the result set (output) of any calculation view. This enables you to define a sort order that will apply when none is specified by the client query that is executed on top of the calculation view, thus guaranteeing a stable result order.

It can also be useful when previewing the data for the purpose of testing your calculation views.


Note

It is technically possible to include a hidden column in the list of columns used to sort a result set. However, this generally does not make sense as the business user will not see how the data set is sorted.
Caution

When the client query defines itself, a sort order on different columns, or only a part of the sort columns used in the calculation view design to order the result set, you cannot be sure that the original order defined in the calculation view on other columns is applied. For example, if the calculation view sorts the result set by SO_ID, then PRODUCT_ID and a front-end tool executes a query ordered by PRODUCT_ID, you will not necessarily have your result set actually sorted by PRODUCT_ID then SO_ID. To get this result, the front-end query needs to specify the two columns in the ORDER BY clause.
Null Values
Columns, both attributes and measures can contain undefined values or null values. You can handle such cases by defining default values that replaces the null values in reporting tools.

For example, you can replace the column values that would usually appear with the null value representation of '?' with a default value 'Null', or 'Empty' or with any other user defined value that you prefer.

Select the Semantics node
Choose the Columns tab
Select a measure or attribute
Select the 'Null Handling' checkbox
Optionally, in the Default Value text field, provide a default value
Note

If you have enabled null handling for columns and if you have not provided any default value, then the tool considers the integer 0 as the default value for columns. However, for columns of data type NVARCHAR, if you have not defined a default value after enabling null handling, the tool displays an empty string, (which means blank), as the default value.
General Properties of Calculation Views
For each calculation view, you can define a number of properties in the View Properties tab of the Semantics node. Depending on the type of calculation view, the list of available properties may vary.

Calculation view properties are organized with four tabs of the semantics node that have the labels General, Advanced, Static Cache, and Snapshots.

The two following tables list the General and Advanced properties of views and give a short description. A number of these properties are described in more detail in the following lessons.

Properties of Views — General
Property	Description
Data Category	For calculation views, this determines whether the view supports multidimensional reporting.
Type	Standard or Time
Run With	Defines how to apply security when executing a script-based calculation view.
Default Client	Defines how to filter data by SAP CLIENT (aka MANDT).
Apply Privileges	Specifies whether analytic privileges must apply when executing a view (SAP Business Application Studio for SAP HANA modeling supports SQL analytic privileges only).
Default Member	Defines the default member to be used for all the hierarchies of this calculation view.
Deprecate	Identifies views that are not recommended for reuse, though still supported in SAP HANA Modeler.
Enable Hierarchies for SQL access	In a calculation view of type CUBE with star join, determines whether the hierarchies defined in shared dimension views can be queried via SQL.
End-User View	Specify whether to offer this calculation view as a source to reporting tools.
Properties of Views — Advanced
Property	Description
Propagate instantiation to SQL Views	If the calculation view is referenced by an SQL view or a Core Data Services (CDS) view, it determines whether the calculation view must be instantiated. For example, it prunes columns that are not selected by the previous queries) or executed exactly as it is defined.
Analytic View Compatibility Mode	If this setting is activated, the join engine ignores joins with cardinality n..m defined in the star join node when no column at all is queried from one of the two joined tables.
Ignore Multiple Outputs For Filter	Optimization setting to push down filters even if a node is used in more than one other node.
Evaluate Session Variables Early	Optimize union pruning by considering session variable values during pruning decision
Pruning Configuration Table	Identifies which table contains the settings to prune Union nodes.
Execute in	Determines whether the model must be executed by the SQL engine or column engine.
Execution Hints	This property is used to specify how the SAP HANA engines must handle the calculation view at runtime.

...

Top View Node
Objective

After completing this lesson, you will be able to describe the function of the top view node.
The Top View Node
Top View Node
When you create a new calculation view, there are always two nodes that are automatically provided.

The very top node is the Semantic node. This node is used to define enrichments to the final results set, such as adding currency information to measures, or apply masking rules to cover up sensitive parts of a data. This is a mandatory node and no additional nodes can be added on top.

Below the semantic node, there is another very important node called the top view node. This node generates the calculated data set before semantics are applied.

How the data category of a calculation view influences the valid top view nodes and result set. The default for a dimension view is Projection. The result is a complete list of attributes. If you switch to Aggregation, the result is a distinct list of attributes. The default for a Cube view is Aggregation. The result aggregates measures over selected attributes. If you switch to Projection, the measures are not aggregated. If you switch to Star join node, you access more attributes from multiple dimensions to aggregate measures.
The type of top-view node that is automatically provided depends on the type of calculation view you are creating. SAP provide a default top-view node type for each of the calculation view types, but it is possible to swap this default node to a different type. You would do this to modify the behavior of your calculation view so that the correct final result is produced before the additional semantics are applied.

Used in conjunction with the switch of calculation view data category, which you find under Properties, it means that you can avoid having to create a brand new calculation view if you just want to change the final step in data preparation. When you switch the node type, the graphical calculation view editor attempts to preserve as much of the existing calculation view definition as possible.

You cannot remove the top-view node or add any additional nodes on top of it.

Note

A very simple calculation view might not require any additional nodes below the top-view node.

The rationale for having the Semantics on top of the top-view node is as follows:

Let you manage the top-view node as any other node of the same type (aggregation, projection, or star join which is exclusively used as a top view node and has a few additional capabilities)
Display separately the view-global information, such as view properties (including caching and snapshots queries), list of columns (including shared dimensions) with all their properties, column lineage, and so on.
Define additional settings such as the semantics of measures (for currency translation purposes, for example), hierarchies, variables, and input parameters.

...

Knowledge quiz
It's time to put what you've learned to the test, get 7 right to pass this unit.

1.
Why do you create a time-based dimension calculation view?
Choose the correct answer.

To define a calculation view that is valid for a specific period of time

To automatically generate time-related attributes from a base date

To generate historical views of master data
2.
What is the role of a dimension calculation view?
Choose the correct answer.

To aggregate measures by one or more attributes

To combine dimensions with a cube to form a star schema

To generate a view of master data from one or more tables
3.
Which is the default top-view node for a dimension calculation view?
Choose the correct answer.

Star Join

Projection

Aggregation
4.
Which are benefits of calculation views?
There are two correct answers.

They calculate live data on-the-fly

They can be defined using any database language

They adapt automatically to the requesting query
5.
What is the role of the cube calculation view?
Choose the correct answer.

To aggregate measures without the need for dimensions

To project master data into a harmonized view

To develop a star schema for ad-hoc analysis
6.
What is the name of the tool that is launched with Data Preview of a calculation view?
Choose the correct answer.

SAP Business Application Studio

SAP HANA Cloud Cockpit

SAP HANA Database Explorer
7.
Why do you hide columns in a calculation view?
Choose the correct answer.

When you want to hide a column that is used in a calculation but is not required to display in a report.

When you do not want to expose a sensitive column to a consuming calculation view.

To ensure they can only be displayed along with other attributes and not used for drill-down navigation of filtering.

When you want to hide some of a calculation view data based on attribute values.
8.
Which of the following are supported data source types for calculation view consumption?
There are four correct answers.

Calculation views

Flat files

Virtual tables

Column tables

Row tables

......

Unit 3
Working with Common Nodes in Calculation Views
After completing this unit, you will be able to:

Describe a projection node.
Aggregate measures using the aggregation node.
Using Projection Nodes
Working with Aggregation Nodes

...

Using Projection Nodes
Objective

After completing this lesson, you will be able to describe a projection node.
Projection Node
Using a Projection Node
The Projection node is one of the most popular node types. Projection nodes usually appear in most calculation views of all types. They provide many useful features that most calculation views require.

Screen capture of a calculation view of a Projection Node. It displays the Mapping tab with some mapped and some unmapped columns.
A Projection node is typically used to achieve the following:

To select only the required columns from a data source.

To define calculated columns.

To define parameters that request values at run-time, such as user-prompts.

To apply a filter on the data source.

...

Working with Aggregation Nodes
Objective

After completing this lesson, you will be able to aggregate measures using the aggregation node.
Aggregation Node
The purpose of an Aggregation node is to apply aggregate functions to measures based on one or several attributes. The node generates a result set that is grouped by the selected attribute columns and computes the selected measure with the specified function, such as SUM, MIN, AVG, and so on.

Screen capture of an Aggregation node of a calculation view. In this example, details on the Columns tab define aggregation function SUM for the measure Amount_Total and MAX for the measure Largest_Amount.
By default, the granularity of the aggregation is defined by the attribute columns that are mapped to the output of the aggregation node. In the example, you will get one aggregate row for each country/order pair, showing two measures: the total amount of each order, and the biggest line item of each order.

Note

If you want to get the total amount and biggest order amount at the country level, removing the Order column from the output is not enough because you will still get the biggest line item detail in each country. In that case, you should first aggregate both measures with SUM in one aggregation node (without the Order column), and add another aggregation node on top defined like in the example (but without the Order column).
If you exclude an attribute from an upper node (in the same calculation view or another one that consumes it), or in a query executed on top of this view, by default these attributes are ignored. In addition, the calculation view aggregates the result set on the remaining attributes.

A graphical calculation view in SAP HANA Cloud supports the following aggregate functions:

Sum (the default function)

Min

Max

Count

Average

Variance

Standard deviation

Median

Caution

These aggregate functions should be used very carefully in stacked scenarios to avoid incorrect results.

In an Aggregation node, a calculated column is always computed AFTER the aggregate functions. If you need to calculate a column BEFORE aggregating the data from this column, you have to define the calculated column in another node, for example a Projection node, executed BEFORE the aggregation node in the calculation tree.

Defining an Aggregation in the Client Query
When you execute an SQL query on top of a CUBE Calculation View (with or without Star Join), an attribute that is not selected is ignored, as discussed earlier. But the aggregation behavior depends on whether your SQL query includes a GROUP BY clause or not.

If your SQL query does not include a GROUP BY clause, each requested measure is aggregated as specified in the Semantics of the Calculation View.
If your SQL query includes a GROUP BY clause, the aggregate function you specify for each requested measure overwrites the one defined in the Semantics of the Calculation View. This provides a lot of flexibility, but can be error-prone in some scenarios. It is recommended to use this with care.
Note

The default data preview SQL statement for a CUBE calculation view (with or without star join) in SAP Business Application Studio and Database Explorer uses the second approach, thus showing you which aggregate function is applied to each measure as per the Semantics. Be careful with the data consistency if you change any aggregate function.
Defining Aggregation Attributes
From QRC 1/2023 onwards, you can provide in the Semantics node of a calculation view of type CUBE (with or without star join) information about which aggregation level is relevant and which is not.

Screen capture of an Aggregation node of a calculation view. In this example, details on the Columns tab define aggregation attribute CURRENCY_CODE for the measure GROSS_AMOUNT_SUM. For the consequence, see the following text
In this example, the GROSS_AMOUNT should not be aggregated without keeping the column CURRENCY_CODE in the result set, at least if the column is not converted. Indeed, this might result in adding amounts expressed in more than one currency, for example USD and EUR, making the output of the calculation view inconsistent.

For each measure of the calculation view, you can define one or several attributes that should be kept when aggregating data.

The information entered in the Semantics is meant to be used by reporting tools through the SAP HANA Analytics Catalog, which is made up of BIMC Views (schema _SYS_BI). The main table holding the aggregation attribute(s) information is BIMC_EXCEPTION_AGGREGATION_ATTRIBUTES.

Caution

The setting defined in the Semantics has no influence on the way aggregations are executed by the calculation engine. Besides, it does not prevent the execution of a query that might return inconsistent data because an aggregation attribute needed for a given column has not been included.
Controlling the Behavior of the Aggregation Node
When you work with Aggregation nodes, the list of columns requested by the client query can influence the way the aggregation is executed, especially in complex scenarios.

The following features can help you control the aggregation of measures, in order to build more flexible models:

Keep Flag

Transparent Filter

Keep Flag
Let’s consider a scenario where a data source contains the details of sales orders by order ID. The only measures available are quantity and price.

Note

Each order ID relates to one store, one customer, and only one product (in order to simplify the example).

You want to calculate the quantity and total sales for the product Mouse and the month of February.

Screen capture of a data preview showing the effect of the Keep Flag. In this example, the raw data contains four sales orders for the month of February and the product Mouse, each with a quantity of 2 and a price of 5. Without the Keep Flag property, the quantities would first sum to 8 and the prices would sum to 20, and then the calculation would result in an incorrect sales value of $160. But, with the Keep Flag property on the ORDER_ID field, the preview correctly displays $40 because the sales value is calculated per ORDER_ID.
Let's mandate a level of aggregation (Month, Product) that is generic. Hence, the total sales is calculated by multiplying the sum of quantities by the sum of unit prices.

Setting the Keep Flag property to true for the Order ID column forces the calculation to be triggered at the relevant level of granularity, even if this level is not the grouping level defined by the client query.

The Keep Flag option can also be defined on the shared columns in the Star Join node of a CUBE with Star Join calculation view (shared columns are the ones defined in the underlying DIMENSION calculation views).

Transparent Filter
In some scenarios, using a filter (where clause) in a client query forces a column to be used in the Group By columns set.

Screen capture of a data preview showing the effect of the Transparent Filter property. In this example, the raw data contains one record for customer Susan in store TigerDirect and two records for customer John in stores TigerDirect and Amazon. Without the Transparent Filter property on the Customer column, the store count would be 1 + 2 = 3, incorrectly missing the duplication of the TigerDirect store. But, with the Transparent Filter property, the StoreCount is correctly 2. For details, see the following text.
In this scenario, for example, calculating the number of stores that sold mice to John or Susan triggers an intermediate calculation of the StoreCount sum by product and by customer, instead of calculating the StoreCount by product.

Setting the Transparent Filter property to true for all models and nodes that reference the Customer column will stop the Customer column from being unnecessarily used in the Group By clause.

This property is required in the following situations:

When using stacked views where the lower views have distinct count measures

When queries executed on the upper calculation view contain filters on columns that are not projected

Note

The Transparent Filter setting, when defined on an output column of a node (that is, in the right pane of the Mapping tab), can only influence the calculation made in the upper node. In particular, if a filter defined in a lower node of a calculation view should be transparent for another view that consumes it, you must select the Transparent Filter property of all nodes from that node up to the top calculation view node.

...

Knowledge quiz
It's time to put what you've learned to the test, get 2 right to pass this unit.

1.
When is the calculated column computed in an aggregation node?
Choose the correct answer.

Before aggregation

After aggregation
2.
What is the purpose of the Projection node?
There are two correct answers.

To apply filters on the data

To extract only the required columns from a data source

To aggregate measures

To join data sources


......

Unit 4
Joining Data Sources in Calculation Views
After completing this unit, you will be able to:

Implement the different join node types to combine data sources.
Specify how a filter applies to a join.
Optimize joins to improve calculation view performance.
Using Join Nodes
Filtering on Join Nodes
Optimizing Joins

...

Using Join Nodes
Objective

After completing this lesson, you will be able to implement the different join node types to combine data sources.
Connecting Data Sources using Join Nodes
One important activity when creating calculation views, is to express the relationships between the different data sources. Frequently, this is done by using joins.

A specific node type, the Join node, is used in modeling to define joins between one or more data sources.

In addition to the join node, the Star Join node, which is used to model a star schema, also defines one or several joins between a main data sources (fact "table") and the dimension calculation views.

For each Join node, you must define which columns of the two joined sources must participate in the join condition, as well as the join type. You can also specify the cardinality, with the help of the Propose Cardinality feature.

Sample Business Case and Data
To illustrate the behavior of the different types of joins in SAP HANA Cloud, consider the following tables:

Sales Order

Customer

State

The objective is to join these tables to retrieve the sales order amounts (facts) with the customer information, including the states in which the customers reside.

Three tables with the sample data for the following join examples: Sales Order (several order ids with customer ids and amount) Customer (customer ids with name, state code, and age) and State (state code with state name). See the following text for details.
To begin with, you can make the following observations so that you will better understand the behavior of each join type that is illustrated in the next section:

Sales Order 8 does not have a customer master record.

Customer TOM does not have any orders.

State TX does not have a description.

No customer resides in Alabama.

Inner Join
The Inner Join is the most basic of the join types. It returns rows when there is at least one match on both sides of the join.

SQL code and its result set of an inner join of dimensions Customer and State. Two customers are not returned because there is no corresponding entry for their state code in the State table.
Inner Join in a CUBE Calculation View
The figure, Inner Join in a CUBE Calculation View, shows the behavior of Inner Joins in a CUBE calculation view.

With the sample scenario data, some facts are not retrieved because customer information is missing.

SQL code and its result set of an inner join of SalesOrder, Customer and State. Facts (some orders) are lost because there is no corresponding entry for their customer or state code in the partner tables.
Left Outer Join
A Left Outer Join returns all rows from the left table, even if there are no matches in the right table.

SQL code and its result set from a left outer join on the Customer and State dimensions. Two customers have placeholders for undefined state texts because there is no matching entry for their state code in the State table.
Left Outer Joins and Design Time Filters
The figure, Left Outer Joins and Design Time Filters, shows the behavior of left outer joins with design time filters.

A result set from combining a filter and a join on the Customer and State dimensions. Filters are applied to both tables, and then the join is performed.
Left Outer Join in a CUBE Calculation View
SQL code and its result set from a left outer join on the SalesOrders, Customer and State dimensions. Two orders have placeholders for missing customer attributes or state texts. Customer TOM is not returned because there is no corresponding sales order item record in the SalesOrder table.
The figure, Left Outer Joins in a CUBE Calculation View, shows the use of left outer joins in a CUBE calculation view.

Compared with the Inner join, all the sales order data (including those with no corresponding customer information) is retrieved, but still an analysis of sales by customer or state will return irrelevant data.

Right Outer Join
SQL code and its result set from a right outer join on the Customer and State tables. The state Alabama from the right table is included though there is no match in the left table (Customer), but it has placeholders for customer attributes.
A Right Outer Join returns all the rows from the right table, even if there are no matches in the left table.

Right Outer Join in a CUBE Calculation View
SQL code and its result set from a right outer join on the SalesOrder, Customer and State tables. Alabama from the right table is included though there is no match in the left table, but it has placeholders for customer attributes and amount.
Full Outer Join
Result set of a full outer join on Customer and State. All customers and states are included.
A Full Outer Join combines the behaviors of the Left and Right Outer Joins.

The result set is composed of the following rows:

Rows from both tables that match on joined columns

Rows from the left table with no match in the right table

Rows from the right table with no match in the left table

Caution

A Full Outer Join is supported by calculation views only, in the standard Join and Star Join nodes.

However, in a Star Join node, a full outer join can only be defined on one DIMENSION calculation view. This view must appear last in the Star Join node.

Referential Join
SAP HANA Cloud offers a type of join that is optimized for performance: the Referential Join.

The key principle of a Referential Join is that, if referential integrity between two tables is ensured, then under some circumstances, the join between these tables will not be executed, which will save execution time.

The concept of referential integrity between two tables (A and B) means that in the joined columns there is always a match in table B for a row of column A, or the other way round, or both.

Let’s take the example of a cube with star join calculation view that is defined with many dimension calculation views, and the joins all have cardinality n..1 (meaning, a fact is connected to, at most, one member or each dimension). Let’s assume that the source system that sent the data to SAP HANA Cloud ensures that all records in the fact table always have a match in all the dimension calculation views.

Note

A referential join can be implemented in a standard join node as well as a star join node.

Therefore, when you execute a query on your cube calculation view, the calculation engine can prune any dimension calculation view from which your query does not request any column. Consequently, the corresponding join will not be executed. This is where optimization occurs.

Referential Join
Relies on Referential Integrity	Referential integrity is the fact that matches exist between the joined tables (in one direction, for example left to right, or right to left, or both).
Optimized for performance	Referential Join is not executed in circumstances where the result (without the join) will be the same as if the join was executed.
Like an Inner Join when join is executed	When a Referential Join is not pruned, it is executed as an Inner Join.
Defining a Referential Join
To define a Referential Join, you first add a Join node to the calculation view scenario, and assign two or more data sources. In the case of a CUBE with Star Join calculation view, you assign a lower node to the Star Join node and add one or more DIMENSION calculation view(s).

You now have to define the following settings:

Join Type
This setting must be set to Referential.

Cardinality
Cardinality must be specified. If it is not, the Referential Join that cannot be optimized.

Integrity Constraint
This setting defines in which direction the referential integrity is guaranteed.

Left: Every entry in left table has at least one match in right table.

Right: Every entry in right table has at least one match in left table.

Both: Every entry in both tables has at least one match in the other table.

With the Integrity Constraint setting, you ensure that the optimization of the join occurs only when it is actually possible, because the cardinality alone is not enough. If Integrity Constraint is set to left, no join optimization is triggered if a query requests columns only from the right table.

Screen capture on defining a Referential join with Cardinality n..1 and Integrity constraint Left. For details, refer to the following text.
Conditions for Referential Join Optimization
A join defined as a Referential Join between two tables or sources, A and B, is pruned (not executed) when all three following conditions are met:

No field is requested from B

Integrity is placed on A

The cardinality on the B side is ..1

Note

When the cardinality on the B side is not ..1, the join will always be executed, even if no column from B is requested. This is a requirement to get the correct number of rows in the output, which depends on the number of matching rows in B for each row in A.

Let’s take an example based on the figure, Referential Join.

The Join is defined as Referential, the Integrity Constraint (Integrity) is placed on the Left "table". This means that any record from the SNWD_SO_I table has at least one match in the CVD_PD calculation view. However, the cardinality is n..1, which also tells us that this is at most one match.

So, all in all, for each record of SNWD_SO_I, there is exactly one match in CVD_PD.

With this join definition, if a query selects NO column from the right "table" CVD_PD, then the join will not be executed.

Caution

Referential Joins must be used with caution because they assume that referential integrity is ensured at any time. Using Referential Joins in a context where referential integrity is not ensured might lead to different results depending on whether or not you select columns from one of the two data sources.

Text Join
Frequently, attributes provide labels in multiple languages to support the scenario where different business users require reports to be displayed in their own local language. SAP HANA Cloud calculation views support this scenario.

A Text Join enables the display of attribute labels according to the language of the business user. The prerequisite condition to using this feature is that the source data provides the attribute labels in various languages. Calculation views do not translate labels.

Screen capture on defining a Text Join between tables MARA (for material attributes) and MAKT (for material texts) with Language Column SPRAS. For details, refer to the following text.
Technically, a Text Join behaves like a Left Outer Join, with cardinality 1:1 but, in addition, you specify a language column, typically called SPRAS in SAP systems tables.

During join execution, the language of the end user querying the calculation view is used to retrieve descriptions from the text table (here, MAKT) in the corresponding language, based on the language column.

Note

In the source system, this design for master data tables is what allows you to store a description for a single item (here, a given Material Number) in different languages using a dedicated text table (here, MAKT). The alternative would be to use a different column in the main table to store each language, which would be more complicated to handle.
Text Join Example
Text join is designed for ERP tables and used when a translation for a dimension is available (typically based on SPRAS column as the language column). The user's language is used as a filter at run-time to find the right translation for that attribute.
Temporal Join
Frequently, master data stores historical values. For example, in the employee table there might be two records for one employee. One record represents the employees job position in the past, and the second record represents the employees job position today. Each record contains dates to represent the validity of the record. So how does a calculation view know which record to request?

It is possible to add a temporal condition to a join in order to find matching records from two tables based on a date. The records are only matched if a date column of one table is within a time interval defined by two columns of the other table.

Temporal Join Example
An example of a temporal join. The join is used to derive the customer status for a contact ID. The date in the Sales Orders table is compared to the DateFrom and DateTo fields in the Customer Status table. For more details, refer to the following text.
In this example, the status of the customers can change over time, and this information is captured in a dedicated table (Customer Status). If you need to analyze the sales orders and include the status of each customer when they issued the order, you create an Inner Join on the ContactID column and add a temporal condition as follows:

Temporal column: Date (Sales Orders)

From Column: DateFrom (Customer Status)

To Column: DateTo (Customer Status)

Temporal Condition: Include Both

Note

Temporal conditions can be defined on columns of the following data types:

timestamp

date

integer

Only columns already mapped to the output of the Star Join node can be defined as Temporal Columns in the temporal properties of the join.

Temporal Joins are only supported in the Star Join of calculation views of the type cube with star join. The join type must be defined as Inner.
Join Cardinality
The cardinality of a join defines how the data from two tables joined together are related, in terms of matching rows.

For example, if you join the Sales Order table (left table) with the Customer table (right table), you can define an n:1 cardinality. This cardinality means that several sales orders can be related to the same customer, but the opposite is not possible (you cannot have a sales orders that relates to several customers).

Caution

We recommend that you specify the cardinality only when you are sure of the content of the tables. If not, just leave the cardinality blank.

Validating a Join
A screen capture of the join definition showing validation options. When you choose the button next to the Cardinality field, then a pop-up window Propose Cardinality is displayed. You confirm the proposed cardinality 1..n with the Yes button.
A feature in SAP Business Application Studio suggests the recommended cardinality, which is based on an analysis of the tables that are joined together.

Caution

This analysis of joined tables is performed at the moment you define the join. If the content of the table evolves after that, the cardinality you have defined might become incorrect.

For example, you are validating the join between the Sales Order and Customer tables, but your data contains only one sales order per customer. In this case, the join validation might suggest a 1..1 cardinality, which does not correspond to the expected scenario in real life.

Joining Multiple Data Source in a Join Node
A join requires, at minimum, two data sources. However, it is possible to include more than two data sources in a join node.

Watch this video to learn about the Multi-Join.

Schematic overview of three example scenarios of the central table of the Multi-join node. See the text below for details.
In the figure, Multi-Join Scenarios, the multi-join order property only applies in scenarios 1 and 2, and affects joins J1 and J2. The precedence between joins J1 and J3 (in scenario 2) or J1 and J2 (in scenario 3) is not controlled by the multi-join order setting.

When does Multi-Join Priority Affect the Join Node Results?
With more than two data sources feeding a Join node, the result sometimes depend on the join execution order, but this is not always the case - for example:

When all joins are Inner Joins, the result set is generally the same regardless of the join execution order.

With a mix of Inner and Left Outer Joins, the result set can vary based on the join execution order.

Consequently, it is up to the Modeler to decide between joining more than two tables in a single node, or sticking to joining "only" two tables in a given join node. This decision is based on the potential differences in behavior, and which approach provides a better legibility of the Calculation View design in the calculation scenario and / or mapping tabs.

Non-Equi Join
SAP HANA cloud provides a type of join, called a Non-Equi Join, where the join condition is not represented by an = (equal) operator. For example, the value of column CUSTOMER_ID in the table ORDERSequals the value of column ID in table CUSTOMERS, but instead is based on other comparison operators such as Greater than.

This new type of join allows more flexible conditions to address specific scenarios.

In SAP Business Application Studio for SAP HANA, this comes as a new type of node in which you define the operator that must be applied when evaluating the join (that is, when comparing the rows based on the values of their joined columns).

Defining a Non-Equi Join condition is possible for the following types of joins:

Inner

Left Outer

Right Outer

Full Outer

Hint

In a Non-Equi Join, the operator can be specified differently for each pair of joined columns, as illustrated in the following figure, Non-Equi Join — Example 1.
Example of Non-Equi join, comparing the date DueOn with plannedDate for different tasks. The tasks that are due earlier than planned date are listed. (This means planned completion date is later than the due date.)
In Example 1, two tables contain a list of products to be delivered and a list of sub-tasks, which provides the availability date of components.

The objective is to display, in a calculation view, which sub-tasks will not finish early enough to allow the company to meet the planned completion date for some products.

This requirement can be achieved with a Non-Equi Join with the following join conditions:

ProductsToBeDelivered.id Equal SUBTASKS.id

ProductsToBeDelivered.dueOn Less Than SUBTASKS.plannedDate

Dynamic Join
Enhancing Model Flexibility with Dynamic Join
In some scenarios, you want to allow data analysis at different levels of granularity with the same calculation view.

This is generally possible in an Aggregation node when measures support the aggregation at different levels, that is, when it is possible to report measures grouped by one set of columns or another. For example, calculating the total sales by country and product in one case, and by region and product in another case.

Example for a dynamic join. The dynamic join is based on Region or on Region and product, based on the selected columns. In this example, the result of a dynamic join is correct, the result of the regular join is incorrect. For details, refer to the following text.
The figure, Dynamic Join, shows an example of a more complex scenario. In this scenario, you want to present two different measures side by side, either by Country or Region:

The sales by product

The total sales (all products)

In this case, assuming that you model your calculation view with a Regular Join on Country and Region, you will get correct results if you analyze the data by country, but the results will be inconsistent if you analyze the data by region.

Note

On further analysis of the example, you can see that the details of total sales by region and product are inconsistent for products that are not sold in all the countries of the region (HT-1001)... or (to be specific) in all the countries of the region that report sales.
Benefits of a Dynamic Join
With a Dynamic Join, only the join columns requested in the query are brought into context and play a part in the join execution. As a consequence, the same calculation view can be used for both purposes, that is, to analyze data by country or by region.

Note

A Dynamic Join can be defined only with multi-column joins.
If we consider the behavior of the join from an aggregation perspective:

In a Regular (static) Join, the aggregation is executed after the join.

In a Dynamic Join, when a joined column is not requested by the client query, an aggregation is triggered to remove this column. Then, the join is executed based only on the requested columns.

Caution

With a Dynamic Join, if none of the joined columns are requested by the client query, you get a query runtime error.

...

Filtering on Join Nodes
Objective

After completing this lesson, you will be able to specify how a filter applies to a join.
Adding a Filter to a Join Node
When you define a filter (a filter expression) in a join node, you have a possibility to optimize the runtime execution of the calculation view by mapping this filter between two (or more) sources of the join node. With filter mapping, when a filter is defined on a column from one source, you can ask the SQL optimizer to also apply this filter to a column of another other source.

Note

From SAP HANA Cloud QRC 4/2023, it is also possible to define filter mapping in non-equi joins.
Screen capture of the filter mapping for a join definition. First, select Filter in a drop-down element to activate the filter mapping. Second, drag and drop column names to create a mapping, for example, map product to product_name. Third, define the direction of the filter mapping. (For the benefit, refer to the text above.)
Defining the filter mapping can be done graphically, in a similar way as defining joins. For this reason, the Join Definition tab offers two modes, to create either a Join or a Filter mapping when dragging a column from a source to a column of another source to create a connector.

Note

You can also choose to display Join connectors only, Filter mapping connectors only, or both.
A Direction must also be specified, so that the optimizer knows from which side (left or right) an existing filter must be mapped to the other side. The default direction is Left ↔ Right, that is, bi-directional. A filter mapping connector has a dedicated color (orange) and a filter icon. One or two arrows show the mapping direction(s).

The improved performance results from the early execution of filters on both sources, before the join is executed. Therefore, filter mapping can be seen as a help to the optimizer in case the optimizer does not filter before executing the join - although it would make sense in your context. In the example shown in the figure, Filtering on Join, the optimizer does not know that product in the left table is the same thing as product_name in the right table. By mapping the filter on product to the column product_name, you can reduce the result set of Projection_2 to reduce the join execution time.

In some cases, filter mapping also helps ensuring that consistent data is returned.

...

Optimizing Joins
Objective

After completing this lesson, you will be able to optimize joins to improve calculation view performance.
Join Optimization
Set Join Cardinalities
When you define a join, you can optionally set the cardinality to describe the relationship between the two data sources, that is, 1..1, 1..n, n..1, and n..m.

Based on the join cardinality setting, some joins can be omitted to improve run-time performance, but a wrong setting can lead to unexpected results. Therefore, you should always set a correct join cardinality. Follow a cardinality proposal where applicable (only tables as sources) and ensure continued correctness of the setting.
We recommend that you always set join cardinalities so the optimizer can decide if joins can be omitted at runtime based on the query that is being sent to the calculation view.

If tables are used as the data sources, and not calculation views, then you can use the Propose Cardinality option.

Caution

The cardinality setting is proposed based on the content of the joined tables at the time you select the Proposal button. The cardinality setting does not automatically update as the table content changes. You might need to come back and change the cardinality setting if the relationship in the data changes. Additionally, remember that you need to use realistic data if you plan to use the proposal. It is no good proposing cardinality on a few test records that later do not make sense on live data.
With a join definition including cardinalities, and under additional conditions, the calculation view instantiation can automatically prune a data source involved into a join node. Let's have a look at these conditions:

An SQL statement together with a screen capture of field mapping that obeys he following prerequisites for join pruning: a. No field is requested from the to-be-pruned table. b. The join type is outer, referential, or text. c. The join cardinality is either .. 1 for the to-be-pruded table or only measures with count distinct aggregation or no measures at all are requested.
The key principle applied by the SQL optimizer is that it will prune a joined source, based on the join type and cardinalities, only if this cannot affect the result set. In other words, whenever the result set might be different depending on whether the join is executed or not, it will execute the join.

Caution

Note that this "default" join pruning is not triggered on an Inner join. To optimize execution time for an inner join, you can use the join type Referential (see the following).
Let's have a look at how cardinalities and join type actually allow the optimizer to prune.

Three examples, one in which pruning is possible, two examples without pruning. For details, refer to the following text.
In the three examples, based on the same starting point scenario, no column from the right table is requested at all.

In case of a left outer join, the right table R can be ignored because the 1..1 cardinality tells the optimizer that executing the join will not impact the result set. Indeed, for each row in L, there would be at most one match in R.

In the second example, using a left join but a 1..n cardinality, it is the other way round. No pruning can occur because the join could match more than one row of R with each row of L.

Finally, in the case of a right outer join, it is not possible to prune the right table, whatever the cardinality. This is because it is not possible to know which rows of L should be kept in the result set without executing the join.

Referential Join Type
Among join types, one is called Referential. When you use a referential join, you tell the optimizer that any row from one source always has a match in the other source. For example, the sales item will always have a header. You also specify whether this property is true for the left source, the right one, or both; this is done with the Integrity Constraint column.

You should use Referential Join where applicable. A screen capture of the join properties shows joint type Referential with integrity constraint Right. This means, every entry in the right table has at least one match in the left table. Other options are Left or Both. This allows pruning under the conditions listed in the text above.
The Referential join can be considered as an inner join supporting optimization. With the actual cardinality defined, it always returns the same result set as an inner join, and uses the Integrity Constraint (in addition to other general pruning conditions settings mentioned previously) to decide if the join execution can be ignored (and one table pruned) to increase performance.

Note

The key difference with Outer or Text joins is that the Referential join behaves as an inner join, as already said. In addition, for a 1..1 cardinality setting and Integrity Constraint placed on both sources (left and right), it allows join optimization regardless of which source (right or left) has no column requested.
General Join Recommendations
Recommendation using joins: Always maintain the cardinality of the joins. Try to use n:1 and 1:1 for left outer joins, or 1:n and 1:1 for right outer joins. Try to reduce number of join fields. Avoid joining on calculated columns. Avoid type conversions at runtime. Check whether dynamic join and optimize join columns can be used. (Screen captures of bad design that needs a calculation or a type conversion in a join.)
Joins are a source of potential performance improvements. Make sure you consider all the recommendations when defining joins to ensure you get the best performance from your calculation views.

Join Column Optimization
We already covered the pruning of data sources in a join but you should learn about another optimization possibility. In the remaining data source, can the column that was part of the join also be pruned?

Background - The Default Behavior
By default, the column on which a join is defined will always be included in the result, regardless of whether the column was requested by the query.

The reason for this is to guarantee a consistent aggregation behavior concerning the join field, even when the field in not requested by the query. If this aggregation behavior changes depending on which columns are requested, this could lead to a change of the resulting values of measures.

Optimize Join Columns Option
When you select the Optimize Join Columns checkbox, you tell the calculation engine that – in case a join partner is pruned – you do not expect changes to the result set depending on whether the joined column from the queried data source is used for aggregation or not. This depends heavily on the type of aggregations that are performed by the calculation view. In particular, the SUM aggregate function is not sensitive to the grouping level, whereas the MAX or MIN are sensitive.

When Does Join Pruning Occur?
When the Optimize Join Columns option is active, pruning of join columns between two data sources, A and B, occurs when all four following conditions are met:

The join type is Referential, Outer or Text (actually, the calculation view cannot be built if join type is Inner).

Only columns from one join partner, A, are requested.

The join column from A is NOT requested by the query.

The cardinality on B side (the side of the join partner from which no column is requested) is ..1.

Caution

As you see, the optimization heavily relies on cardinality. Therefore, you must ensure that the cardinality is set according to the actual data model. If it is not, the Optimize Join Columns option will produce unstable (though sometimes faster) results.
Greedy Join Pruning
SAP HANA Cloud QRC 3/2022 has introduced a new type of join optimization called Greedy Join Pruning. It allows the SQL optimizer to prune a joined source if it is not queried at all (no column requested), regardless of the cardinalities and join type settings. This can significantly improve query runtime. However, you should use it only when the expected results do not depend on whether the join is executed or not.

To achieve this, you define the Greedy Pruning setting of a join as Left, Right, or Both. Left or Right refer to which joined source can be pruned in a greedy mode.

Note

When greedy pruning is enabled, it does not prevent other join pruning mechanisms. Therefore, even if a condition is not met for greedy join pruning (for example, the Greedy Pruning is Left but you request columns from the left table of the join), join pruning could still happen because the cardinalities, join type and so on, allow it.
Greedy Pruning can be defined at three different levels:

Screen captures of ways to define Greedy Pruning. First option: At the join level, choose the greedy pruning direction. Second option: use SQL query execution hint greedy_join_pruning: <value>. Third option: For an entire Calculation View, define an execution hint at the view properties on the Advanced tab, then add an execution hint, search for pruning, find the entry Left side greedy_join_pruning with a value 7.
Greedy Pruning at Different Levels
Level	Precedence	How to Activate Greedy Pruning
One join of a calculation view	3	In the Join Definition tab, in the Properties pane, use the Greedy Pruning dropdown list.
All joins of a calculation view	2	In the Semantics, display the View Properties > Advanced tab. Then choose Add Execution Hints, search for Pruning and select the desired greedy pruning setting.
All calculation views used in an SQL query	1	
Inside your SQL query, add the following hint clause:

SELECT <...> FROM <...> ('PLACEHOLDER' = ('ce_settings', '{"greedy_join_pruning": "<value>"}')
The values are as follows:

1: Disabled
7: Left
11: Right
15: Both
The precedence column in the previous table means that, for example, a greedy pruning setting defined for an entire calculation view overrules any greedy pruning setting defined in any of its joins. Similarly, a greedy pruning hint inside an SQL query overrules any setting defined for the entire calculation view or in a specific join.

...

Knowledge quiz
It's time to put what you've learned to the test, get 5 right to pass this unit.

1.
What do you implement to allow data analysis at different levels of granularity with the same calculation view?
Choose the correct answer.

Non-equi join

Referential join

Dynamic join
2.
How does a referential join type improve performance?
Choose the correct answer.

By pruning joins

By filtering the data

By combining similar joins
3.
When do you implement a non-equi join?
Choose the correct answer.

When your match is not on an exact value of a column

When your data sources are in different source systems

When you want to match on different data types
4.
What is the primary objective of join pruning?
Choose the correct answer.

To simplify the design of a calculation view for easier maintenance

To improve the performance of a calculation view
5.
What can you achieve by mapping a filter defined on a join node?
There are two correct answers.

Data consistency in the result set

Pruning one of the joined data sources

Better performance of join execution

Make sure the two data sources contain the same number of rows after filtering
6.
Which are options for specifying the multi-join order?
There are two correct answers.

Inside-Out

Left-to-Right

Outside-In

Right-to-Left


......

Unit 5
Working with Union Nodes in Calculation Views
After completing this unit, you will be able to:

Combine data from different sources using the union node.
Implement union pruning to improve performance of calculation views.
Working with the Union Node
Implementing Union Pruning

...

Working with the Union Node
Objective

After completing this lesson, you will be able to combine data from different sources using the union node.
Union Node
A Union node is used to combine two or more data sets to a common set of columns.

Screen capture of the definition of a union node that combines actual and plan data. If you want to combine multiple result sets with identical structures into a single result set, you can use a union node. A mapping of the sources to the target is required and allows you to adapt structural differences. This can be done via a drag-and-drop interface.
Depending on how different the column names are in the data source, you can map the columns automatically by name (columns with identical names will be mapped automatically) or define the mapping manually.

A union node can be implemented in a dimension, cube and cube with star join calculation views.

Standard Union
Depending on the requirement, you can use one of the following approaches:

A standard union

A union with constant values

Schematic example of a standard union. Two views A and B have the same 3 fields, Customer, Amount and Flag. Flag is always A (actual) for view A, and P (plan) for view B. The result after aggregation lists the summed amounts per customer and flag.
A standard union is where, for each target column in the union, there is always one source column that is mapped. In the example, you see how both sources provide a mapped column to the union. The source column does not have to have the same name as the target column. For example, a source column month could be mapped to a target column period. Additionally, a standard union does not have to provide a mapping for each source column. In other words, there could be source columns that are left behind and play no part in the union. This scenario is useful when you want to combine measures from multiple sources into one column. As the data sources can provide an attribute that describes the type of measure (for example, Plan or Actual), it means that we do not lose the meaning of each row.

Union with Constant Values
Schematic example of a union with constant values. View A has fields Customer and Amount_P (amount, plan data). View B has the fields Customer and Amount_A (amount, actual data). The union has fields Customer, Amount_A and Amount_P. Fields that are not mapped are filled with constant values 0. The result after aggregation lists the summed Amount_A and Amount_P per customer.
A union with constant values is where you provide a fixed value that is used to fill a target column where the data source cannot provide a mapped column. For example, you have a data source that provides the actual amount and you also have a second data source that provides the planned amount. You decide to avoid combining these measures into one column as they would lose meaning, because there is no attribute to describe the meaning. In this case, you would map the measure to a separate target column and provide a constant value ‘0’ to the target column for the missing measure on each side.

The choice between a standard union and a union with constant values depends on the data you are using and the way you want the end users to report on data.

If it is more beneficial to present different measures in different columns, you can use a union with constant values so that you have a way to provide a value (probably zero) to a column where there is no source mapping. On the contrary, if it is easier to present measures in a single column and differentiate them with an attribute, such as an ‘amount type’, use the standard union.

Mapping Columns Based on their Names
It is possible to automatically map columns in a Union node based on their names. This helps to save time when the data structures have some similarities in terms of column names.

The Auto Map by Name feature can be used in two different ways, depending on whether you make a selection in the Data Source area before selecting the icon.

Two Options to Map Columns by Name Automatically
Scenario	Result
You have not selected any data source (default behavior)	
All the columns of all data sources are added to the output.

Columns with matching names are mapped together.

You have selected one or several data source(s)	
Only the columns of the selected data source(s) are added to the output.

The columns from the other data source(s) are added to the output only if they have a matching column in the selected data source(s). They are mapped to their matching columns.

Note

Selecting a data source before triggering Auto Map by Name is useful when one or several other data sources have a lot of columns that you do not want to include in the output.
Unmapped Columns in a Union Node
There could be instances when a union needs to be performed between data sources that do not provide the same data structure.

If column names are different but contain the same type of information (for example, CURRENCY versus CURRENCY_CODE, you must define the mapping manually. This can be done with drag and drop.

If a column exists in only one data source but you need it in the target, you map it to the target. Then, for the other source, you must decide whether you allow the data in this column to be null or you can define a constant value that will be assigned to all the rows from this data source.

As an example, you have a data source that provides the customer status attribute and you have a second data source where the customer status attribute cannot be provided; there is no column for this. You know this second data source contains only customers with the status Active. Therefore, you simply fill the target column with a constant value for this data source to Active for every row.

Even if you have a source column available for mapping, you can choose to use a constant value instead. This creates the effect of overriding the source value.

Manage Mapping of Unmapped Columns in a Union Node
Although we described how constants are often used when one of the data sources cannot, or should not, provide a value to the union target column, we can also use constants when none of the data sources can provide a value. To do this, we first create an empty column in the union target. We then define a constant value for each data source. For example, I would like to create a union between table A, that contains part-time employees’ data, and another table B, that contains full-time employees’ data. The employment status is very important for my analysis but neither table contains a column that indicates employment status. So, I simply add a new column to the union target called ‘Employment Status’ and define a constant value for one data source as Full Time and the other data source as Part Time. Basically, I have manually tagged each source with a fixed label that now appears in each row and I can use this new column for filtering, aggregation, and so on.

To set the constant value, right-click the target column and choose Manage Mappings.

Screen capture showing how to define a constant value. The example shows the context menu of the output column CLIENT of a union node mapping. Choose Manage Mappings. Then, on the Manage Mapping window, enter a source column or a constant value, for example the CLIENT source column if the source model is ACTUAL and constant value 800 if the source model is PLAN.
Empty Union Behavior
We know that the data sources to a union can provide different columns. So, what happens when a query requests columns that are present in one source but not another source? Do you want to be made aware that one source was not able to provide a row? Or is this not important?

Screen capture of the property called Empty Union Behavior on the Mapping tab of the union node. For details, refer to the following text.
To illustrate this feature, imagine you have a data source, A, that contains the columns Product and Product Group and another data source, B, that contains only the column Product. If a query requests only the column Product Group, how does data source B respond when it does not have this column?

The answer depends on the setting in the property, Empty Union Behavior, which is set for each data source. The default behavior, as you might expect, is to provide No Row for data source B.

However, there are times when you might prefer to know that no rows could be returned from data source B. In that case, you should proceed as follows:

Add a constant column to the union output.

Provide a constant value for each data source with a suitable value that help you identify each source.

Change the property Empty Union Behavior to Row with Constants.

To test this, you simply consume the calculation view that contains the union, with a query that does not request any column from one of the data sources. In our case, stated previously, the query should request only the column Product Group. The results will then contain multiple rows from data source A, and also a single row with the constant value that you defined for the data source B and nulls will fill the empty columns that it could not provide.

Combine Two Data Sources with a Union Node
Demo
Start Demo
Union All or Union Distinct
A Union node generates a list of all values from the input data sources, even if values are repeated. For example, if two data sources both include the same customer, then the customer will appear twice in the output data set.

This is the equivalent of UNION ALL in SQL and is desirable in many cases.

Schematic example of a Calculation view. A lower level UNION node is displayed. It generates a UNION ALL result set with repeated entries. To obtain the result set of a UNION without duplicates, include an AGGREGATION node on top. Only distinct values remain.
If repeating values are not required, you should include an Aggregation node on top of the Union node to aggregate the attributes. We normally associate aggregation behavior with measures, such as SUM, AVE, and so on, but aggregation can also be performed on attributes. The effect of aggregating attributes is to simply produce a distinct list of values.

This is the equivalent of UNION DISTINCT (or just UNION) in SQL.

...

Implementing Union Pruning
Objective

After completing this lesson, you will be able to implement union pruning to improve performance of calculation views.
Union Pruning
What is Union Pruning?
There are opportunities to significantly improve the performance of unions by implementing union pruning. There are three approaches to implementing pruning rules in unions. Two of these approaches are based on column values and one is based on column names. We will first cover union pruning using values.

Constant Pruning
For each data source in a union node, you can define an additional column and fill it with a constant value to explicitly tag the meaning of the data source. For example, you could assign the constant valueplan to the data source that represents plan data and assign the constant value actual to the data source that represents actual data. If a query filter - where clause - does not match the constant value for a specific source of a union, then the source is ignored. This is called explicit pruning. Explicit pruning helps performance by avoiding access to data sources that are not needed by the query.

Series of screen captures showing an example of explicit Union pruning. Use the Manage Mapping dialog to populate a new field temperature with constant value old for a source with older data, and value new for a source with current data. When a query contains a clause ...WHERE temperature = 'old', the source for current data is pruned.
In the example in the figure, Union Pruning, the union node has a new column defined in the union node. To assign the constant value, we use the Manage Mapping pane and enter a constant value for the new column per data source. In our case, the new column is named temperature. In our case, either the value old or new is assigned. This is a simple way to prune data sources in unions where the rules are simple and rarely change. Although a union node usually has just one constant column, you can create and map multiple constant columns and use AND/OR filter conditions in your query. However, a key point is that if you do not explicitly refer to the constant column in the sending query, then the data source is always included in the union and is not pruned.

Explicit pruning is a very simple and direct way to implement union pruning but it requires maintenance of the constant value in each calculation view union node. Maintaining pruning rules inside each calculation views could result in high maintenance.

Implicit Pruning
Implicit pruning is implemented by defining one or more pruning rules in a dedicated table called the Pruning Configuration Table. The pruning configuration table sits outside the calculation view and allows you to describe in greater detail the rules that determine when each source of a union is valid.

This implicit pruning approach does not need to provide constant values in the union node for each data source inside the calculation view. With a union pruning configuration table, you can define multiple single values per column that are treated with ‘OR’ logic. You can also use the greater than and less than operators. You can as well define values across different columns in a rule. Values across columns are treated with ‘AND’ logic.

This gives much more flexibility over explicit pruning where you can only enter a single value per data source. It also means that you can easily update the pruning rules as these are simply records in a table that is shared by all calculation views.

Series of screen captures showing an example of implicit Union pruning. Use a configuration table to list conditions for each source: In the example, field ChangedAt fcontains values lower 2012-10-19 in the source for older data, and values starting with 2012-10-19 in the source for current data.
The first time you create a pruning configuration table, you will need to provide a table name. You then provide the pruning rules using various operators:

The possible operators are =, <, >, <=, >= and BETWEEN.

The BETWEEN operator refers to a closed interval (including the LOW and HIGH values).

If several pruning conditions exist for the same column of a view, they are combined with OR.

On different columns, the resulting conditions are combined with AND.

The conditions in a Pruning Configuration Table can be defined on columns of the following data types:

INT
BIGINT
VARCHAR
NVARCHAR
Date
For example, you could define that a source to a union should only be accessed if the query is requesting the column YEAR = 2008 OR YEAR = 2010 OR YEAR between 2012 – 2014 AND column STATUS = ‘A’ OR STATUS = ‘X’.

This means that if the requested data is 2008 and status ‘C’, then the source is ignored. If the request is for 2013 and status ‘A’, then the source is valid.

We can use input parameters to direct the query to use only the union sources that contain the data required.

Whichever technique you choose, you are providing the optimizer with important information that it uses when making runtime decisions regarding the union sources. This ensures the best possible performance by avoiding processing sources that are not needed.

Implement Value-based Union Pruning
Optimize Union Pruning
Watch this video to learn about optimizing Union Pruning.

Column-based Union Pruning
Column-based Pruning
Column-based pruning is another approach to eliminating data sources within unions that are not required by queries. However, unlike the other approaches, which are based on checking column values, column-based pruning looks at whether specific columns have been requested by a query and if those columns are defined as important to a result set.

Column-based pruning explained by screen captures. If a column is not mapped from one source, this source can be pruned for this column.
There are two parts to the definition of the rules.

Map-required columns to the union output
Define Focus column(s)
In our graphic, we see that both data sources could provide the SALES column, but only one data source salesOrder2019 provides the column to the output.

We also see that the SALES column has been defined as a Focus Column, which means that if a query requests this column and the data source cannot provide it then we are not interested in that data source, even if other columns (YEAR and so on) could be provided.

It is possible to define multiple Focus Columns and they can be attributes or measures or a mix of both.

This approach provides a way to define flexible pruning conditions that reacts to the calling query based on which columns are requested.

Note

It is possible to mix all three union pruning approaches, if it makes sense.
Implement Column-based Union Pruning
Column-Based Pruning
Watch this video to learn about how to optimize Union Pruning based on the columns that are included in the query executed on top of the Calculation View.



...

Knowledge quiz
It's time to put what you've learned to the test, get 2 right to pass this unit.

1.
Which of the following are union pruning approaches?
There are three correct answers.

Explicit pruning using constants

Column-based pruning

Implicit pruning with a configuration table

Cache-based pruning
2.
Which of the following statements are correct with reference to the union node?
There are two correct answers.

You do not have to select all source columns

Selected columns to union must have the same source name

You can provide a constant value when a source does not provide a column


......

Unit 6
Creating Data Slices
After completing this unit, you will be able to:

Generate data slices from multiple sources using minus and intersect nodes.
Implementing Minus and Intersect Nodes

...

Implementing Minus and Intersect Nodes
Objective

After completing this lesson, you will be able to generate data slices from multiple sources using minus and intersect nodes.
Intersect and Minus Nodes
The intersect node is used to select the rows from one data source that also appear in another data source. You choose the columns on which the intersection check is made.

The minus node is used to select rows that appear in one data source but not another. Again, you choose the columns on which the minus check is made.

Watch this video to learn about the Set Operations - Intersect and Minus.

For the Minus node, the data sets are considered based on their order in the list of data sources for the node. Therefore, the output contains items from the FIRST data source that are NOT in the SECOND data source. In SAP HANA Cloud, it is possible to change the order of the data sources: just right-click on the Minus node and choose Switch Order.

Intersect and Minus operations are dynamic. A screen capture shows an example where country and name are mapped to the output of an Intersect node. If a query selects both columns, the combination of both columns determines the intersect result. If only name is selected, only name is compared and more rows may be returned than in the first query. For other details, refer to the following text.
Filtering relies on the list of attributes that are queried at runtime. In other words, a column that is provided by both source nodes, for example, Country, but is not queried at runtime, is ignored.

...

Knowledge quiz
It's time to put what you've learned to the test, get 1 right to pass this unit.

1.
Why do you implement a minus node?
Choose the correct answer.

To select the rows from one data source that also appear in another data source.

To select rows that appear in one data source but not another data source.

......

Unit 7
Ranking Data
After completing this unit, you will be able to:

Configure a rank node to identify the top or bottom values of a data set.
Implementing Rank Nodes

...

Implementing Rank Nodes
Objective

After completing this lesson, you will be able to configure a rank node to identify the top or bottom values of a data set.
Rank Node
The purpose of the Rank node is to enable the selection, within a data set, of the top or bottom 1, 2, ... n values for a defined measure, and to output these measures together with the corresponding attributes and, if needed, other measures.

For example, with a Rank node, you can easily build a subset of data that gives you the five products that generated the biggest revenue (considering the measure GROSS_AMOUNT) in each country. The country, in this example, defines a Logical Partition of the source data set.


Note

The Rank node itself does not perform any type of aggregation on the source data set (this important topic is discussed later on in this lesson).
Main Settings of a Rank Node
The main settings of a Rank node are as follows:

Main Settings of a Rank Node
Setting	Purpose
Aggregation Function	Define the key computation executed on the data set (see below)
Result Set Direction	Decide whether to extract the Top or the Bottom (Down) rows from the ordered data set
Result Set Type	Absolute or Percentage
Target Value and Offset	Define the number of rows to return
Generate Rank Column	Indicate if you want to output a rank column and specify its name
Logical Partition	Partition the source data set by one or several columns, before executing the rank computation
Dynamic Partition Elements	Define whether the partition can be adjusted automatically based on the columns that are selected by an upper node or an upper view/query that you execute on top of the current one
Sort Column	The columns that are used to order the data set to execute the ranking
Partitioning the Source Data Set
The source data set can be partitioned by one or several columns. This means that the extraction rule you define - for example, return the top five total sales amount - will be executed in each partition, for example, for each Country and each Year.

If you choose the Dynamic Partition Element, the columns listed in the Partition will be ignored if they are not requested by an upper node or top query. To follow the same example, you could return the top five total sales for each Country only, for each year only, just by excluding the column you do not need from your top query, and without redesigning your calculation view.

Choosing an Aggregation Functions
After partitioning the source data set by one or several columns and ordering it, an Aggregation Function is applied to the data set.

The Rank node offers four Aggregation Functions which can be classified into two categories:

Three functions computing the row numbers. These are as follows:

Row
Rank
Dense Rank
One function computing the values of the sorted column, for example, a Sales amount.

This is Sum.

Table with sample data and different possible outputs depending on the applied aggregation function.
The figure, Aggregation Function, shows the behavior of the different aggregation functions on the same data set. Here is the description of each of them:

Aggregation Functions
Aggregation Function	Description
Row	Standard row numbering (ties have different ranks)
Rank	Olympic ranking (ties have the same rank)
Dense Rank	Enumerates every rank value (no gap in numbering sequence)
Sum	Adds up values of the Order By column
Row, Rank, and Dense Rank only differ in the way they deal with tie values (identical values in the sorted column). The Sum aggregation function generates a cumulative sum of the sorted column up to the current row.

Using Multiple Sort Columns
In scenarios where more than one sort column are defined, they are treated in sequence of appearance for the aggregation functions Row, Rank, and Dense Rank. This can be useful to better handle identical values in the first sort column.

The Sum aggregation function only uses the first Sort Column.

Generating the Result Set
The final stage is to extract the result set. This is done based on the Target Value setting.

Tables of sample data to illustrate the target value. Target value 2 always returns 2 rows if the aggregation function is Row, but may return more than 2 rows if the aggregation function is Rank. For the aggregation function Sum, the target value acts as a limit for the sum value.
The setting must be a rank value - for example, extract the data up to the rank 2), except for the Sum aggregation function where you set a cumulative value (for example, extract the data up to a cumulative Sales amount of 200).

The target value can be fixed, that is, defined in the Rank Node definition, or it can be set at runtime by means of an input parameter.

Note

The rank node returns the rows for which the computed ranking is lower or equal to the target value.

SAP HANA Cloud QRC 2/2024 introduces an additional capability: you can now set the Target Value setting to All Values. The rank node then sorts the value without eliminating any of them from the result set.

Working with Percentage Instead of Absolute Values
Instead of Absolute, which has been used up to now in our examples, you can set the Result Set Type to Percentage.

Note

The property is called percentage, but the calculated values are rather fractions between 0 and 1. For 50%, you will write or see the value 0.5.

The figure, Percentage Result Type, shows the difference between the two methods.

Tables with sample data for Result Set Type Percentage for all possible aggregation types. Percentage value is calculated as absolute value of the current row divided by the maximum absolute value. The target value should be lower than 1, for example 0.5. The aggregation function may affect the number of rows returned.
With the Percentage Result Set Type, you can address requirements such as the following:

Return the 50% best-selling products for each customer (for example, with a Row aggregation function)

Return the best-selling products representing 30% of the total sales for each customer

(with a Sum aggregation function).
The figure, Percentage Result Type, shows in a frame the rows that would be included in the extracted data set.

Defining an Offset on the Result Set
It is possible to exclude a number of elements from the top of the sorted data set by defining an offset.

Tables with sample data for the offset property for various aggregation types. Target value 2 and offset 2 means that up to 2 further rows with rank higher than 2 are returned.
In this example, the rows ranked up to (and including) 2 are excluded. Then the rank node returns the following rows based on the target value you have set.

Another example is when you want to assign members of a statistic collection to their inter-quartile interval. You could define a rank node as follows:

Aggregation Function: Row

Result Set Type: Percentage

Target Value: 0.5

Offset: 0.25

Is an Aggregation Needed Before Ranking?
As already discussed, the Rank node can partition the source data, but it does NOT perform any aggregation on the source data.

In other words, the way the source data is structured has a major impact on the way the Rank node will compute this data.

For example, if your source data contains a Sales Order ID column, you might not process directly a ranking of best-selling products in each country, because there might be several rows for the same Country and Product (but different Sales Orders).

The effect of aggregation before ranking. This example shows a source table with country (used for logical partitioning), product, order id, and sales amount. Option 1: Do not aggregate, returns the products and amounts of the best individual orders per country. Option 2, Aggregate data by Country and Product before ranking, returns the products per country that sell best overall.
When the source data for the Rank node is a table, you must make sure that the data structure suits your modeling needs. If not, you might need to first aggregate data by adding an Aggregation node used as a data source by the Rank node.

When the source data for the Rank node is a CUBE calculation view, the aggregation defined in that calculation view is implicitly triggered based on the columns requested by the rank node, but a column that is mapped to the output of the Rank node but not consumed by the upper node will not be requested to the data source.

Caution

As is the case with other types of nodes, some columns are passed to the upper nodes even when they are not requested. For example, this is the case when the source calculation view has a sort order defined in the Semantics node (all the columns used for sorting are passed to consuming views). This is also the case when special settings such as Keep Flag are used.
Finally, keep in mind that further processing after a rank node (in the calculation view containing the rank node, or one that consumes it) might change the order of rows in your result set. To avoid this, in some scenarios, you might need to do one of the following:

Use an ORDER BY clause on the rank column in the query to keep the result set ordered

Use the Sort Result Set option in the semantics of the calculation view.

Assigning a Type to the Rank Column
Depending on how you want to use the ranking information, you can decide to assign to the rank column the type Attribute or Measure in the Semantics node.

Rank Column: Measure or Attribute?
Attribute

Assigning the type, Attribute, is a simple approach.

It is probably a bit less error-prone because you are not tempted to perform an irrelevant aggregation of ranking positions.

Measure

With the type, Measure, the rank column can provide some flexibility.

For example, if you set the default aggregate function to MAX, you can retrieve summarized data, such as the total sales generated by the five biggest orders in each country, while keeping the information about how many orders are actually totaled in each country. Indeed, there might be countries that have received less than five orders over the considered period, and this information could be of interest when analyzing the data.

Because the information in the rank column can differ a lot depending on the aggregation function and result set type you used, you might want to give it an explicit name if you find that Rank_Column (default column name) is too generic.

Use Rank Nodes
Demo


...

Knowledge quiz
It's time to put what you've learned to the test, get 1 right to pass this unit.

1.
Why might you implement a rank node?
There are two correct answers.

To identify the best selling products in each region for each of the last 5 years.

To identify the training courses that represent the bottom 10%, based on bookings per year.

To order the sales amount by country and product each year.

To sort the sales amounts by product in decreasing order.

......

Unit 8
Embedding Functions in Calculation Views
After completing this unit, you will be able to:

Generate a restricted column.
Generate a calculated column.
Implement a filter to restrict data.
Describe how to implement currency conversion.
Generating Restricted Columns
Generating Calculated Columns
Filtering Data
Implementing Currency Conversion

...

Generating Restricted Columns
Objective

After completing this lesson, you will be able to generate a restricted column.
Restricted Columns
A restricted column is a generated column that is made from a measure filtered by one or more attributes values, for example:

Sales Revenue for France
Profit for Automotive Parts in 2022
The value in a restricted column is 0 for all rows that do not match the defined filter condition(s).

The benefit of a restricted column is that it provides the modeler with a ready-made object that is useful when you are creating calculated columns as the base objects are already filtered. This simplifies the creation of calculated columns.

From SAP HANA Cloud QRC 3/2023 onwards, it is possible to define the restriction based not only on attribute columns, but also on measure columns. For example to populate the sales order amount in two different columns, depending on whether the order has more or less than 10 line items (assuming that the number of line items per order is stored in a measure).

Restricted columns that are based on one or several attributes. For details, refer to the following text.
Restricted columns can also be used as a base measure to create further restricted columns. For example, if you create a restricted column Quantity for Spain, you could then use this to create two more restricted columns Quantity for Spain in 2021 and Quantity for Spain in 2022. For the second and third restricted columns, you only need to add the additional filter for the individual years. You can create very meaningful and focused columns using this feature. If you change the definition of the underlying restricted columns, those on top will immediately inherit the change.

The modeler should always provide a meaningful description for the restricted column so that the business user understands how the measure has been filtered as the filters are not visible to the user and they rely on column descriptions.

Creating Restricted Columns
An example of a restricted column. The base measure 'TOTAL_SALES' has been defined and conditions based on attributes has been added: 'Product_Text' equal 'Keyboard' or 'Product_Text' equal 'Mouse' .
Setting the Restriction
Screenshot of a restricted column definition showing the different operators available: Equal, GreaterThan, LessThan, Is Not Null...
The restrictions for a restricted column do not have to be limited to one single Column: you can filter based on multiple columns depending on your reporting requirements.
Multiple operators can be used for the filter restrictions, such as Equal, Greater Than, Less Than and others.
You have the option to hide the restricted column, for example to reuse it in a calculated column and thereby make it unavailable for reporting.
If there are different lines in the restriction, all the lines defined on the same column are combined with the logical operator OR, and then, all the sets of restrictions for different columns are combined with the logical operator, AND.

Note

This is the case regardless of the order in which you define the lines.

Displaying and Editing Restriction Expressions
Another option to define or edit the restrictions is to use an Expression. The expression is written in the SQL language. This provides more flexibility when the standard operators for the column-based, graphical restrictions do not fulfill your needs. The expression can use column names, operators, input parameters. Restricted column expressions can also include Functions.

Example of an expression-based restricted measure.
Several restrictions on the same column are combined with an OR operator.
Restrictions on different columns are combined with an AND operator.
To visualize the expression, select the Columns tab.
You can also edit the expression to enrich it.
In this case, the column-based definition can no longer be edited.

Note

For example, extracting the year YYYY from a date column like YYYY-MM-DD with a string or date function is possible within an expression.
Create a Restricted Column
Demo


...

Generating Calculated Columns
Objective

After completing this lesson, you will be able to generate a calculated column.
Calculated Columns
It is possible to create additional calculated columns in any type of calculation view. Calculated columns are generated on the fly at runtime and do not persist the result.

An example of a calculated column might be as follows: you have two columns containing the first and last name of the customer, but you would like to combine the first and last name in a single column. You can do this by creating a calculated column based on a string function. Alternatively, you could use an 'If / then / else' expression to check the value of a column and decide what to do if it is empty, perhaps by providing a fixed value 'Other' if it is empty.

A calculated column that replaces a 'NULL' value with the text 'Other'. First click the '+ 'icon, then enter general properties, such as column name 'DIVISION_2' and data type 'VARCHAR(50)', and in the lower part an expression, such as 'IF (isnull(DIVISION),'Other',DIVISION)'.
The calculation can be arithmetic using measures, or a character string manipulation using attributes. You can even use this feature to generate a simple, constant value when one data source does not provide a column that is required. For example, you know a table only contains data for year 2022. However, there is no column for that year, and that column is needed in a join; you could generate a calculated column using a constant value '2022'.

It is possible to nest calculated columns, so that one calculated column is based on other calculated columns.

When to Use Calculated Columns
When you include calculations in your calculation views using calculated columns, you take advantage of the speed of SAP HANA Cloud by letting the database engine perform calculations instead of doing these calculations in your client reporting tool. So it is recommended to define calculations within the calculation view and not in the reporting front end tool.

Having ready-made calculations in calculation views can also help simplify reporting by re-using calculations in calculation views and not having to define the calculation in each report. Besides, you can also hide complex and/or sensitive calculations by defining them in the calculation view. For example, (If 'Employee Region' = "South" and 'Manager' = "Roland" then 'New Salary' = 'Current Salary' - 8%).

Defining a Calculated Column
When defining a calculated column, the first step is to choose the column type, measure, or attribute.

Screenshot of a calculation view. A calculated column has been defined. To Edit the Expression, select the Expression Editor button. The editor allows you to define the expression by selecting elements, predefined operators and functions.
A calculated column is defined within an information model and can use the string functions, mathematical functions etc. available in the editor.
You can define a calculated column as measure or attribute.
Double-click or drag-and-drop Elements, Operators and Functions to build the expression.
When you type, use Ctrl+Space to autocomplete the name of functions, columns, and input parameters.
Consider Granularity When Creating Calculated Columns
A table of sample data (units, price, units * price) showing that multiplications are not meaningful on already aggregated measures.
For certain measures, it is not possible to perform the calculations when the measures are already aggregated. The aggregated granularity of, for example, Price does not mean anything.

In the figure, in the sum line highlighted in red, the units as well as the price have been aggregated. Multiplying these to aggregates does not give a meaningful result.

Triggering Calculations at the Right Level
A calculation view with calculations and aggregation in different stages.
For measures where calculation needs to be done before aggregation, you need to be wary of the different stages of your calculation view.

Ensure that you create the calculated column in a node prior to where aggregation is performed.

By analyzing your reporting requirements, you can arrive at a decision at which precise stage, the calculation should be performed.

Caution

Try to minimize calculations before aggregation and always define calculations after aggregations to minimize the impact on performance.

Calculation Before Aggregation
A table of sample data and a correct sum, because the calculations are performed on the correct granular level.
•

When defining calculation on the right level of aggregation you end up with a correct sum as the calculations are performed on the correct granular level.

When data is calculated at the correct level of granularity, the total sales measure is correctly calculated from units and prices- that is, 98,750.

Client Side Aggregation
Watch this video to learn about the Client Side Aggregation.

Consider an example where you have created the measure MAXIMUM_GROSS_AMOUNT, giving you the maximum value of the gross amount. By selecting the Enable client side aggregation checkbox, you propose to the reporting client to also apply a maximum aggregation on the client side as defined in the Client Aggregation dropdown list.

Generate a calculated column
Demo


...

Filtering Data
Objective

After completing this lesson, you will be able to implement a filter to restrict data.
Using Filter Operations
Filtering data is frequently required when analyzing data to reduce the result set to make it more meaningful to the business user. Typically, you might want to retrieve the sales details for a particular country or region, for a particular range of products, or specific customers (new customers, customers who have not ordered any product for more than one year). These three examples relate to filtering based on attributes.

On the other hand, you might want to filter data based on measures. For example, you might want to list only the sales orders for which the total amount exceeds a threshold.

The two approaches can also be combined, for example, when you want to retrieve the list of US customers who have created more than 10 orders during the past month. In this case, filtering the data by country (US) can be done as early as possible in the data flow, but you must compute the total number of orders per customer (for last month) before applying the threshold (10).

Therefore, filtering is sometimes a question of mitigating between performance (filtering as early as possible) and the consistency of the result you get (filtering too early without care can lead to wrong results). This is particularly true when data is aggregated.

Comparing different options to restrict data. A fixed filter in a calculation view defines the restriction value at design time. The flexible prompt enables users to choose a value for the restriction at runtime.
Filter Criteria: Hard-coded in Calculation View or Flexible at Run-Time?
A key question when defining filters is whether they should be hard-coded in the calculation view, or provided at runtime to provide more flexibility. It is possible to execute a query with a reporting tool that passes different filter criteria to the calculation view.

A primary goal during modeling is to minimize data transfers between the database and the front end consuming tools. For example, an end user will never need to display a million rows of data. Such a large amount of information just cannot be consumed in a meaningful way.

Whenever possible, data should be aggregated and filtered to a manageable size before it leaves the data layer on its way to the front-end reporting tool.

Filters in Calculation View Nodes
In Calculation Views, filters can be applied to many node types. These include the following:

Projection

Union

Join

Rank

Aggregation

Star Join

To define a filter, you display the Filter Expression tab and define a valid expression using the SQL language.

An example of filter expression is "COUNTRY"='US'. The expression can combine several columns and logical operators, for example, "COUNTRY"='US' OR "COUNTRY"='CA'.

Caution

When a column from a data source (for example COUNTRY), is mapped to the node output with a different name (for example, COUNTRY_CODE), the output column name must be used in the expression. In this case, the correct expression would be something like "COUNTRY_CODE"='US'. If you enter the expression "COUNTRY"='US', the expression might validate but building the calculation view will fail.

Where and When is the Filter Applied?
The behavior of filter expressions depends on the node where the filter is defined.

Filters defined in the top node

In the top node, a filter expression is generally applied before the data processing defined in this node. For example, in an Aggregation or Star Join node, the filter is applied to the data before the aggregation is executed.

Filters defined in other nodes
In other nodes (NON top nodes), the filter expression is generally applied to the output of the node.

Define and Use Filters
Demo
Start Demo
Filter Pushdown
Unblock Filter Push-down
By default, nodes that are consumed by multiple nodes prevent filter push-down from the consuming nodes to ensure semantic correctness.

If push-down does not violate semantic correctness, flag "Ignore Multiple Outputs For Filter" can be set to enforce filter push-down.

Screenshot of a calculation view scenario with one source projection then two branches and a union node on top. the Ignore multiple Outputs for Filter property. is selected on the source projection.
In your calculation view, if you have defined a node that is a source to more than one other node (in other words the data flow splits), then you need to be aware that this can potentially block filter push-down. The SQL optimizer does not want to break the semantics of the query and so by default it keeps the filter at the node at which it is defined (and this node might be high up in the stack causing large data sets to travel up the stack).

You can set the flag Ignore Multiple Outputs for Filter so that the blockage of filter push down is removed and the optimizer is free to push the filter down to the lowest level to get the best performance, but be very careful to check that the query semantics are not broken and wrong results are produced.

Note

A similar setting can be applied at the view level in the semantic node View Properties > Advanced. It has the same effect but applies to scenarios where one calculation view is consumed multiple times by another calculation view.
Client-Dependent Views
The CLIENT (sometimes also called SAP Client to avoid any confusion with the concept of customer) is a general concept in SAP Systems such as SAP S/4HANA, SAP BW/4HANA, and their predecessors. The main purpose is to isolate different types of data (for example, development versus test data) based on a specific column (generally named CLIENT or MANDT, which stands for Mandant (a German word for Client). The values in this column are three-digit numbers, such as 001, 200, 800.

Almost all tables in SAP applications are client-dependent. When you select data from an SAP application table, it usually only makes sense to request data for one client number. Mixing client data makes no sense.

The good news is that a calculation view can use the SAP client column to enable automatic client filtering when you work on data from SAP applications. You do not need to define an explicit filter on the client column each time you create a calculation view.

Whenever you create calculation views on SAP data sources that include a client column, we recommend that you always define joins on this column. Otherwise, your view might return inconsistent results if there is data for several clients. For example, you might join data for clients 200 and 800 (Cartesian product), which in general would not make sense.

Defining Client-Specific Filtering in Calculation Views
Client-specific filtering makes sense only when some (or all) of the source tables have an SAP Client column.

How to Define a Client Dependant View:
Define the Client Column in the properties of the table.
Set the Default Client property to Session Client.
In the User Management application of your SAP HANA Cockpit (or using a SQL statement), assign a Session Client to the user.
When the user runs a query based on the displayed model, the user’s Session Client (in this example: 800) is used to filter the source data.

Watch this video to learn about creating a Client-Dependent View.

To implement client-based filtering, you proceed as follows:

For all relevant data source tables, define the SAP client column.

This is done in the Mapping tab of the node where the source table is referenced, with the Client Column property.

Define how you want to retrieve data.

This is done in the Semantics node of the Calculation View. You choose one of the three following options for the Default Client setting:

Cross-Client: All the data is retrieved regardless of the client number.

Fixed Client: You specify an SAP Client number (for example, 200) and the data sources are automatically filtered to include only the rows with this client number.

Session Client: The source tables are filtered dynamically based on a client value that is specified for each user in the USERS table of SAP HANA Cloud.

The Session Client is the default setting applied to a new view.

Let’s consider a simple table that contains data for two different clients, 200 and 800. This information is stored in a column named MANDT. You create and build a calculation view based on this table, and then query the data with your user. The following table shows you the data that is retrieved depending on both the default client setting for the calculation view and whether or not the MANDT column has been defined as the (SAP) Client Column in the view properties.

Default Client Setting Effect
Default Client Setting	Effect when NO Client Column is specified	Effect when MANDT is defined as the Client Column
Cross-Client	All rows	All rows
Fixed Client: 200	All rows	Only rows for which MANDT=200
Session Client (use case 1: the user has no default client assigned)	All rows	No rows at all
Session Client (use case 2: the user has default client 800)	All rows	Only rows for which MANDT=800
Important Note Regarding the Technical User in SAP Business Application Studio
When you work in SAP Business Application Studio, most of the tasks you perform when defining a calculation view and previewing its data are executed on your behalf by a technical user, <HDI_container_schema_name>_..._RT.

As the technical user has no default client assigned, if you specify a Column Client property for a calculation view and set the Default Client to Session Client, the data preview in the Developer perspective will not retrieve any data.

To by-pass the technical user and apply client filtering based on a classic database user, you can do one of the following with this user:

Use the data preview feature Data Preview with Other Database User
Display the column view data in the SAP HANA Database Explorer from a connection to the classic database. A query from the classic database connection does not involve the technical user.

This can be done either by opening the column view from the catalog or by executing an SQL query on top of it in an SQL console.

Define Filtering on SAP Client
Demo

...

Implementing Currency Conversion
Objective

After completing this lesson, you will be able to describe how to implement currency conversion.
Currency Conversion in SAP HANA Cloud
Most organizations operate in multiple currencies but will usually want to report in a single currency. When we load data to SAP HANA Cloud database the individual transactions are often stored in mixed currencies. SAP HANA Cloud calculation views can convert currencies during runtime.

The concept of currency conversion. Source data contains a measure amount with different currency values (EUR, USD, GBP). In the Semantics node, define conversion rules. The output contains only EUR currency value with calculated corresponding amount values.
Even if the reporting tool is able to convert currencies, you should always consider implementing currency conversion in the calculation view to ensure high performance as the conversion in SAP HANA Cloud is processed completely in-memory. Additionally, by defining the conversion rules in the calculation view you ensure consistency across reports that use the same calculation view as you define the rules only once and not in each report.

Currency Conversion
The key elements provided by SAP HANA Cloud are as follows:

A set of tables to store master data about currencies, exchange rate types, and the exchange rate values.

A semantic typeAmount with Currency which you set for the measure you want to convert.

Calculation view settings to define, for each measure, how the conversion should be processed (which rate and conversion dates should be applied, where to find the source and target currency).

Standard SAP Currency Conversion Tables
SAP HANA Calculation Views implement conversion mechanisms in a similar way as SAP solutions such as S/4HANA or BW/4HANA. This allows you to calculate converted data on-the-fly, or even to simulate conversion currencies with different rates, conversion dates, and so on.

Standard tables exist in most SAP systems (in particular, SAP Business Suite and SAP S/4HANA), and SAP HANA Cloud can use them to compute data conversion.

Standard SAP Currency Conversion Tables
Table Name	Description
TCURC	Currency codes
TCURR	Exchange rates
TCURV	Exchange rate types for currency translation
TCURF	Conversion factors
TCURN	Quotations
TCURX	Decimal places in currencies
TCURT	Currency code names
TCURW	Usage of exchange rate Types
To enable currency conversion in SAP HANA Cloud, these tables must be available in the SAP HANA Cloud database. The tables are not provided by SAP HANA Cloud by default and you must provide them and update them yourself, either by creating a synonym to tables from an external schema, or using any of the available data provisioning tools.

Implementing Conversion in Calculation Views
To setup currency conversion in a calculation view, you need to follow some basic steps:

The process of currency conversion. First, assign the Amount with Currency semantic type to a measure. Second, enable it for conversion. Third, for the conversion, select the target currency, the source currency, and the conversion date and exchange rate type.
Assuming you have made the currency tables (TCUR*) available in the SAP HANA Cloud database, you can proceed.

Setting the Semantic Type
Setting the semantic type. In the Columns tab of Semantics node details, in the Semantics column, choose the Amount with Currency option for the relevant measure, such as GROSS_AMOUNT_USD.
When a measure contains a money amount, and if you want to enable conversion, you need to change its semantic type to Amount with Currency Code.

In the Columns tab of the semantic node, select a measure. Now, using the Semantics dropdown list, choose this semantic type: Amount with Currency Code.

Note

The semantic type Amount with Currency Code can be used even if conversion is not required. You would do this to provide currency information to the front-end tool so that it can display the amount alongside the currency.

In this case, you only have to set the Display Currency property to identify the currency in which the amounts are expressed. Do not set the Conversion flag.

Defining Currency Conversion Settings
The details of the currency conversion for GROSS_AMOUNT_USD in the Definition tab. For details, refer to the following table.
After assigning the semantic type, Amount with Currency Code, you then must enable conversion and define the main parameters used for conversion.

Key Settings for Currency Conversion
Key Settings for Currency Conversion
Setting	Description	Options
Client	The client (MANDT) to use to filter the TCUR* tables content	Session Client / Fixed Client Number / Column / Input parameter
Source Currency	The currency in which the amounts to convert are expressed	Fixed / Column
Target Currency	The currency in which the amounts must be converted	Fixed / Column / Input Parameter
Exchange Type	The type of rate used to convert amounts. Example: Spot rate, average rate...	Fixed / Column / Input Parameter
Conversion Date	The date used to match an amount and the corresponding conversion rate	Fixed / Column / Input Parameter
Exchange Rate	(optional) A column from the source data that contains the exchange rate to be used	 
Data Type	The data type of the converted measure (overrides the data type of the converted column)	Example: Decimal (15,2)
Generate	If selected, this option creates a column that indicates for each converted amount the (target) currency in which it is expressed.	The result currency column is never exposed to client tools. It is only available to other calculation views, where it can be used in additional calculations.
Upon Failure	Specifies the behavior if the conversion cannot be executed (for example, if the rate	Fail (a query on the view generates an error), NULL (the column is not populated), Ignore (keeps the source amount without converting it)
Accuracy	Defines how the conversion must be performed	Intermediate rounding / Retain all possible digits
It is important to carefully define how the exceptions must be handled when you are converting data. In addition, to reduce the risk of conversion failure, make sure that the currency conversion tables TCUR* in your SAP HANA system are updated on a regular basis, in particular in a side-by-side scenario where they should always be in sync with the data imported from the remote SAP system.

Decimal Shift and Rounding
By default, the precision of all values is two digits in SAP ERP tables.

As some currencies require accuracy in value, decimal shift moves the decimal points according to the settings in the TCURX currency table. If you want to round the result value after currency conversion to the number of digits of the target currency, select the Rounding checkbox.

A decimal shift back is necessary if the results of the calculation views are interpreted in ABAP. The ABAP layer, by default, always executes the decimal shift. In such cases, a decimal shift back helps you to avoid wrong numbers due to a double shift.

Reusing Currency Conversion Settings between Columns
It is possible to reuse the currency conversion settings for other measures in the same node of a Calculation View. This reduces manual definition and allows for more consistency by avoiding mistakes. The settings can be applied to several measures at the same time.

The currency conversion settings can be reused in two different ways:

Reference

The settings defined in a measure are applied as is in the other measures that you select, and cannot be modified in the other measures.

In other words, the settings will always remain consistent and only the source measure for currency conversion setting can be changed, thus impacting the ones that reference it.

Copy

The setting defined in a measure is just copied to the other measures, but they are not bound to each other. The currency conversion settings of the other measures can be freely modified.

Using an Input Parameter for the Currency
The input parameter can also be described as a prompt, in that it asks the user what currency to use.

The definition of an input parameter for currency selection at runtime. Define Parameter Type: Direct, Semantic Type: Currency, Data Type: VARCHAR, and Length: 5.
If you want to define the currency at runtime, when the view is executed, you can create an input parameter.

VARCHAR (5) is the way that the currency code is defined in the TCUR* tables, so to be consistent, we recommend that you define the input parameter with the same data type.

Implement Currency Conversion
Demo


...

Knowledge quiz
It's time to put what you've learned to the test, get 3 right to pass this unit.

1.
A calculated column can generate an attribute or a measure?
Choose the correct answer.

True

False
2.
Why might you create a restricted column?
Choose the correct answer.

To generate a filtered column to simplify the creation of calculated columns

To define rules for a measure that must be protected from unauthorized viewing
3.
Filters can be defined on attributes but not measures - true or false?
Choose the correct answer.

True

False
4.
Why should you implement currency conversion in an SAP HANA Cloud calculation view instead of in reporting tools?
There are two correct answers.

Performance is usually better

Ensure consistency of results across reports

Easier for business users to adjust the conversion settings

...

Unit 9
Creating Dynamic Calculation Views
After completing this unit, you will be able to:

Implement variables to filter data by attributes.
Define input parameters to pass dynamic values to a calculation view.
Map variables and input parameters.
Implementing Variables
Implementing Input Parameters
Mapping Variables and Input Parameters

...

Implementing Variables
Objective

After completing this lesson, you will be able to implement variables to filter data by attributes.
Variables
You define a variable in a calculation view when you want to pass dynamic values to filter attribute columns.

When a calculation view that contains a variable is called by a front-end reporting tool, the calculation view passes a request to the front-end tool to provide a value (usually using a user prompt) to complete a WHERE clause. The WHERE clause is added to the query that runs on top of the calculation view. For example, if the user was prompted to choose a country and the user chose 'JP', the SQL that runs on the calculation view looks like this:

SELECT <columns> FROM <calculation_view_name> WHERE (("COUNTRY" = 'JP')) GROUP BY <group_by columns>;

Creating Variables
You define variables in the Semantics node of a Calculation View, in the Parameters tab.

Screenshot of a variable definition. It includes a Value help view/table and the CUSTOMER attribute from that view/table as Reference Column, Single Value as the selection type, and the properties Mandatory and Multiple Entries are selected. The variable is applied to the CUSTOMER column.
A variable definition includes:

View/Table for Value Help and Attribute: These settings define which view/table and which attribute from this view/table is used as a reference to provide a list of values at runtime
Selection Type: Whether selections should be based on intervals, ranges or single values.
Multiple Entries: Whether multiple occurrences of the selection type are allowed.
You can also define whether specifying the variable at runtime is Mandatory and/or if it should have a Default Value.
You define which attribute(s) of the current view the variable should be applied to.
The View/Table Value Help setting is used to define which table/view SAP HANA will fetch the data to show in the prompt. The Reference Column defines which (unique) column in this table/view will provide the possible values for the variable.

By default, the View/Table Value Help setting refers to the current calculation view. This means that the values of the specified attribute column, for example CUSTOMER (customer ID), will be scanned in order to propose the various values in the prompt.

A different approach is to define a dedicated source table/view for the variable. For example, you could define the master data table CUSTOMERS as the Value Help source for the variable, and - of course - choose the column from this master data table that matches the column of your calculation view you want to filter on: in our example, the Customer ID.

When the Value Help table and one column are chosen, you assign the APPLY FILTER attribute. That is, you identify which attribute column will be filtered at runtime, based on the variable values chosen by the user.

Note

When the Value Help source table/view is the calculation view itself, the reference column you define is automatically assigned to the APPLY FILTER section as a filter attribute. However, you can define another attribute if needed, provided that it is consistent with the reference column.
The behavior of variables at runtime depends on whether an entry is required for the variable or not. These scenarios can be defined as follows:

If a variable is defined as mandatory, the user needs to provide the values, ranges, or intervals at runtime.

For non-mandatory variables, if nothing is specified at runtime, all the data for the corresponding attributes is returned by the view without filtering.

Creating Variables (2)
The Columns table of the Semantics node of a calculation view. The VAR_CUSTOMER variable is assigned to the CUSTOMER column and the VAR_CURRENCY variable is assigned to the CURRENCY column..
In the semantics of a view, you can see, and also define, which variable is assigned to which attribute.

Note that one variable can be assigned to multiple attributes.

Creating Variables (3)
The preview of a view with a variable or input parameter. The value help dialog is displayed that lists the possible values.
When displaying the data of a view that contains a variable or input parameter, the Value Help Dialog helps you or the reporting user to find the possible values.

More than one value can be chosen for a variable when you select the Multiple Entries checkbox.

Note

In the Data Preview, From and To are displayed in the Variable Values dialog even when a variable has not been defined as range.

Value Help for Variables
When a dialog box appears to the user they must make a selection. However, rather than an empty field appearing and the user having to guess valid values or figure out the format for a value (for example, is the country code UK or GB?), we can have a dialog box populated with the run-time values available. To do this, you make a selection in the setting View/Table for value help. The default entry is the calculation view where the variable is being created. This means that you present all possible values from the column that is assigned to the Attribute setting in the variable definition.

While this might seem like a great idea, remember that the list may be huge and would be difficult for the user to navigate. Imagine presenting a list that includes every employee in a very large organization? If the list should be restricted to offer limited values, such as employees in your department, then you should reference an external calculation view or a table that exposes a restricted list. It is also good practice, from a performance perspective, to refer to a restricted list from an external calculation view or table.

Value Help Based on Hierarchies
When creating a variable on an attribute that is associated with one or several hierarchies, you can specify one of the hierarchies in the variable definition. With this option, the user can navigate the hierarchy, rather than a flat list, to select the values in the value help. This makes navigation much easier when there are a lot of values; imagine being able to first select your country, then your department, before the list of employees appears? You can use either parent-child or level hierarchies.

The following are some of the basic rules that apply when implementing value help based on hierarchies:

If you refer to a parent-child hierarchy, the variable attribute column must be defined as the parent attribute and not the child.

If you refer to a level hierarchy, the variable attribute column must be defined at the leaf level; that is, the bottom level.

Note

Variables are not relevant for measures.
Use Variables
Demo


...

Implementing Input Parameters
Objective

After completing this lesson, you will be able to define input parameters to pass dynamic values to a calculation view.
Input Parameters
An input parameter is similar to a variable in that it supports to passing of dynamic values to a calculation view at runtime. But whereas a variable has only one purpose, that is to provide a dynamic filter based on attributes, an input parameter can fulfill additional requirements.

Input parameters can be used in different places to provide dynamic values, in particular, calculated columns, restricted columns and filter expressions. Here are a few examples:

Calculated Column

You want to execute an ad-hoc calculation of the forecast sales for next year, based on the current year and an overall expected sales volume increase - for example, 11%, entered by the user at runtime.

Restricted Column

You want a calculation view to return the sales for a year chosen by the user at runtime - for example, 2019, and the previous year, using two restricted columns.

Filter

You want to filter the data set for sales orders that have a total amount greater than $10.000. The threshold value is entered at runtime.

You want to filter sales data for products that have an name containing the string 'GTR', the string is chosen at runtime by the user.

You want to control specifically where a filter expression based on a user entry is applied inside the calculation scenario. Variables do not let you precisely choose, in the calculation view, to which node the filter is applied. Variables apply filter always on top.

Input Parameters can also be used when you have scalar or table functions in your Calculation View. The parameters of these functions can be fed with values that you enter at runtime, when querying your calculation view. This requires that you map the parameters, which will be discussed later on in this lesson.

Note

Filtering data by attributes could be achieved using variables or input parameters, It is more straightforward to use variables in this case, because you do not need to write the filter expression. In addition, some front-end clients may not support input parameters, but most will support variables.
Input Parameter Types
The following types of Input Parameters are supported:

Type	Use Cases
Direct: Currency	For currency conversion, when you want the end user to specify a source or target currency.
Direct: Date	To retrieve data based on a date entered by the end user (or chosen in a calendar type input box).
Direct: Unit of Measure	To retrieve data based on a unit of measure choosen by the end user
Static List	To provide the end user with a predefined list of values in which he/she chooses one or several items.
Column	To provide the end user with a list of values from a column of the calculation view
Derived From Table	When you want the end user to have a set list of values from a table (not necessarily included in the view)
Derived From Procedure	When you want the parameter value to be passed to the calculation view based on the scalar output of a stored procedure
Direct (without semantic type)	When none of the above applies and/or when you want the user to enter a parameter without choosing it from a predefined list.
The Direct parameter type can be combined with a semantic type such as Date, Currency, or Unit of measure. This means that the value help will be based on these types of values. For example, if you specify Date, then a popup calendar will appear for the user prompt. If you specify Currency, then a list of valid currencies will be presented in the value help. This allows us to provide flexible input for the user, but it also allows us to control the type of values that are allowed.

Currency and Unit of Measure Semantic Types
For the Currency and Unit of measure semantic types, the list of proposed values will be created based on the corresponding reference tables in SAP HANA. This setup requires that the default schema assigned to the view contains the reference tables.

Input parameters support multiple values. This means that, at runtime, the end user has the possibility to provide several values to the parameter. Some examples of use cases include the following:

Applying filters of the types List of values and Not in List

Expression of calculated columns and expression of filters in projection nodes, provided that the expression requires a multi-value input

When you define an input parameter of the type, Derived from Procedure/Scalar Function, it is possible to map parameters to the input of the scalar function or procedure.

Note

Input parameters of the types Derived from table and Derived from Procedure/Scalar Function do not generate a prompt for the end user (they pass the parameter values directly), except if you select the Input Enabled option. In this case, the values returned by the table, procedure, or scalar function, can be modified by the end user.

Upon the execution of the calculation view, input parameters are passed inside the SQL query using a PLACEHOLDER clause - for example:

SELECT <columns> FROM <calculation_view_name> (placeholder."$$IP_YEAR$$"=>'2019') GROUP BY <group_by columns>;

Not all applications are able to pass values to the PLACEHOLDER. That is why sometimes you must use variables to achieve the same outcome.

Note

When you have defined an input parameter, you must then use it in an expression. Otherwise, it is ignored.

Creating Input Parameters
As discussed already, unlike a variable, an input parameter can be used in a conditional expression. For example, we can use an input parameter to determine which measure should be displayed in a particular column.

To illustrate this, we create a calculated column called AMOUNT, which can be filled with either the gross amount or the net amount, depending on the value that the user chooses when querying the view.

In our example, we have chosen to use an input parameter of the type Static List. This means that we predefine the allowed value that can be chosen by the user in a list. This is fine for short lists but, when the list becomes large, it becomes cumbersome to manage because you would have to edit the calculation view and rebuild it each time. Of course, you could choose the type Direct, which would mean the user could input anything. However, that would mean that apart from the user not having any guidance, he/she could also mistype the value, or enter the value in the wrong format (perhaps adding leading zeros when they were not required).

A good solution would be to define the input parameter with the type Column and then, in the View / Table Value Help, enter the name of a table or view where the allowed entries are presented. This also means that this list can be used by multiple input parameters and encourages central maintenance of the consistent, allowed values list.

Input Parameters
An example for an input parameter with a static list.
If we want the end user to decide whether Gross or Net amount should be shown in a view, the first step is to create an input parameter that will be used in a calculation.

The Input Parameter can be of any suitable type, for example a Static List type.

In this example, the user will be able to choose either "Gross Amount" or "Net Amount".

Default value GROSS will be assigned to the input parameter if the user does not specify anything.

An input parameter used within a formula does not necessarily have to be of the type Static List. For example, it can also be a Direct numeric value used in multiplication or any other calculation type.

Calling an Input Parameter in a Calculation
Calculated Column using an input parameter in its expression.
The next step is to use the Input Parameter in a Calculated column.

This is done by calling it within single quotes and double dollar signs.

In this example, the input parameter is used in the condition of an IF expression: if('$$GROSS_OR_NET$$'='Gross',"GROSS_AMOUNT","NET_AMOUNT")

If the user selects GROSS, the calculated column (of type Measure) will display the GROSS_AMOUNT measure in the AMOUNT column. Any other selection will result in NET_AMOUNT being displayed.

Input Parameter Using Dates
Screen capture of an input parameter with data type set to DATE.
An Input Parameter type of type "Direct" with a semantic type "Date" can be useful when you want to create calculations based on a date specified by the reporting user.

You can create a date range by creating a pair of input parameters (for example, "Date From" and "Date To").

Note that the Data Type must be set to "DATE".

Using a Calendar Dialog for Date Input Parameters
Screen capture of opening the value help for an input parameter. If it is of type Date, the end user can choose the appropriate date from a calendar after starting the value help in the preview dialog.
When using the type "Date" you are making it easier for the end user to select a date by utilizing a calendar dialog for selecting the appropriate date.

In the figure, the user is asked for a single value. Dates can also be selected as ranges.

Note

SAP recommends using input parameters instead of variables whenever possible, because input parameters provide more flexibility and features than variables. However, some front-end tools cannot process input parameters and so variables must be used.
Define Input Parameters to Pass Dynamic Values
Demo


...

Mapping Variables and Input Parameters
Objective

After completing this lesson, you will be able to map variables and input parameters.
Mapping of Variables and Input Parameters
Parameter Mapping
In some cases, your calculation view needs to pass information provided by the user, via a variable or input parameter, to another object. This could be another calculation view or even a procedure or function.

This is called parameter mapping and is an important feature of SAP HANA calculation view modeling.

There are four types of parameter mapping:.

Type	Description
Data Sources	Map input parameters of the underlying data source to input parameters of the calculation view
Procedures/Scalar Functions for input parameters	If you are using input parameters of type procedure/scalar function, and if you want to map the input parameters defined in the procedure or scalar function to the input parameters of the calculation view
Views for value help for variables/input parameters	If you are using input parameters or variables, which refer to external views for value help references and if you want to map input parameters or variables of external views with the input parameters or variables of the calculation view.
Views for value help for attributes	If you are creating a calculation view, and for the attributes in the underlying data sources of this calculation view, if you have defined a value help view or a table that provides values to filter the attribute at runtime.
To enable parameter mapping, you must use the Input Parameter/Variables Mapping feature. You can find this feature in the Parameters tab in the calculation view.

Use the Parameters tab to map input parameters from a source calculation view to the consuming calculation view or procedure. For more details, refer to the following text.
When you open the mapping pane, you must first select the type of mapping you want to work with using the dropdown selector.

When you make your type selection, you will then see (on the left side) the input parameters and variables that are defined in the calculation views from all lower layers in the stack, which are related to the mapping type you selected. On the right side, you will see the input parameters and variables that are defined in the current calculation view (the one you are editing).

Note

You can only map Variables to Variables and Input Parameters to Input Parameters. Cross-Mapping (such as an Input Parameter to a Variable) is not possible.
You simply drag a line between the left and right side to map them. There is also an auto-map feature which means that if the names are the same, the mapping is done with a single click. The auto-map feature generates the input parameters or variables for the current view with the same name as the source variables and also maps them. This means that you do not have to manually create the input parameters or variables in the current view.

From SAP HANA Cloud QRC 1/2024 onwards, it is possible to define an input parameter and at the same time propagate this parameter to other views within the same HDB module that consume this calculation view. Similarly, it is possible to delete an input parameter and propagate the deletion to the consuming calculation views.

Pushing Down Input Parameters and Variables to Lower Level Calculation Views
In many cases, calculation views use other calculation views as data sources. This is not necessarily confined to two levels; we can go on and layer the calculation views to create a stacked model. When you execute a calculation view in which variables or input parameters are defined, it is possible to pass their values (entered by the end user at runtime) to the lower level calculation views. In fact, the input parameters and variables at the lower levels are usually ignored unless you define input parameters and variables at the top level and map them to the input parameters and variables in the lower levels.

Illustration of a parameter mapping example, from a top level view to a data source calculation view. The user enters a country that is passed to the input parameter of the inner view to filter or calculate data.
Pushing filters down to the source views using parameter mapping is a common scenario. To enable this, choose the type Data Sources from the dropdown list in the Manage Mapping dialog.

Hint

Mapping parameters of the current view to the parameters of the underlying data sources moves the filters down to the underlying data sources during runtime, which reduces the amount of data transferred across them. This is a great way to improve performance.
Watch the following video to see an example of data sources parameter mapping:

Another common scenario is when you want to push parameters down from the main calculation view to a calculated column in a lower view to support a calculation. Again, this would be the type Data Sources.

Note that in the type of mapping Data Source, you only map input parameters to input parameters. In other words, a Variable defined in an underlying Calculation View cannot be mapped to a Variable defined in the current Calculation View. However, it is possible to access these variables from the Extract Semantics feature and copy them to the current view. To do that, you right-click the data source in the calculation scenario and choose Extract Semantics. Then choose the Variables tab and select the ones you want to copy to the semantics of your Calculation View.

If a data source for your calculation view is an SQL view and the SQL view requires parameters, you can provide the values from input parameters of the consuming calculation view.

Use the parameters tab to map a required parameter of an SQL view to a parameter of the current view. They can have different names.
This technique is often used to push down the filters down to the SQL view in order to reduce the workload of the Calculation View.

Note

Mapping Input Parameters from SQL views is possible from SAP HANA QRC 4/2021 QRC onwards.
From SAP HANA Cloud QRC 1/2022 onwards, it is possible to pass input parameters from a Calculation View to an SQL View or a Calculation View that is accessed remotely as a Virtual Table. This ensures that filters are pushed down and executed as close to the data sources as possible.

This new feature requires Smart Data Access (SDA), relying on the odbc adapter – it is not possible with remote sources accessed through the Smart Data Integration (SDI) adapter. The scenario supports remote SAP HANA databases running on SAP HANA Cloud, or SAP HANA On-Premise version 2.0 minimum.

Note

Besides, in SAP HANA On-Premise, Input Parameters in SQL Views are supported only from version 2.0 SPS02 onwards.
Mapping for Procedures or Scalar Functions Used in Input Parameters
In some cases, you need to calculate the value of an input parameter, not just take the value entered by the user. In this scenario you use a procedure or a scalar function to return the needed value. This procedure or function will then have an input parameter that should be the value entered by the user.

Illustration of a parameter mapping example, from a top level view to a procedure. The user enters a country that is passed to the procedure. the procedures uses it to calculate a customer value that is used for another input parameter in the top level view. This parameter is defined as derived from procedure.
Watch the following video to see an example of procedure parameter mapping:

Mapping for External Value Help Views used in Variables or Input Parameters
Another important use case for mapping input parameters and variables is to enable dynamic value help views.

When you define input parameters and variables, the default data source that generates the value help list is taken from the calculation view itself. So, essentially you are getting an unrestricted list of all possible values to choose from. However, you can also redirect the value help to use a list from another table or view. The main reason we do this is to expose a restricted value help list.

This is also good practice for performance because the value help is not competing with the main calculation view for data. For example, you could create a calculation view on a table that contains all possible products. Here, your calculation view could include a fixed filter expression that restricts the products to a specific product group. This means that the value help list presents only products of a specific product group to the user.

What if you wanted to change the product group? You could go back to the calculation view and change the fixed filter expression, but this would be inefficient.

What we should do is replace the fixed value in the filter expression with a variable based on product group. Then, we should map this variable to a variable we define in the main calculation view for the product group. This means that when a user is prompted for a product group in the main view, the value chosen is passed through the mapping to the value help calculation view, so that the products are filtered by the product group that was chosen. The list of products is then presented as the value help for the Product column. This is also known as cascading prompts. Cascading is not restricted to two levels; you can also cascade prompts across multiple levels. For example, you could prompt for product family, which then restricts the list of Product Groups, which in turn restricts the list of Products, and so on.

The architecture of cascading prompts example. A user chooses the product group, then a product of this group. To see only corresponding products, the variable for product uses a value help view with a product group variable that is mapped to the product group variable of the top level view.
To implement value help parameter mapping, you must select the option Views for value help for variables/input parameters from the dropdown list in the Manage Mapping dialog.

Note

An external view based on a hierarchy could also be considered as a value help cascading solution, and might be more visually appealing to the user.
Watch the following video to see an example of cascading prompt:

Mapping for External Value Help Views Used for Attributes
The last scenario is probably less common but can be useful. The result sent by your calculation view to your client reporting tool can be further filtered by the user. You might want the user to have a value help on certain attributes to allow easier selection. This can be achieved with assigning a value help to some attributes in the semantics node.

Again, you can use an external view with a variable to restrict the values presented to the user. You then need to map this variable with a variable on the main calculation view.

Illustration of a parameter mapping example, from a top level view to a view used for value help of an attribute. The user enters a currency that is passed to the value help calculation view. the view uses it to filter a list of products that is used for the PRODUCT column value help in the top level view. This value help can be used in the client tool to select a product.
Watch the following video to see an example of view for value help for attribute parameter mapping.

...

Knowledge quiz
It's time to put what you've learned to the test, get 3 right to pass this unit.

1.
What must you do if you want to use the IP_COUNTRY input parameter value to filter the value help of the IP_CITY input parameter?
Choose the correct answer.

Create a value help view for the IP_COUNTRY parameter, listing countries with a filter on Country using an input parameter. Then map this input parameter to the IP_COUNTRY parameter.

Create a value help view for the IP_CITY parameter, listing cities with a filter on Country using an input parameter. Then map this input parameter to the IP_CITY parameter.

Create a value help view for the IP_CITY parameter, listing cities with a filter on Country using an input parameter. Then map this input parameter to the IP_COUNTRY parameter.

Create a value help view for the IP_COUNTRY parameter, listing countries with a filter on Country using an input parameter. Then map this input parameter to the IP_CITY parameter.
2.
Variables can be defined on attributes and measures - true or false?
Choose the correct answer.

True

False
3.
Why might you create an input parameter instead of a variable?
There are two correct answers.

To filter data by measures

To pass a dynamic value to an expression in a calculated column

To filter data by attributes


......


Unit 10
Implementing Hierarchies in Calculation Views
After completing this unit, you will be able to:

Define a hierarchy to organize data for efficient navigation.
Modeling Hierarchies

...

Modeling Hierarchies
Objective

After completing this lesson, you will be able to define a hierarchy to organize data for efficient navigation.
Hierarchy
Hierarchies play a key role in data modeling as they provide an easy-to-use navigation aid when drilling through lots of data. As the user clicks on the hierarchy node, the node is expanded to offer even more lower level values. Typically, measures are accumulated from the lower level to the upper levels. A lot of business data is organized by hierarchy so this is a very popular topic for modeler.

Example of a hierarchy - juice and water is under beverage, burgers and soup is under food, beverage and food is under catering.
SAP HANA Cloud calculation views support two types of hierarchy: Level and Parent-Child.

The type of hierarchy you want to create will depend on the structure of the source data:

A level hierarchy requires each level to be stored in a separate column. Rows represent leaf nodes.
A parent-child hierarchy requires columns of the same data type for parents and children. Rows represent each node.
Source data tables for 2 different types of hierarchy. For a level hierarchy, a table with two columns with the same data type. For the Parent-Child hierarchy, a table with as many columns as the number of levels.
Note

Parent-child hierarchy columns usually contain IDs or key fields instead of plain text.

Hierarchy Comparison
Let's take a look at an example of each type of hierarchy:

Comparing two types of hierarchy. Example of a parent-child hierarchy : 4 has 2 as parent which has 1 as parent. 1 is also parent to 3. Example of a level hierarchy : Bayern and Hamburg have Germany as country and DACH as region.
Here are the main differences between the two hierarchy types:

Parent-Child Hierarchy
Distinct fields define the parent-child relation.
Parent and child fields usually have the same data type.
Parent and child fields usually have the same data type.
Level Hierarchy
Heterogeneous fields (possibly with different data types) are combined in a hierarchy.
Level Hierarchies
Procedure to Implement Level Hierarchies
A level hierarchy is defined by choosing columns from a source data set and organizing them in a sequence that provides a drill-down path. Each level is based on a separate column in the source data.

To implement level hierarchies, use the following procedure:

Step 1
Select the source tables(s) for the view.

Creation of a Level Hierarchy. Step 1: In the shown example, in the base join node, the PRODUCT and PRODUCT_GROUP tables are selected.
Step 2
Select the columns that should be part of the view, including any columns required for the hierarchy.

Creation of a Level Hierarchy. Step 2: Here the PRODUCT_ID, PRODUCT_TEXT, PRODUCT_GROUP and PRODUCT_GROUP_TEXT columns have been selected as output.
Step 3
In the Semantics node, select the Hierarchies tab and click the '+' button in the Hierarchy pane.

Creation of a Level Hierarchy. Step 3: The + button in the Hierarchies tab of the Semantics node is selected.
Step 4
Add the columns to the hierarchy in the correct level order from top to bottom, with the lowest granularity at the lowest level of the hierarchy.

Additionally, you can define an ascending or descending sort direction per level.

Creation of a Level Hierarchy. Step 4: the PRODUCT_LEVEL_HIERARCHY is defined. First level is PRODUCT_TEXT and second level is PRODUCT_TEXT.
Node Styles
Node styles are used to define the output format of a node ID.

Using a fiscal hierarchy example, the following table demonstrates the different node styles:

Node Styles
Level Style	Output	Example
Level Name	Level and node name	MONTH.JAN
Name Only	Node name only	JAN
Name Path	Node name and its ancestors	FISCAL_2015.QUARTER_1.JAN
Level Types
A level type specifies the semantics for the level attributes. For example, the level type TIMEMONTHS indicates that the attributes are months such as January, February, or March.

The level type REGULAR indicates that the level does not require any special formatting.

Hierarchy Member Order
Using the Order By dropdown list, an attribute can be selected for ordering the hierarchy members in the order specified in the Sort Direction column.

Orphan Nodes
An orphan node in a hierarchy is a member that has no parent member.

Level hierarchies offer four different ways to handle orphan nodes. They are as follows:

Root Nodes

Any orphan node will be defined as a hierarchy root node.

Error

When encountering an orphan node, the view will throw an error.

Ignore

Orphan nodes will be ignored.

Step Parent

Orphan nodes are assigned to a step parent you specify.

Parent-Child Hierarchies
When you are creating a parent-child hierarchy, the first step is to define the nodes that make up the hierarchy.

Creation of a Parent-Child Hierarchy. In the Hierarchies tab, the EMPLOYEE_HIERARCHY is defined. DNUMBER is selected as Child column and PARENT_DNUMBER is selected as Parent column.
The Child column contains the attribute used as the child within the hierarchy, whereas the Parent column contains the attribute used for its parent.

You can define multiple parent-child pairs to support the compound node IDs - for example:

CostCenter → ParentCostCenter

ControllingArea → ParentControllingArea

The preceding list of parent-child pairs constitutes a compound parent-child definition to uniquely identify cost centers.

Advanced Properties of a Parent-Child Hierarchy
Additional attributes can also be added to the hierarchy, making it easier to report on.

Creation of a Parent-Child Hierarchy. The Aggregate All Nodes property is selected. Orphan Nodes are defined to be added to Root Nodes. Root Node Visibility is set to Add Root node if Defined. Cycles property is set to Break up at load time.
Aggregate All Nodes
The Aggregate All Nodes property defines whether the values of intermediate nodes of the hierarchy should be aggregated to the total value of the hierarchy’s root node. If you are sure that there is no data posted on aggregate nodes, you should set the option to False. The engine then executes the hierarchy faster.

Default Member
The Default Member value helps you to identify the default member of the hierarchy. If you do not provide any value, all members of the hierarchy are default members.

Orphan Nodes
In a parent-child hierarchy, you might encounter orphan nodes without a parent. The Orphan Nodes property defines how these should be handled.

Note

If you choose to assign an orphan node to a step parent, the following rules apply:

The step parent node must be already defined in the hierarchy at the ROOT level.

The step parent ID must be entered according to the node style defined in the hierarchy.

Root Node Visibility
The Root Node Visibility property is used to define whether an additional root node needs to be added to the hierarchy.

Cycles
Cycles are typically not desirable in a hierarchy.

In such cases, the Cycles property is used to define how these should be broken when encountered, or whether an error should be thrown.

Time-Dependent Hierarchies
Time dependency is supported for more complex hierarchy data, such as human resources applications with their organizations, or material management systems with BOMs where information is reliant on time.

Caution

Defining a time dependency is only possible in calculation views, and for parent-child hierarchies.

Creation of a Parent-Child Hierarchy. In the TIME DEPENDENCY section, Enable time dependency is selected. Valid From and Valid To columns need to be defined. You can choose to query the hierarchy using either a date range or a single specific date.
Enabling time dependency supports hierarchies based on changing elements valid for specific time periods. This allows displaying different versions of a hierarchy.

Your source data needs to contain definition columns consisting of a Valid From and a Valid To column.

Determining the Validity Period
When a hierarchy needs to show elements from an interval, you have to define two input parameters; a From Parameter, and a To Parameter. If the hierarchy needs to show elements valid on a specific date, you need one input parameter defined as the Key Date.

Create a Parent-Child Hierarchy
Demo
Start Demo
Type of Generated Hierarchy
When you define a hierarchy in an SAP HANA Cloud calculation view, whether it is a level or parent-child hierarchy, the hierarchy is materialized upon build/deployment by various tables and/or views in the HDI Container schema (column views or classic SQL views) or in other locations, especially the BIMC* tables and views in the _SYS_BI schema. These generated objects provide detailed hierarchy node relationship data to enable processing of the hierarchy when the front-end tool is not able to generate the hierarchy relationships itself.

A key setting allows you to influence the way SAP HANA translates the defined hierarchy into technical tables and views in the database. This is the Hierarchy Type setting, which you can define in the Semantics of the Calculation View, in the View Properties → General tab.

The setting can take three values:

Auto (default value)
This setting is useful if you exchange views between SAP HANA Cloud and On-Premise, because upon build, SAP HANA generates the following:

In SAP HANA Cloud: classic SQL views to materialize the SQL hierarchy

In SAP HANA On-Premise: MDX hierarchies (only compatible with SAP HANA on-Premise), including the metadata defining the hierarchy, as well as column views materializing the MDX hierarchy. No SQL Hierarchy view is generated.

Note

In SAP HANA Cloud, the setting Auto has exactly the same effect as the following SQL Hierarchy Views. The key benefit is that you can transfer Calculation Views that have the Auto setting from SAP HANA On-Premise to SAP HANA Cloud without a need to change their properties. For more details about SQL hierarchies in analytical queries, you can consult SAP Note 3139372.
SQL Hierarchy Views
Upon build, classic SQL views are generated to materialize the SQL hierarchy.

No Hierarchy Views
Upon build, the metadata of the hierarchies is generated (_SYS_BI.BIMC* tables) but the hierarchies themselves (detailed list of members, and so on) are NOT generated.

This setting is should be used when the consuming front-end tool itself can generate the set of hierarchy members and their relationships, based on the hierarchy metadata defined in the BIMC tables.

The table below summarizes which objects and references are generated based on the chosen option.

Hierarchy Type
 	Auto	SQL Hierarchy Views	No Hierarchy View
Hierarchy Meta-data (records in the BIMC* tables in schema _SYS_BI)	Yes	Yes	Yes
SQL Hierarchy (classic SQL View in the container schema)	Yes	Yes	No
Name of a Calculation Views and its Associated SQL Hierarchy Views
Let's take an example to show how the various objects related to a Calculation View are named in the corresponding HDI Container schema.

Assume one level hierarchy, PROD_LEV_HIER, has been modeled within the Calculation View HC300::CVC_SALES_HIER.

Then the SQL Hierarchy view is called HC300::CVC_SALES_HIER/PROD_LEV_HIER/sqlh/PROD_LEV_HIER.

...

Knowledge quiz
It's time to put what you've learned to the test, get 1 right to pass this unit.

1.
What are two types of modeled hierarchy that you can create using calculation views?
There are two correct answers.

Parent-Child

Referential

Level

Temporal

......

Unit 11
Developing Custom Logic using SQL
After completing this unit, you will be able to:

Write and execute SQL in the SQL Console.
Implement SQL in a calculation view.
Query a modeled hierarchy using SQL.
Describe the additional features provided by SQLScript compared to standard SQL.
Describe how functions can be consumed by calculation views.
Introducing SAP HANA Cloud SQL Console
Implementing SQL in Calculation Views
Querying a Modeled Hierarchy Using SQL
Working with SQLScript
Creating and Using Functions

...

Introducing SAP HANA Cloud SQL Console
Objective

After completing this lesson, you will be able to write and execute SQL in the SQL Console.
Working with the SQL Console
Importance of SQL in SAP HANA Modeling
SQL plays an important role in SAP HANA Cloud modeling and can be implemented in different modeling objects, including the following:

Expressions in calculation views

Procedures

Functions

SQL-based analytic privileges

Screenshot of a Calculation View scenario showing a Table function as source. In the projection node above it, a calculated column is created. The Expression should be written in SQL.
SQL is used to implement custom logic in calculation views where standard features of the graphical calculation view are not sufficient. SQL provides a very large number of functions that can provide users with access to complex data processing tasks.

SAP HANA Cloud modelers should acquire skills in the SQL language so that they can be fully effective in projects. In addition to writing custom logic to be embedded in calculation views, modelers should understand the behavior of calculation views at runtime when SQL is generated from the calculation view, and to use the debugging tools that display the generated SQL.

Caution

SQL should not be considered as an alternative approach to graphical modeling. For example, you should NOT create an SQL-based object (function or procedure) to achieve a result that a graphical Calculation View can achieve, even if you have advanced SQL knowledge. Calculation views provide more optimization possibilities than SQL as they generate their own SQL code at runtime based on the variety of query conditions and implement advanced pruning to generate the leanest code to provide the best performance.
Launching the SQL Console
The SQL Console, which is available in the Database Explorer, is used to write and execute SQL statements.

Although the SQL Console is ideal for executing one-time instructions, such as creating a temporary table or executing a test query, it is not recommended for generating persistent modeling objects such as functions, procedure or tables that will become part of an application. For this, we recommend you insert your SQL code in a source file that corresponds to the type of object you wish to create, for example:

To create a table, always use source file type .hdbtable.

To create a function, always use source file type .hdbfunction.

When you deploy the source file, the corresponding runtime database object is generated.

Nevertheless, the SQL Console is an important tool for testing and executing one-time statements, so let's take a look at it.

Steps to use the SQL Console. 1- Icon to open the SQL Console. 2- SQL Editor to write SQL. 3 - Icon to execute SQL. 4- Result Pane.
The steps to use the SQL Console are as follows:

Open SQL Console
Write SQL
Execute
View results
The SQL Console is a feature of the Database Explorer, which is launched from SAP Business Application Studio. The Database Explorer a separate tool from SAP Business Application Studio. The Database Explorer opens in a new browser tab.

Each time you select the SQL Console, a new tab appears inside the Database Explorer where you write your code. You can open as many SQL Console tabs as you wish.

Note

You might notice another button: Open SAP HANA SQL Console. This provides most of the functions of the standalone Database Explorer, but it is conveniently embedded in SAP Business Application Studio and does not open the separate tool. A new SQL Console tab opens in SAP Business Application Studio, in this case. This version does not contain the full feature set of the standalone Database Explorer tool, but might be sufficient for your needs.
The Database Explorer can contain many database connections that point to different parts of the database. When you launch an SQL Console you need to be aware of which database connection you are using for that console because the executed SQL will run against that database connection. For example, if the SQL statement you execute refers to a table that cannot be found, it might be that the table does not exist under the database connection you are using. If you change the database connection, you might then find the table.

To launch the SQL Console, choose the Open Database in Database Explorer button next to the database connection you would like to use. This opens up the Database Explorer in a new browser tab.

You can clearly see which database connection you are currently using because this is always displayed at the top of the screen.

Regardless of which connection you choose, you can easily swap the connection by using the button in the toolbar Connect this SQL console to a different database. This is useful when you want to test your SQL against a alternative connection that points to a different set of tables without having to re-write the SQL.

HDI or Classic Database Connections
There are two main types of database connection:

HDI Container

SAP HANA database (also known as catalog)

It is important that you understand which type you are writing statements against in the SQL Console because this affects how you write the SQL code.

A HDI container is generated automatically the first time you deploy your database module in your development project. The container is basically a hidden schema. Later in the course, HDI containers are covered in more detail.

Examples of SELECT statement with explicit schema reference (for classic catalog access) and without explicit schema reference (for HDI containers).
The key thing to remember is that when executing statements against an HDI Container, you do not need to specify the schema. The schema is already known to the container you are running the statement against.

However, if you are executing statements against a connection that is an SAP HANA Database catalog type, then you must specify the schema in your statements.

Access and work with the SQL Console
SQL Console Basics
Watch this video to learn about SQL Console Basics.


...

Implementing SQL in Calculation Views
Objective

After completing this lesson, you will be able to implement SQL in a calculation view.
SQL Expressions in Calculation Views
Expressions in calculation views are written in SQL. You write expressions in the following:

Calculated columns

Filters

Restricted columns

In addition, SQL can be used to generate values for input parameters. For example, SQL can determine the top 5 customers of the current month and pass these to the input parameter of the calculation view to be used as a filter. This is a very powerful feature because input parameters are used throughout calculation views to provide dynamic values to filters, ranking thresholds, pre-filling user prompts, calculations, and so on.

The Expression Editor
The Expression Editor is used to create the SQL Expression.

You can add any of the proposed components to your expressions:

Elements: Columns, Parameters, etc.
You can also access value help to include specific column values in the expression.

Functions
Functions are displayed in categories (Date, String,...) and you can filter on keywords.

Operators
Once you have finished writing the expression, you can check the syntax.

SQL Expression Editor. Select the Component, search or expand the list, click items to add them to the expression and check the syntax.
One of the key benefits of using SQL is that it provides a long list of ready-made functions. These functions can provide significant additional options when you are trying to develop some logic where data must be manipulated. For example, SQL provides many string manipulation functions that are useful for re-formatting a field, or extracting some characters from it. In addition, there are many predefined functions available that can help you to calculate using dates. An example of this is finding the number of days between two dates.

Note

SQL expressions in calculation views uses plain SQL and not the syntax of the SAP enhanced version, SQLScript.
From QRC 1/2023 onwards, it is possible to include User-Defined Functions in the expression of Calculated Columns and Filters. This feature can help in scenarios where you want to reuse logic in different calculation views or pre-process values of input parameters before using them in expressions.

The Expression Editor lists all the User Defined Functions (either local or from other containers accessed via synonyms). Select one to add it to your expression.
You can access both User-Defined Functions created inside your local container, and the ones from an external HDI container that are made available using a synonym.

As in the case for expression components from the other tabs, you can choose an object from the UDFs tab to add it to the Expression pane.

...

Querying a Modeled Hierarchy Using SQL
Objective

After completing this lesson, you will be able to query a modeled hierarchy using SQL.
Query Hierarchies using SQL
A hierarchy that is defined in a calculation view can also be accessed directly by SQL statements. The benefit of this is to provide access to the node values at any level of the hierarchy in your SQL code. You read the nodes as if they were columns in a data source.

For example, you want to calculate the total number of absence days for the Service line of business. The service line of business sits within the organization hierarchy and has many levels below it that represent all departments. If you roll up all the lower levels we can calculate the total for the service line of business.

A simple SQL statement such as : SELECT sum (absence_days) WHERE hier_node = ‘Service’ clause, would provide this value. We do not need to read all the lower levels separately to reach the total. The lower levels are automatically rolled up.

But before you can write the SQL query, you need to work on the following setup:

The hierarchy you wish to query must be created in a dimension calculation view. The dimension calculation view is then consumed in the star join node of a cube with star join calculation view. This is because only shared hierarchies can be read by SQL.

You must then enable the shared hierarchy to be exposed to SQL by setting a parameter in the Properties tab of the semantics node of the cube with star join calculation view.

Screenshot of a Calculation View properties. The Enable Hierarchies for SQL access property is selected.
In the SQL Access section of the Hierarchy tab of the semantic node, you should locate the name that is given to the node column, because you will need to refer to this in the SQL statement.

This name can be overwritten so that a more meaningful name can be supplied.

Screenshot of the Hierarchies tab. In the SQL Access section you can find the Node Column name. The default name for H_GEO hierarchy is H_GEOnode.
Reading hierarchies using SQL is supported for both level and also parent-child types.

Note

The Hierarchy Expression Parameter is not used in the current release of SAP HANA Cloud. Ignore this setting.
Aggregating Values in a Hierarchy using SQL
The following figure shows you how the SQL is written and how the result would appear. Simply refer to the generated hierarchy node column name in the SELECT statement as if it were a regular column in the data source.

The node column can be used in the GROUP BY clause. The result then shows the aggregated measure for each node of the hierarchy.

Result of an SQL statement summarizing the Amount column grouped by hierarchy node.

...

Working with SQLScript
Objective

After completing this lesson, you will be able to describe the additional features provided by SQLScript compared to standard SQL.
SQLScript Capabilities
SQLScript is a database language developed by SAP. It is based on standard SQL but adds additional capabilities to exploit the advanced features of SAP HANA Cloud.

Here are some of the key features of SQL Script:

SQLScript extends ANSI-92 SQL. Just like other database vendors extend SQL in their databases, SAP HANA extends SQL using SQLScript.
SQLScript enables you to access SAP HANA-specific features like column tables, parameterized information views, delta buffers, working with multiple result sets in parallel, built-in currency conversions at database level, fuzzy text searching, spatial data types, and predictive analysis libraries.
SQLScript allows developers to push data intensive logic into the database.
SQLScript encourages developers to maintain algorithms using a set-oriented paradigm, instead of a one record at a time paradigm.
SQLScript does however allow looping through results and using if-then-else logic
SQLScript allows you to break complex queries into multiple smaller statements, thereby simplifying your SQL coding, and enhancing parallelism via the optimizer.
Note

The version of standard SQL that SQLScript is based on is the popular ANSI-92 version which is supported by nearly all databases.

Additional features of SQLScript include working with SAP HANA Cloud column tables, passing parameters between calculation views and SQLScript, working with spatial data types, and predictive analysis libraries.

Developing Business Logic with SQLScript
In traditional client-server approaches, the business logic is defined and executed in the application server using application programming languages such as ABAP or C++. With SAP HANA Cloud, much of this logic can now be defined using SQLScript, which allows the developer to define and execute all data processing tasks in the database in order to achieve improved performance.

SQLScript allows you to use variables to break a large complex SQL statement into smaller, simpler statements. This makes the code much easier to understand. It also helps with SQL HANA Cloud’s performance because many of these smaller statements can be run in parallel.

Watch this video to look at an example:

In the studied example, we want to find out which publisher has the most books:

Code Snippet

Copy code

Switch to dark mode
books_per_publisher = SELECT publisher, COUNT (*) AS num_books FROM BOOKS GROUP BY publisher;
publisher_with_most_books = SELECT * FROM :books_per_publisher WHERE num_books >= (SELECT MAX (num_books) FROM :books_per_publisher);
The SQLScript compiler and optimizer determine how to best execute these statements, whether by using a common sub-query expression with database hints or by combining this into a single complex query. The code becomes easier to write and understand, more maintainable, and developer productivity increases.

By breaking the SQLScript into smaller statements and filling table variables, you also mirror the way in which you have learned to build your calculation views. Just like when you start building calculation views in layers, starting from the bottom up, you do the same with your SQLScript code. Keep in mind that the precise sequence of your SQLScript steps is not necessarily the runtime order, because the optimizer will always decide on the best execution plan. However, the optimizer will never alter your desired outcome. This is also true for calculation views.

The Use of Variables
The use of variables in SQL statements is not specified by the ANSI-92 SQL. In SAP HANA Cloud, SQLScript implements the concept of variable as an extension to ANSI-92 SQL.

Variables can be declared, assigned, and reused within a script.

Using Variables in SQLScript
Declare a variable

DECLARE <variable_name> <type> [NOT NULL] [= <value>]

Example 1: DECLARE a int;

Example 2: DECLARE b int = 0; (the value is assigned when creating the variable)

Assign a variable (define the value of a variable)

<variable_name> = <value> or <expression>

Example 1: a = 3;

Example 2: b = select count(*) from baseTable;

Use a variable in an expression

:<variable_name> (with a colon) returns the current value of the variable

Example: b = :a + 5; (assigns the value of a + 5 to b)

SQLScript Data Types
SQLScript also adds extra data types to help address missing features from standard SQL - for example, spatial data types and text data types.

ANSI-92 and SQLScript Data Types
Classification	Standard ANSI-92 Data Types	SQLScript-specific Data Types
Date/Time	DATE, TIME, TIMESTAMP	SECONDDATE
Numeric	DECIMAL, INTEGER, SMALLINT, FLOAT, REAL, DOUBLE	TINYINT, BIGINT, SMALLDECIMAL
Character string	CHAR, NCHAR, VARCHAR, NVARCHAR	ALPHANUM, SHORTTEXT
Binary	BINARY, VARBINARY	 
Large Object	 	BLOB, CLOB, NCLOB, TEXT, BINTEXT
Boolean	 	BOOLEAN
Spatial	 	ST_POINT, ST_GEOMETRY
Processing Input Parameters Defined in Calculation Views with SQLScript
When you use a select statement in SQLScript to query a calculation view that has input parameters, you use the PLACEHOLDER keyword with the name of the input parameter wrapped in $$ symbols, and the value you want to pass to the input parameter - for example:

Passing input parameters to the calculation view
Use the PLACEHOLDER keyword to pass input parameters to the calculation view in a select statement:

Code Snippet

Copy code

Switch to dark mode
SELECT "PRODUCT", sum("TAXED_AMOUNT")
      FROM "_SYS_BIC"."MyPackage/SalesResults" 
       ('PLACEHOLDER' = ('$$Tax Amount$$', '17.5')) 
GROUP BY "PRODUCT"  
Referring to the input parameter in SQL Script
Within SQLScript, you refer to the input parameter using a colon:;

Code Snippet

Copy code

Switch to dark mode
Sales with tax = Sales Revenue * :Tax_Amount 
It is also possible to pass output parameters generated from an SQLScript, into a calculation view using the parameter mapping feature.

Looping and Conditional Logic
In general, SQL code is written in a declarative way. This means that you state the intent of the action and do not dictate how the data processing steps should be carried out in a particular order. We do this so that you allow the SQL optimizer to figure out the best execution plan based on many dynamic factors at runtime.

However, sometimes you need to control the flow logic of the SQL so that steps are performed in a particular order. For example, when you need to perform a step only if a certain condition is met, or you need to loop a number of times. In these cases, you will use imperative logic.

This is (mostly) not available in ANSI SQL, and therefore SAP HANA provides declarative logic language as part of SQLScript to provide flow control.

A lack of imperative logic language is what often causes the developer to move the processing of data to the application layer. When an application runs on HANA, you really should push as much data processing as possible to the database. SAP HANA SQLScript imperative logic enables this so less data handling needs to be done in the application layers and more can be done in the database.

Hint

Remember that imperative logic cannot have the same performance as declarative logic. This is because the SQL optimizer is not free to decide on the sequence of steps and has to take care to follow your logic, which might destroy optimizations. If you require the best performance, try to avoid imperative logic.
Conditional Logic
The IF statement
Code Snippet

Copy code

Switch to dark mode
IF <bool-expr1> 
THEN 
  {then-stmts1} 
{ELSEIF <bool-expr2> 
THEN 
  {then-stmts2}} 
{ELSE 
  {else-stmts3}} 
END IF 
Example of an IF statement using an ELSE clause.
The previous figure shows you an example of the IF statement, which consists of a Boolean expression, :found is NULL. If the expression is evaluated as true, then the statement CALL ins_msg_proc('result of count(*) cannot be NULL'); in the mandatory THEN block is executed. The IF statement ends with END IF. The remaining parts are optional.

If the Boolean expression :found is NULL does not evaluate as true, then the ELSE branch is evaluated. In most cases this branch starts with ELSE. The statement CALL ins_msg_proc('result of count(*) not NULL- as expected'); is executed without further checks. After an else branch, no further ELSE branch or ELSEIF branch is allowed.

Alternatively, when ELSEIF is used instead of ELSE, another Boolean expression can be evaluated. If It evaluates as true, the following statement is executed. You can add an arbitrary number of ELSEIF clauses in this way.

This statement can be used to simulate the switch-case statement known from many programming languages.

WHILE and FOR Loops
The WHILE loop
Code Snippet

Copy code

Switch to dark mode
WHILE <bool-stmt> DO
  {stmts}
END WHILE
The WHILE loop executes the statement stmts in the body of the loop as long as the Boolean expression at the beginning bool-stmt of the loop evaluates as true.
The FOR loop
Code Snippet

Copy code

Switch to dark mode
FOR <loop-var> IN {REVERSE} <start>..<end> DO
  {stmts}
END FOR
The FOR loop iterates a range of numeric values – denoted by start and end in the syntax – and binds the current value to a variable (loop-var) in ascending order. Iteration starts with value start and is incremented by one until the loop-var is larger than end.
Therefore, if start is larger than end, the body loop will not be evaluated. For each enumerated value of the loop variable the statements in the body of the loop are evaluated. The optional keyword REVERSE specifies to iterate the range in descending order.

...

Creating and Using Functions
Objective

After completing this lesson, you will be able to describe how functions can be consumed by calculation views.
Introducing Functions
Functions allow developers to define a reusable block of data processing logic using the powerful SQLScript language to generate a result as either a single value, or a complete data set. For a modeler, functions are interesting because they can be used in calculation views to provide custom logic when the standard features of graphical calculation view editor are insufficient.

Functions can consume input parameters defined in calculation views. This means that the same function can be reused in different calculation views.

SAP HANA Cloud supports scalar and table functions. Table and scalar functions are created as source files in a project folder. The source file has the extension .hdbfunction The same extension is used for table and scalar functions.

Functions, both scalar and table, are always read-only. This means it is not possible to use any data definition language such as create table or alter table. It is not possible to use data manipulation statements such as update or insert into. Functions never change the database.

Functions can call other functions, which encourages a modular design approach to maximize reuse.

Scalar Function
One of the common uses of a scalar function in a calculation view, is to derive values for input parameters.

Illustration of a scalar function used as value for an input parameter.
A simple example of a scalar function could be to return the current date. However, scalar functions can be more complex and can read tables and call other database objects, including other functions.

A scalar function returns one or more parameters but each parameter always has only a single value. Scalar functions often have one or more input values, but these are not mandatory. For example, returning the current date requires no input parameter, whereas if you wanted to return the date that was x days earlier than today, then you would need to provide x as the input parameter.

The following is a breakdown of a scalar function’s code:

Syntax of a scalar function definition.
Notice the use of the keyword RETURNS for scalar functions. The keyword RETURNS TABLE is used for table functions. In this example, we are generating two output parameters: RESULT_ADD and RESULT_MUL.

Both output parameters are defined as the Double data type to handle floating point integers.

Table Functions
Table functions are used to return a tabular data set as opposed to a single value parameter. A tabular data set can consist of one or more columns and usually has multiple values (rows). Table functions are commonly used by modelers to define a customized data source in a calculation view. You use a function wherever a data source can be defined, such as in a calculation view Projection node or Join node just as you would include a table. In fact, it might help to think of them as dynamic tables that are created at runtime.

Illustration of a table function used as data source in a Join node.
As an example of a table function, consider that you may need to read through a table that contains sales orders so that you can find open orders that have been recently created. The date used for the determination of the required orders is based on an input parameter passed from the calculation view. The resulting open orders are passed to the calculation views as a data source.

The following is a breakdown table function’s code:

Syntax of a table function definition.
Notice the use of the keyword RETURNS TABLE for table functions. Remember, the keyword RETURNS is used for scalar functions.

All output parameters of the table function are offered as input columns to the calculation view node. If your table function requires input parameters you can first define input parameters in the calculation view and then use the input parameter mapping function to map them.

In a calculation view, it is possible to choose a table function as a data source to any node, just as if you were choosing a table. However, in the calculation view editor, there is also a dedicated node: Table Function. This node is specially dedicated to the handling of the different types of mapping of the input parameters of a table function.

Screenshot of a Calculation View scenario using a Table Function node.
There are three types of input mapping available in the Table Function node. They are as follows:

Data Source Column
First, add a data source to the Table Function node. Then, map one of the data source columns to one or more table function input parameters.

Input Parameter
Choose an input parameter from the calculation view and map it to one or more table function input parameters. This could be useful when you want a user to manually enter a value at runtime.

Constant
Manually define a single, fixed value, and map it to one or more table function input parameters. This is really only useful when the input value rarely/never changes.

Note

You should also be aware of Procedures that have many similarities with functions. Just like functions, procedures provide a way to define re-useable, custom SQLScript code for complex data processing. A key advantage of procedures is that they support data definition and data manipulation language that alter the database. However, a modeler rarely needs to alter the database and simply needs to read data. Unlike functions, you cannot use a procedure as a data source in a calculation view. So why should a modeler learn to define procedures? It is because you can define a flexible analytic privilege using SQLScript logic defined in a procedure. See the SAP Help documentation for more details on procedures.
Consume a Table Function in a Calculation View
SQL Function
Watch this video to learn how to consume a Table Function in a Calculation View.



...

Knowledge quiz
It's time to put what you've learned to the test, get 5 right to pass this unit.

1.
Why is knowledge of SQL important to an SAP HANA modeler?
There are two correct answers.

So that they can develop calculation views using SQL instead of using the graphical editor

So that they can develop custom logic using an SQL function and consume this within a calculation view

So that they can extend the capabilities of a calculation view using SQL expressions
2.
What types of user-defined SQL functions can you include as a data source in a calculation view node?
Choose the correct answer.

Table Functions

Column Engine Functions

Scalar Functions
3.
In a calculation view, why might you implement SQL code?
There are three correct answers.

To generate values for input parameters

To define filter conditions for restricted columns

To define a calculated column

To create a level hierarchy
4.
Why should I define a table using a source file rather than using the SQL statement ‘CREATE TABLE’ in the SQL Console?
Choose the correct answer.

The syntax is simpler

Performance of the table is better.

The table definition is easily transported as part of the overall application to ensure all project artifacts are kept together.
5.
How does SQLScript extend standard SQL?
There are two correct answers.

By providing table variables to break up larger statements in to smaller ones

By allowing JavaScript language to be used

By providing conditional statements such as IF..THEN..ELSE
6.
In which type of calculation view do you create a hierarchy so that it can be read by SQL?
Choose the correct answer.

Cube

Dimension
Next unit

......

Unit 12
Applying Best Practices for Modeling
After completing this unit, you will be able to:

Implement recommended modeling practices.
Implement best practices in calculation view nodes.
Implement Recommended Modeling Practices
Implement Best Practices in calculation view nodes

...

Implement Recommended Modeling Practices
Objective

After completing this lesson, you will be able to implement recommended modeling practices.
Understand the Instantiation Process
Before we dive into the detail of how to improve the calculation view performance, it is helpful to learn a little about the way a calculation view responds to a query that calls it.

A schema of the optimization process for queries on stored models. First, the calculation engine instantiates a run-time model for the query, then optimizes it, and finally the SQL optimizer takes over. For details, refer to the following text.
The calculation engine pre-optimizes queries before they are worked on by the SQL optimizer.

A calculation view is instantiated at run-time when a query is executed. During the instantiation process, the calculation engine simplifies the calculation view into a model that fulfills the requirements of the query. This results in a reduced model that can be further optimized by the calculation engine. For example, it considers settings such as dynamic join, join cardinality and union node pruning.

After this, the SQL optimizer applies further optimizations and determines the query execution plan - for example, it determines the optimal sequence of operations, and also those steps that could be run in parallel.

Details of the instantiation process. The query that calls the calculation view generates a pruned calculation view that reads only required parts of the original calculation view. It does not contain attributes or measures that are not part of the query.
The instantiation process transforms an original (one that you define) calculation view into an execution model based on a query that is run on top of a calculation view. The generated view is pruned of unnecessary elements from the original calculation view and is technically a column view that references one specific node of the original calculation view.

During the instantiation process, the query and the original calculation model are combined to build the optimized, execution calculation model.

Best Practices for Modeling
In this lesson, we will cover recommendations and best practices for data modeling with SAP HANA Cloud.

General Design Recommendations
Screen capture of a huge calculation view and how it could be split into smaller chunks. You should identify recurring patterns and then split models into smaller parts to avoid monolithic views.
Break down large models into individual calculation views so that they are easier to understand and also allow you to define reusable components, thus avoiding redundancy and duplicated maintenance as a consequence.

Try to develop calculation views in layers so that each layer consumes the lower layers. Any changes to the lowers layers immediately impacts the higher layers. The lowest layers should provide the foundation calculation views. The higher layers add increasing semantics to provide more business meaning to the views.

Note

Do not be concerned that breaking up large calculation views into individual building blocks will damage overall performance. At runtime, the SQL optimizer attempts to flatten the entire stack of calculation views into a single executable statement. Remember the calculation view definitions, however many there are in the stack, represent a logical view of the data flow. How the optimizer constructs the final SQL query can never be known at the design stage by simply observing the calculation view definitions. Use SQL tools such as SQL Analyzer to check the generated SQL, if required.

Aim for Query Unfolding
You should aim for query unfolding. The idea is that query objects should be directly accessible to the SQL optimizer on a detailed level. The advantage is a faster run-time performance of unfolded plans, but the compilation time of the query may increase.
At runtime, your calculation view is analyzed by the SQL processor. A key aim is to convert each calculation view operation into a corresponding SQL operator to convert the complete data flow graph into a single SQL statement. This is called unfolding. A benefit of unfolding to SQL is that only the SQL engine is needed and no other SAP HANA engine is called, such as the Calculation Engine or other SAP HANA Cloud engines such as Spatial or Graph. However, due to your calculation view design, there might be steps that cannot be translated into SQL operators. If these steps cannot be translated, they will be processed in the specialist SAP HANA engines which generate one or more materialized column views. These materialized column view are then read by SQL. So this means, instead of SQL reading directly from source tables, column views have to be first generated and then the SQL has to read from the interim column views. That would result in a partially unfolded query that might not perform as well as a completely unfolded query due to limited SQL optimizations.

Some nodes and operators cannot be unfolded when there is no equivalent SQL operator for the calculation view feature you are implementing.

It should be emphasized that although unfolding is generally desirable, sometimes unfolding can actually damage overall performance. For example, query compilation time can increase when the query has to be unfolded each time the calculation view is called with new selections. Therefore, it is possible to set a SQL hint that prevents unfolding for individual calculation views. This is found under Semantic Node → Properties → Advanced.

You can set a global parameter to prevent unfolding for all calculation views.

You can check to see if your calculation view is unfolded by using the Analyze > Explain Plan in the SQL Console, where you can see the list of operators and data sources.

How you check unfolding: Screen captures of the SQL Analyzer for an unfolded query and for a folded query. The unfolded query accesses a column table. The folded query accesses column views.
If a column view is used as a data source instead of a table, then you know this is not an unfolded query as the result has been materialized as an interim view and not read directly from a table. A completely unfolded query only accesses tables.

Caution

In some scenarios, the operator name can also be TABLE SCAN, which also reveals that the underlying object "physical" object has been accessed, meaning that this branch of the scenario has been unfolded.
In addition, check the query compilation time versus the execution time to see how unfolding is affecting the overall runtime.

It is possible to identify how many queries were not unfolded. Run a query on system view M_FEATURE_USAGE as follows:

Code Snippet

Copy code

Switch to dark mode
SELECT
 	FEATURE_NAME,
 	CALL_COUNT 
FROM 
	"M_FEATURE_USAGE" 
WHERE 
	COMPONENT_NAME = 'CALCENGINE'
AND
	FEATURE_NAME 
IN 
  ('QUERY TOTAL','QUERY UNFOLDING')
Use Dedicated Views or Tables for ‘List of Values’
For performance issues, we generally recommend that you create dedicated calculation views or tables to generate the list of help values for variables and input parameters.

Schema of a value help view that reads product master data. The value help view is applied to a view on Sales data and anohter view on Purchases data. Advantages of using value help views are faster value-help dialogs, consistent lists of variables across consuming views, and support for Analytic Privileges in the sense that users see their list of allowed values.
This approach enables a faster display of the list of values without reading the entire view or table on which the calculation view is based.

This is also best practice so that the list of values is reusable and consistent from one calculation view to another, and users see the same selection values.

The list can be driven by simple fixed filters in the help of the calculation view or even by analytic privileges that offer a dynamic list restricted to what each user is allowed to choose from in the interface.

...

Implement Best Practices in calculation view nodes
Objective

After completing this lesson, you will be able to implement best practices in calculation view nodes.
Best Practices in Calculation View Nodes
In this lesson, we will cover recommendations and best practices for nodes in your calculation views.

Column Pruning
Recommendation for column mapping and pruning. Verify that the intermediate sets are as small as possible. Therefore, investigate which attributes are requested on each operations and why. For more details, refer to the following text.
Make sure that you do not map columns that are not needed. This might sound like a fairly obvious recommendation, but it is very common for developers to simply map all columns without considering if and why there are used. They are often added 'just in case' they are needed later. Too many attributes can lead to very large, granular data sets especially when only aggregated data is needed.

Optimal Filtering and Calculated Columns
Recommendation for filtering and column calculations. For details, refer to the following text.
Always try to filter the stack as early as possible. The objective is to prune data as early as possible to avoid large data sets that can harm performance.

Avoid including calculated columns in filter expressions. It is better to first create a new column based on the calculation, and then add a projection node on top to filter the result set based on the column (which is no longer calculated).

Calculate as high as you can in the stack so that you avoid calculating on granular data and instead calculate on aggregated data.

Switch Calculation View Expressions to SQL
You should rewrite expressions that use column engine language to SQL language. A screen capture here shows how you can use the case function instead of an if clause.
Expressions are used in many places within calculation views, including calculated columns, restricted measures, and filter expressions.

For SAP HANA on-premise, it is possible to write expression using either plain SQL or the native HANA language called column engine language. SAP recommends that you only use SQL and not column engine language to ensure best performance (Column Engine language limits query unfolding).

When you build calculation view expressions on SAP HANA Cloud, you can only choose SQL expression language. Column engine language is not available in SAP HANA Cloud.

Calculation views that were imported to SAP HANA Cloud from SAP HANA on-premise and include column engine language expressions will still run, but you should change the expression language to SQL from the column engine, to achieve better performance. When you select SQL language, the language selector is then grayed out so that you cannot return to Column Engine language.

You should be able to recreate column engine expressions to plain SQL. For example, the concatenate operator + is replaced with the plain SQL operator || or you can use the CASE keyword instead of If...Then.

Optimal Aggregations
Recommendations for aggregations. For details, refer to the following text.
Always reduce the data set by introducing aggregations early in the modeling stack.

If you consume a calculation view that produces attributes, do not change them to measures and vice versa.

Choose a cube with star join to build dimensional OLAP models. Do not try to create OLPA models using standard join nodes which creates a relational model. These may not perform as well.

...

Knowledge quiz
It's time to put what you've learned to the test, get 2 right to pass this unit.

1.
What are the best practices for Calculated columns ?
There are two correct answers.

Use Column Engine expressions.

Use SQL expressions.

Move calculations to the highest node possible.

Move calculations to the lowest node possible.
2.
It is best practice to avoid stacking calculation views, and instead, try to include all logic within one calculation view - true or false?
Choose the correct answer.

True

False

......

Unit 13
Using Tools to Check Model Performance
After completing this unit, you will be able to:

Validate calculation views with the Performance Analysis mode.
Debug calculation views with the Debug Query Mode.
Analyze executions with the SQL Analyzer.
Validating calculation views with Performance Analysis mode
Debugging Calculation Views with the Debug Query Mode
Analyzing executions with the SQL Analyzer

...

Validating calculation views with Performance Analysis mode
Objective

After completing this lesson, you will be able to validate calculation views with the Performance Analysis mode.
Introducing the Performance Analysis Mode
When you are developing a calculation view, the calculation view editor is able to highlight settings that might lead to poor performance.

To enable this, you use the Performance Analysis mode. This tool is launched from inside the calculation view editor in the toolbar (the icon looks like a speedometer). You do not need to activate or build a calculation view in order to use this tool. The basic idea is that you are able to react at design time to warnings and other information relating to your choices, which might lead to poor performance so that you can consider alternative approaches.

Performance Analysis – Detailed Information
Screen capture of the Calculation View editor. The Performance Analysis mode is selected and hints are displayed showing Join information such as referential integrity and proposed cardinality. Data sources details show if tables need to be partitioned, depending on number of rows.
You switch your calculation view to Performance Analysis mode by pressing the button that resembles a speedometer in the toolbar. When you do this, the Performance Analysis tab appears for all nodes except the semantics node. Choose this tab to view detailed information about your calculation view, in particular the settings and values that can have a big impact on performance.

Examples of the information presented on the Performance Analysis mode tab include the following:

The type of tables used (row or column)

Join types used (inner, outer, and so on)

Join cardinality of data sources selected (n:m and so on)

Whether tables are partitioned, and also how the partitions are defined, and how many records each partition contains

When you leave Performance Analysis mode, the Performance Analysis tab disappears from all nodes.

Performance Analysis Validation Warnings
Screen Capture of a Calculation View editor in Performance Analysis mode. A join node is selected and a tool tip shows performance validation warnings because a calculated column has been used in the join definition.
As well as information about the calculation view, Performance Analysis mode also provides you with warnings such as the following:

The size of tables with clear warnings highlighting the very large tables

Missing cardinalities or cardinalities that do not align with the current data set

Joins based on calculated columns

Restricted columns based on calculated columns

This information enables you to make good decisions that supports high performance. For example, if you observe that a table you are consuming is extremely large, you might want to think about adding some partitions and then apply filters so that you process only the partitions that contain data you need.

Working with Performance Analysis Mode
Performance Analysis Mode
Watch this video to learn about Performance Analysis Mode.



...

Debugging Calculation Views with the Debug Query Mode
Objective

After completing this lesson, you will be able to debug calculation views with the Debug Query Mode.
Introducing the Debug Query Mode
To examine the runtime behavior of a calculation view when it is called by a query, you use the Debug Query mode. In Debug Query mode, you can interrogate each node in the calculation view to see how the runtime SQL is generated.

When you switch to Debug Query mode, a generic query is automatically created to call the calculation view. This generated query simulates a reporting tool or application that is sending a query to the calculation view. The generated query requests all columns of the calculation view and does not have filters. In other words, the generated query requests everything from the calculation view. However, you can modify the generated query if you wish, for example, to remove columns or add filters to simulate a user requesting different views of the data.

When you execute the generated query on the calculation view, you can then view the SQL that is generated for each node of the calculation view, and even execute the SQL at each individual node to display the interim results. This helps to ensure that your calculation view is behaving as you would expect, or perhaps to investigate a performance or output issue that has been reported. By stepping through the nodes one by one, examining the SQL, you can check if the expected query is generated. For example, you could see how a WHERE clause is added to the query when a filter is needed, or check how a PLACEHOLDER is behaving when using an input parameter to pass a value.

Using Debug Query mode, you can even navigate to the lowest level of the entire modeled stack. This means that you do not have to start the Debug Query mode in separate calculation views, you simply begin debugging at the top of the model and continue debugging to the lowest level.

One of the most useful things that you can discover when using Debug Query mode is to see how each node is pruned of columns and even complete data sources under various query conditions. For example, this is a great way of testing if union or join pruning is working as you would expect by running queries that request different data sets.

Debug Query Mode

You can reset the top-level debug query in the Semantics node at any time using the reset button, which is adjacent to the Execute button.

Note

Remember that the Debug Query mode calls the active version of the calculation view. This means that if you have made changes to the calculation view but not yet rebuilt it, then you switch on Debug Query mode, the debug results will not reflect those latest changes.

Work with Query Debug Mode
Query Debug Mode
Watch this video to learn about Query Debug Mode.



...

Analyzing executions with the SQL Analyzer
Objective

After completing this lesson, you will be able to analyze executions with the SQL Analyzer.
Introducing the SQL Analyzer
What is the Purpose of the SQL Analyzer?
Although your calculation view may appear well designed, ultimately it is the performance of the calculation view that really counts. You have seen how to display the generated SQL in a calculation view using the Debug Query mode. While this is helpful to see how filters are pushed down and data is pruned, this does not tell us how the SQL actually performs. We really need to know the answers to the following questions:

What was the total execution time?

How long did each step take?

Are there any steps that are taking significantly longer than other steps?

Are there any bottlenecks?

How many records did each step process?

In what sequence are the steps carried out?

Which operators are being called?

To what extent was SAP HANA Cloud able to parallelize the query?

Which processing engines are being invoked?

Were all the query plan steps unfolded?

To help us learn more about the performance of the SQL, we can use the SAP Business Application Studio view HANA SQL Analyzer.

Note

To enable this view, when creating your SAP Business Application Studio development space, or later on, you must add the extension SAP HANA Performance Tools to this space.

Screen capture of SAP Business Application Studio. The HANA SQL Analyzer view is displayed. The Overview Tab shows key information about the execution: Time elapsed, memory used, operators, data usage, network traffic and other system information.
The SQL Analyzer is a set of views, tables, graphs that you can use to analyze any SQL Query. You can drill-down in a graph execution, analyze the timeline of query compilation and execution, visualize how many tables, and which tables, were used, and so on.

In addition to analyzing the SQL generated by the calculation views (either the standard query or a modified query), it can be used wherever SQL is defined, for example in table functions.

How to Get the Query Statement?
To generate the plan file of an SQL query, you must put it in an SQL Console. You do not need to execute it.

For a calculation view, generating the SQL query can be done in the following ways:

In the SAP HANA Database Explorer view of SAP Business Application Studio, select the HDI container, right-click the column view and choose Generate SQL Statement.
This is the easiest way, because from within the embedded SAP HANA Database Explorer view, the generated plan file will open directly in the HANA SQL Analyzer view.

In the Explorer view, navigate to the design-time file of the calculation view, right-click and choose SQL Editor. You can also choose Data Preview and, in the Raw Data tab, choose the Edit SQL Statement in SQL Console icon.
In the external SAP Database Explorer, select the HDI container, select the column view, right-click and choose Generate SQL Statement.
The two last methods are a bit less straightforward, as they require to download the SQL plan file (.plv) from the SQL Console and upload it to the HANA SQL Analyzer view in Business Application Studio.

Note

In any scenario, it is always possible to modify the default query. For example, removing some columns to make sure node/data source pruning works as expected. It is also possible to create the SQL query on top of a calculation view from scratch.
Generating the SQL Analyzer Plan File
To get started, you generate a file that collects the run-time information of your SQL query. Use the menu option Analyze → Generate SQL Analyzer Plan File. Choose a file name prefix for the plan file, and choose Save. The location is already set for you and you cannot change it at this stage.

Screen Capture of the Business Application Studio SQL Console. A SELECT statement is displayed and the Analyze menu drop down list shows the Generate SQL Analyzer Plan File option.
If you have generated the SQL plan file from an SQL Console opened from the SAP HANA Database Explorer view of SAP Business Application Studio, the plan file opens immediately in the HANA SQL Analyzer view.

Screen Capture of the Business Application Studio HANA SQL Analyzer view. The generated file from previous step is selected.
You can then proceed to the last step of this procedure.

If you have generated the SQL plan file from another tool, such as the Data Preview of SAP Business Application Studio, or from the external SAP HANA Database Explorer (outside of SAP Business Application Studio), you need to download the SQL plan file to your computer. Choose Download, and this is then handled by your web browser. Choose a convenient location to store the SQL plan file.

Screen Capture of the SAP HANA DATABASE EXPLORER. The Database Diagnostic Files folder is expanded and the Other folder is selected. The content of the folder is displayed and you can right click on the PLV file and choose to Download it to your computer.
The next stage is to upload the SQL analyzer plan file to the Explorer view of SAP Business Application Studio. Upload the plan file to any project folder by using the context menu Upload.

Set of screen captures showing the process to use an external PLV file. First upload the file to Business Application Studio project, then switch to HANA SQL Analyzer view and add the plan file using the + icon.
Switch to the SQL Analyzer.

Note

In order to see the HANA SQL Analyzer view, remember to add the extension SAP HANA Performance Tools to your development space. This can only be done when the development space is stopped.

Add the plan file to HANA SQL Analyzer.

You must first locate the plan file in the folder where you uploaded it.

Select the plan file to display the results in the main window. Screen capture of the SAP Business Application Studio HANA SQL Analyzer view. The PLV file is selected and the PLAN GRAPH tab is displayed showing the SQL statement execution plan in graphical mode.
How to Analyze a Generated SQL Plan File Later on?
In case you want to generate plan files first, and access the generated SQL plan files at a later time, you can proceed as follows, depending on how you generated the plan files:

Embedded SAP HANA Database Explorer (in SAP Business Application Studio):
You access these plan files from the Explorer view, in the folder /home/user/.vscode/data/User/globalStorage/sapse.hana-database-explorer/. They can be opened directly from this location or, If relevant, you can move the plan files to your project.

External SAP HANA Database Explorer and other views of SAP Business Application Studio:
You access these plan files from the External SAP HANA DB Explorer, within the HDI container, in the section Catalog → Database Diagnostic Files → <DB Instance ID> → other. They must be downloaded, and then uploaded to SAP Business Application Studio.

Which Privileges are Needed to Run SQL Analyzer?
Analyzing an SQL plan file with SQL Analyzer can be performed either from the technical user <container_schema_name>...RT of your HDI container, or a classic database user.

In both cases, the user analyzing the query needs the two following SYSTEM privileges:

TRACE ADMIN
INIFILE ADMIN
Note

You will learn more about users, roles and privilege management in a dedicated lesson.

...

Knowledge quiz
It's time to put what you've learned to the test, get 3 right to pass this unit.

1.
To work with calculation view Debug Query Mode, you first need to deploy the calculation view?
Choose the correct answer.

True

False
2.
Which of the following steps is required before you can visualize a query plan file from the SAP HANA Database Explorer embedded in SAP Business Application Studio?
There are two correct answers.

Write an SQL query

Execute an SQL query

Generate an SQL Analyzer plan file

Download the SQL query plan file
3.
What can you check with Performance Analysis Mode?
There are two correct answers.

SQL expressions performance

Calculation view results accuracy

Join definition optimization

Table partitioning need


......

Unit 14
Implementing Features to Improve Performance
After completing this unit, you will be able to:

Control Parallelization.
Define partitions to improve calculation view runtime.
Controlling Parallelization
Partitioning Tables

...

Controlling Parallelization
Objective

After completing this lesson, you will be able to control Parallelization.
Controlling Parallelization in a Data Flow
SAP HANA Cloud always attempts to automatically apply parallelization to queries in order to optimize performance. Following good modeling practices will ensure the queries that run on your calculation views have the best chance of being parallelized. However, there are cases when the optimizer will not apply parallelization in a complex, long-running data processing step as it is not able to understand the business semantics and is concerned it might break the results if it tried to parallelize. So instead, it cautiously leaves the processing step as sequential even though it might harm performance.

However, if you are able to ensure the step could be safely run in a parallel way by splitting up the data into semantic partitions, then you can dictate when parallelization should occur.

Within a calculation view, it is possible to force parallelization of data processing by setting a flag Partition Local Execution to mark the start and also the end of the section of a data flow where parallelization is required. The reason you do this is to improve the performance of a calculation view by generating multiple processing threads at specific positions in the data flow where performance bottlenecks can occur.

Screen capture of a Calculation View scenario. The first projection is used to break up data into partitions depending on the Country attribute. The second projection, called Currency_convert_In_Parallel executes a process on three different partitions corresponding to the three Country values: India, France and Iceland. A union node is then used to merge data back together.
The parallelization block begins with a calculation view node that is able to define a table as a data source. This node could be a projection node or an aggregation node. It is not possible to use any other type of data source such as a function or a view.

In the Properties of the chosen start node, a flag Partition Local Execution is set to signal that multiple threads should be generated from this point onwards. It is possible to define a source column as a partitioning value. This means that a partition is created for each distinct value of the selected column. For example, if the column chosen was COUNTRY, then a processing thread is created for each country. Of course, it makes sense to look for partitioning columns where the data can be evenly distributed. The partitioning column is optional. If it is not selected, then the partitioning defined for the table is used.

If you do not explicitly define the partitioning column, then the partitioning rules of the underlying table are applied (assuming the table is partitioned).

To end the parallelization block, you use a union node. However, unlike a regular union node that would always have at least two data sources, a union used to terminate the parallel processing block is fed only from one data source. The union node combines the multiple generated threads but the multiple inputs are not visible in the graphical editor and so the union node appears to have only one source from the node below. You cannot combine any other data sources in the union node that is used to terminate the parallelization. In the Properties of the union node, a flag Partition Local Execution is set to signal the ending of the parallelization block.

Screen Capture of Calculation View scenarios illustrating restrictions in the use of parallelization. See text after the image for details.
There are some restrictions that you should be aware of here.

The parallelization block always starts with a node that includes a data source that must be a table. You can use a table defined in the local container or a synonym which points to an external table. You cannot use another calculation view as the data source or a table function.

Screen Capture of Calculation View scenarios illustrating restrictions in the use of parallelization. See text after the image for details.
Only one parallelization block can be defined per query. This means that you cannot stop and the start another block either in the same calculation view or across calculation views in the complete stack. You cannot nest parallelization blocks, for example, start a parallelization block then start another one inside the original parallelization block.

It is possible to create multiple start nodes within a single parallelization block by choosing different partitioning columns for each start node. If you create multiple start nodes, then all threads that were generated are combined in a single ending union node.

Note

In addition to defining logical partitions using this technique, always remember that SAP HANA will attempt to apply additional parallelization to the logical partitions.

To check the partitioning of the data, you can define a calculated column within the parallelization block with the simple column engine expression partitionid(). In the resulting data set, you will then see a partition number generated for each processing thread.

Other techniques to monitor parallelization:

You can also collect trace information by adding WITH PARAMETERS ('PLACEHOLDER' = ('$$CE_SUPPORT$$','')) to the end of the query statement.

Use SQL Analyzer to navigate to the nodes that should be parallelized and you should see the nodes are duplicated according to the number of partitions.

Caution

It is important to not overuse this feature as over-parallelizing can lead to poor overall performance where other processes are affected.

...

Partitioning Tables
Objective

After completing this lesson, you will be able to define partitions to improve calculation view runtime.
Partitioning Tables
Table Partitioning
Data in column store tables is separated into individual columns to support high performance on queries and also for better memory management. However, it is also possible to subdivide the rows of column tables into separate blocks of data. We call this table partitioning. If column separation is vertical subdivision, think of table partitioning as horizontal subdivision.

A table containing three columns: Year, Country and Quantity. It contains rows for three different years (2020,2021 and 2022). Three partitions are displayed, containing each a subset of the table rows, depending on the Year value.
Note

Partitioning only applies to column store tables and not row store tables. This is why it is recommended that you define tables as a column store that will be used for querying large data sets.
Generating partitions is usually the responsibility of the SAP HANA Cloud administrator. There are many decisions to be made relating to partitions including the type of partition. For example, hash, round robin, or range. Partitions can also include sub-partitions. The administrator uses monitoring tools to observe the performance of partitions and makes adjustments. What the modeler will need to do is to provide the information relating to the use of the data - for example, how queries will access the data so that the partitions can be defined optimally.

Reasons for Partitioning a Column Store Table

...

Knowledge quiz
It's time to put what you've learned to the test, get 2 right to pass this unit.

1.
How many parallelization blocks can you have in a calculation view?
Choose the correct answer.

3

1

5
2.
Which limitation can you overcome by using table partitioning ?
Choose the correct answer.

The number of columns in a row-store table

The number of rows in a column-store table

The number of rows in a row-store table

The number of columns in a column-store table

......

Unit 15
Storing Calculation View Results
After completing this unit, you will be able to:

Improve calculation view performance with static cache.
Define snapshots queries on a calculation views to store its results.
Define MDS Cubes for a Calculation View.
Implementing Static Cache to Improve Performance
Creating Snapshots
Defining MDS Cubes

...

Implementing Static Cache to Improve Performance
Objective

After completing this lesson, you will be able to improve calculation view performance with static cache.
Caching View Results
Complex calculation views place a heavy load on the system. To reduce this, it's possible to cache the results of a calculation view in order to speed up the time needed to get the results when executing a query against the view. This feature is useful when you have complex scenarios or large amounts of data so that the results don't have to be recalculated each time the query is executed. The benefits aren't just the reduced time for the results to appear for the user of the query, but it also means that CPU and memory consumption is reduced, leading to better performance for other tasks.

Overview of the static cache principles. A first execution of Query A at 9:10AM will fetch the data from the database and store the result in the cache, taking a certain amount of time. A subsequent execution at 10:30AM will only have to read the data from the cache, thus taking much less time.
If the cache can't be used, due to a query requesting a different data set than what is held in cache, a query doesn't fail. It means that the query will need complete processing.

Caching shouldn't be applied to raw data but to data that has been highly processed and reduced through aggregations and filtering. In practice, this means applying caching to the top-most nodes of a calculation view.

Cache can only be used for calculation views that don't check for analytic privileges. This means analytic privileges should be defined on the top-most view only, in a calculation view stack. The optimal design would be to define the cache setting at the highest calculation view possible in the stack, but not at the very top where analytic privileges are checked. This means that the user privileges are always checked against the cached data, if the cache is useable.

Screen Capture of a Calculation View properties. In the Static Cache tab, the Enable Cache property has been selected. Three columns have been specified as to be cached. A Query selecting only cached columns will use the cache, a query selecting any other column will not.
It's possible to fine-tune the calculation view cache using column and filter settings.

Firstly, calculation view caching can be defined at the column level. This means queries that use only a sub-set of the cached columns can be served by the cache. It's recommended to cache only the columns that are frequently requested in order to reduce memory consumption and speed up cache refresh.

Caution

If you don't specify any columns in the cache settings, then all columns are cached.
Screen Capture of a View properties. In the Static Cache tab, three columns are listed as being cached and a filter is defined on COLUMN3. Only queries using the cached columns and at least the exact filter as defined, will use the cache.
The cache size can be further reduced by defining filters in the cache settings. Queries that use either exactly the cache filters, or a subset of the filters, are served by the cache. Cache is only used if the query filter matches exactly the filter that is defined for the cache, or reduces the data further.

Screen Capture of a View Properties. The Static Cache tab shows a value of 10 for the Retention Period property (the maximum acceptable age of cached data in minutes before a query triggers a cache refresh).
It's possible to define a retention period of the cache so that the cached data expires. The value entered is in minutes.

Note

It's currently not possible to specify that cache should be invalidated if new data is loaded to the underlying tables. This feature is expected to come later. For now, we just have a time-based expiry (in minutes).
Static Cache Prerequisites
In order to use the calculation view static cache, there are some important prerequisites that must be met.

Enable Cache is selected
Calculation View can be unfolded
No granularity tracking calculations that prevent cache-use (to avoid unexpected results)
Firstly, the basic setting Enable Cache must be set in the calculation view to be cached.

Screen capture of a Calculation View properties. In the Static Cache tab, the Enable Cache property is selected.
Then, the query must be able to be fully-unfolded. This means that the entire query must be fully translatable into a plain SQL statement and not require the calculation of interim results. You can use the Explain Plan feature of the SQL Analyzer to check if unfolding can occur.

There must be no calculations that depend on a specific level of calculated granularity as the cache is stored at the level of granularity requested by the first query. This may not align to the aggregation level required by subsequent queries and could cause incorrect results.

Static Cache Invocation
Even with these prerequisites met, the cache feature is disabled by default and is only considered when explicitly invoked by using one of the following RESULT_CACHE hints:

Hint is added to your SQL Query:
SELECT ...WITH HINT (RESULT_CACHE)

A database hint is used in the top-most view, which consumes to-be-cached view
Configuration parameter is set:
indexserver.ini -> [result_cache] -> enabled

Screen capture of Calculation View properties. In the View Properties tab, some execution hints are set to use the RESULT_CACHE. This is the top most consuming view. Views used as data sources, which will be cached, do not need any hints.
If the prerequisites for cache usage aren't met, it might still be possible to force the cache to be used. You can use the setting Force so that the cache is used. However, you should test extensively to ensure that correct results are returned.

...

Creating Snapshots
Objective

After completing this lesson, you will be able to define snapshots queries on a calculation views to store its results.
Calculation View Snapshots
Sometimes, storing calculation view results isn't just for performance. You could need to take pictures of your data at different points in time (snapshots). This could be done to store historical values. Or you could need performance but not on real-time data, just on a latest snapshot of the system taken in the morning, for example.

The Snapshot Process
Let's look at how the snapshot feature works.

Calculation View snapshot process. Setup your Calculation view by creating a snapshot, generate objects, then execute the query and query the snapshot or the entire Calculation View. See details in the text after the image.
The calculation view snapshot process is as follows:

First, you need to deploy your calculation view to generate the snapshot table.
Procedures are also generated to truncate the snapshot table or to trigger insertion of data.
You can execute the snapshot manually or schedule it to get new data regularly.
You can then query the snapshot table to get historical data.
You can always continue using the calculation view as usual to get real-time data.
Generating Snapshot Artifacts
First, if you want to use this feature, you need to setup your calculation view to create a snapshot.

To setup a snapshot you need to go to the View Properties tab of your calculation view and then to the Snapshots tab. Here you can create a query on top of your calculation view. It's the result of this query that will be stored in the generated snapshot table.

Procedures are also generated to create or drop the snapshot table and to insert or truncate data. You can trigger the procedures whenever a change in the snapshot data is needed. You need to define the security mode for the generated procedures. If you choose DEFINER, any user with the right to execute those procedures will be able to use them. If you use INVOKER, you'll have to give access to the snapshot table.

Watch this video to learn how to setup snapshots for a calculation view.

Executing Snapshots
The INSERT procedure can be used at any time to execute the query and store the results into the snapshot table.

Caution

If you call the procedure to insert data twice, without truncating data, you'll have duplicates.
You could, of course, schedule execution of those procedures to get regular updates of the snapshot data.

You can also decide to provision the snapshot table when deploying the calculation view. For this, you need to select the Create Snapshot After Deployment Automatically option. By default, data will be truncated and potentially inserted again each time you deploy your calculation view. If you don't want to delete data between two deployments, you can use the Keep Snapshot During Re-deployment option.

Watch this video to learn how to provision the snapshot table.

Using Snapshots
In order to report on snapshot data, you just need to select from the snapshot table. If you need real-time data, you can still use your calculation view as usual.

If you want to be able to get both possibilities with one view, you can define an interface view. This generated view does a union with both the snapshot table and the calculation view and uses an input parameter to filter data from one source or the other.

Watch this video to learn how to generate and use the interface view.

...

Defining MDS Cubes
Objective

After completing this lesson, you will be able to define MDS Cubes for a Calculation View.
Defining MDS Cubes for a Calculation View
Consider you want to prepare a model in SAP Analytics Cloud (SAC) for a few business users. This is based on the SAP HANA Calculation View technology. One business user needs to evaluate net sales amount and gross sales amount by business partner, distribution channel, and month. Other business users want to see the same information, but sometimes they need details by product, distribution channel, and date for these measures. Instead of creating two different calculation views, it is easier to create a single view with all these drill-down options. However, the more complex the calculation view becomes, the more processing resources will be needed. This is especially true when remote data access is involved. To improve the performance, you could allow caching for this view. However, the first dialogue would then take a long time because the cache is not yet filled. Additionally, it might not be possible to serve such different drill-down queries with the static cache. It's better to store pre-processed data in a format that can serve all these complex drill-down queries and allows to quickly answer these most frequently required drilldown situations. For this purpose, calculation views offer an additional property called multidimensional services cube (MDS cube).

Screen captures of the steps to create an MDS cube: Create or open a Calculation View. Create a new MDS Cube. Open its details and add attributes and measures. Deploy the view.
An MDS Cube is defined for a specific underlying calculation view and represents a submodel of this view, this means, a subset of its measures together with a subset of its attributes. When you define an MDS cube as the basis of a model in SAP Analytics Cloud, you must include all fields into the model that are required in your SAC model.

Note

MDS cubes can only be defined for calculation views of Data Category Cube.

You can define multiple MDS Cubes for the same calculation view. Deployment of the calculation view builds the view and generates all MDS cubes that are defined for it, initially without data. After loading them (see section below, The Management of MDS Cubes), preaggregated data is stored in MDS Cubes in a highly optimized manner to speed up analytical queries, like a predefined persisted cache. But in contrast to a cache, the data is not stored as a flat datastructure, but in a multidimensional, star-join-like structure that is optimized for tailored data access.

MDS hints can optionally be added for loading and processing the MDS Cube, but they are typically not required. Use them only if you are advised by incident processing.

Note

MDS Cube and MDS Cube Cache are different things. Do not confuse them.
Two MDS cubes based on the same source data with different levels of aggregation. For example, amount per country in one MDS cube, but amount and quantity per product in a second one.
Data in MDS Cubes reflect the state at the time that the MDS Cube was loaded. Only data that were visible to the user who loaded the MDS Cube are included. When the source data changes after populating the MDS cube, the loaded data is not automatically updated. This can lead to differences when comparing reporting based on MDS Cubes and based on the online calculation view. To reflect data changes in reporting, it is necessary to manually reload the MDS Cube. This means that if you need real-time data, MDS cubes can't be used.

Comparing Static Cache, Query Snapshot and MDS Cube regarding performance optimization and flexibility. MDS Cube is faster than a query snapshot. MDS Cube can be slower or faster in comparison to static cache. MDS cube serves for more queries than these options. A Calculation View without performance optimization is much slower.
In comparison to caching, caches can be faster, but then the query must exactly match the cached data, which is rarely the case. MDS cubes can also perform complex postcalculations, such as restrictions, ranking, formula, and standard or exception aggregation as they are typically defined in SAP Analytics Cloud. MDS Cube could even be faster than Static Cached views for which only intermediate results are already cached. If the cache provides the data on a higher granularity than is requested by the query an aggregation needs to happen. MDS Cubes can aggregate faster due to their optimized data structure for OLAP analyses.

This optimized data structure can be used in analytical reporting based on MDS metadata, such as SAP Analytics Cloud, but is not accessible by SQL. Therefore, the SQL Analyzer or Explain Plan view does not show if an MDS Cube is used. As of today, MDS Cubes are not listed in the Database Explorer.

In SAP Business Application Studio, the document generated by the Generate Document function also includes information about the MDS cubes.

The Management of MDS Cubes
To query metadata, load, or delete the data of MDS Cubes, specific APIs exist. The API works with a built-in procedure, which accepts a single JSON input parameter and returns a single JSON object as an output.

Execute an SQL statement, such as:

Code Snippet

Copy code

Switch to dark mode
MANAGE_MDS_CUBE('<JSON MDS Cube API>',?)
Examples can be found here: API for Administrating MDS Cubes | SAP Help Portal

This JSON statement describes what you do with which object. Here is how it looks like for (re-)loading the cube.

Code Snippet

Copy code

Switch to dark mode
{
   "Cube": {
      "Command": "Reload",
      "Target": {
         "DataSource": {
            "SchemaName": "<Schema>",
            "ObjectName": "<MDS Cube Name>"
         }
      }
   }
}
The target here is the MDS cube. An MDS Cube is always tied to the calculation view on which it is defined. Therefore, the calculation view is automatically determined as the source. If the command returns successfully, the state of the MDS Cube changes to 'Ready', indicating that the MDS Cube is ready to be queried. If the state was already 'Ready' before loading the data, the MDS Cube will be reloaded. During the reload, the MDS Cube can still be queried and will display the old data until the reload finishes with success. For monitoring, check the _SYS_EPM.MDS_METADATA table.

Limitations
Currently, the following limitations exist:

MDS Cubes cannot contain calculated measures. Instead, you can generate the calculation in SAC.
MDS Cubes support variables, but no input parameters. If the calculation view with an MDS Cube contains a mandatory input parameter without default value, deployment of the calculation view will fail. If you use input parameters in a restriction, we recommend that you convert them manually to variables.
Joins with two or more join condition fields are not supported.
Columns of the following datatypes can be used:
Integer Types: TINYINT, SMALLINT, INT, BIGINT
Floating Types: REAL, DOUBLE, FLOAT, FLOAT(p)
Decimal Types: DECIMAL; DECIMAL(p,s)
Boolean Types: BOOLEAN
Character Types: VARCHAR, NVARCHAR, CHAR,NCHAR
Date/Time Types: DATE, DAYDATE, TIMESTAMP
But, VARBINARY columns can not be used.
Note

In an SAC model that is based on an MDS Cube, you see only the fields that are part of the MDS cube. So, make sure that you add all required fields.
If MDS Cubes does not support a calculation view feature, the MDS Cubes tab is inactive, and a list of features is displayed that prevents the usage of MDS Cubes. Removing these features from the calculation view activates the MDS Cubes tab. For example, a join with more than one join field prevents the usage of MDS cubes. If an MDS Cube is already defined in a calculation view, features of the calculation view that MDS Cubes don't support are deactivated. When hovering over the feature, you are informed that it is inactive due to the existence of an MDS Cube definition. If the MDS Cube definition is deleted, the feature becomes available again.

Authorizations
To load or delete an MDS cube, you need the EXECUTE privilege for procedure MANAGE_MDS_CUBE. You can grant it as shown in the code snippet:

Code Snippet

Copy code

Switch to dark mode
GRANT EXECUTE ON MANAGE_MDS_CUBE TO <DB_USER>;
Additionally, you need privilege CREATE ANY for the HDI schema in which the Calculation view of the MDS Cube resides. Moreover, you must have SELECT privileges for the data source of the MDS Cube.

You can either create roles with the authorizations or directly assign them to users.

Analytic privilege settings of a calculation view are applied automatically also to all MDS Cubes that apply to within the calculation view. If the calculation view is secured by analytic privileges, users also need analytic privileges to access the data of the respective MDS Cubes. Analytic privileges that are defined on a calculation view automatically apply to the MDS Cube as well. Analytic privileges that are assigned to users who report on the MDS Cube must include only columns that are included in the MDS Cube.

References
Help page:
https://help.sap.com/docs/hana-cloud-database/sap-hana-cloud-sap-hana-database-modeling-guide-for-sap-business-application-studio/define-mds-cube-based-on-elements-in-calculation-view

Administration of MDS Cubes :
https://help.sap.com/docs/hana-cloud-database/sap-hana-cloud-sap-hana-database-administration-guide/managing-and-monitoring-mds-cubes#about-mds-cube

...

Knowledge quiz
It's time to put what you've learned to the test, get 3 right to pass this unit.

1.
Which is the frontend tool for which you profit from MDS cube definition?
Choose the correct answer.

SAP Analytics Cloud (via InA interface)

SAP Lumira (via Javascript)

Any third party tool (via SQL statement)
2.
Why would you use a snapshot?
There are two correct answers.

To improve performance while executing the calculation view.

To store the results of a query on the calculation view.

To improve performance when querying historical data.

To store intermediate results when executing a query on the calculation view.
3.
What is the benefit of configuring static cache?
Choose the correct answer.

Improve calculation view performance.

Collect calculation view usage statistics.

Capture SQL execution steps for debugging.
4.
The MDS Cube is a performance optimization technology for multidimensional operations that uses a persistency that is optimized for multidimensional queries.
Choose the correct answer.

True

False

......

Unit 16
Using Additional Modeling Productivity Tools
After completing this unit, you will be able to:

Use features to increase productivity of calculation view development.
Audit calculation views using provided tools.
Productivity Aids for Efficient Calculation View Development
Working with Modeling Content in a Project

...

Productivity Aids for Efficient Calculation View Development
Objective

After completing this lesson, you will be able to use features to increase productivity of calculation view development.
Features to Enhance Productivity of Calculation View Design
Calculation views offer a lot of flexibility, in particular the possibility to have a large number of nodes. As calculation views become more complex, maintenance can be challenging. The SAP Business Application Studio provides functionality to manage nodes and semantics. The following features are available:

Feature	Purpose
Switching Node Types	Switch one node type with another without losing the link to the data source or the upper and lower nodes.
Replacing a Data Source	Replace one node source with another without losing the output columns, calculated columns, and so on, throughout all of the upper nodes.
Extract Semantics	Apply the semantics from an underlying node or data source to the Semantics node of the calculation view.
Propagate to Semantics	Map columns to the output across all upper nodes, up to the top node of the calculation view.
Previewing the Output of Intermediate Nodes	To troubleshoot problems in view design, preview the data of an intermediate node rather than the whole calculation view.
Map Input Parameters between nodes	Map input parameters of a view to input parameters defined in underlying (source) views of other nodes.
Switching Between Certain Types of Node
It's possible to convert a Projection node into an Aggregation node and the other way around without losing the reference to the data sources of the node.

For the top node, it's also possible to switch to a Star Join node.

Replacing the Data Sources of a Node
Depending on the type of node and view configuration, you can replace the data source of a node (for example, a table, a Calculation view, or another node) by another data source (table, Calculation View).

This functionality is useful, for example, when you want to have the columns still propagated to all the upper nodes. You can also keep the existing calculated columns defined in this node working despite the change of data source.

When you do the change, a column mapping dialog box appears. It helps you assign the columns of the new data source to the output columns previously defined in the node.

Extracting Semantics
It's possible to extract the semantics from the underlying data sources. This especially useful when these underlying data sources include rich semantics and you don't want to have to redefine them again. You can choose to extract different types of semantics such as column labels and aggregation functions. It's also possible to extract hierarchies and variables from the included calculation views. To allow this, the corresponding columns must be available in the current view (the "extracting" view). For example, it doesn't make sense to extract a hierarchy based on product if the column product doesn't exist in the extracting calculation view.

Propagate to Semantics
This feature is helpful in calculation views with multiple stacked nodes. The purpose is to map one or several columns, already in the output of a node, to the output of all upper nodes right up to the Semantics node.

This is an alternative approach, and less tedious than adding manually, the same column(s) to the output of each of the nodes in a calculation view.

To do this, select one or several columns in the output of a node. Then right-click the selection, and choose Propagate to Semantics.

Previewing the Output of Intermediate Nodes
To understand and fine-tune the different steps of a calculation view, each node can be previewed independently from the calculation view itself.

To achieve this preview, the calculation view must have already been successfully deployed. Be sure to select the node header and not the data source within the node, to preview the results of the node. If you select the data source and use Data Preview, you'll see the data in the original data source and not the results of the node output.

Map Input Parameters Between Nodes
You can map input parameters (and also variables) defined in a calculation view to the input parameters and variables defined in the source views. You can do this to pass the parameter value itself, but also to pass the relevant list of values to the input parameter dialog box at runtime (when the view is executed).

Note

In the Manage Mappings view, you can even copy and map an input parameter from a source view.

That is, you don't need to create a new input parameter in the calculation view (and define all its settings) before mapping it. This task can be done automatically in just one click.

Copy and Paste Part of a Scenario
It's possible to copy and paste a node within the Scenario pane of the same calculation view. If this node uses other nodes as its data sources, it's possible to decide whether you want to copy this node only, or the node and all nodes below, together with the connections between them.

Insert a Node between Two Other Nodes
In the Calculation View scenario, you can insert a new node between two existing nodes that are already linked. By doing this, you keep the columns consistent along the entire scenario.

Note that this is only available as a graphical feature: You must drag the new node from the palette and drop it exactly onto the existing link between the existing nodes.

For example, if you've started designing a CUBE calculation view where the aggregation node uses a join node as its data source. You've already mapped the required columns, defined the aggregate functions, semantics, and so on. Then you realize you need a projection to calculate a column before aggregating. In that case, by adding the new Projection between the Join node and the Aggregation node (instead of basically removing the Join node from the aggregation node and adding it to a new projection node), you keep most of the work you already made and save time.

Navigating to Source Calculation Views
It's possible to open a calculation view directly from calculation scenario of another view that consumes it. To do this, in the Scenario pane, right-click the data source (Calculation View) in the node that consumes it, and choose Open.

...

Working with Modeling Content in a Project
Objective

After completing this lesson, you will be able to audit calculation views using provided tools.
Auditing Dependencies Between Calculation Views
Two features are available in SAP Business Application Studio to analyze modeling content within a project. These are:

Data lineage

Impact analysis

They work in a symmetrical way. For any calculation view you choose, you can show the dependencies with other modeling content.

To use these features, right-click a calculation view file in the Explorer view, and choose Show Data Lineage or Open Impact Analysis.

Data Lineage
Screen capture of a data lineage result showing referenced tables and calculation views.
Data lineage shows all the calculation views and source tables on which a given calculation view depends.

Impact Analysis
Screen capture of an impact analysis result showing dependent calculation views.
The purpose of impact analysis is to show all the chain of calculation views that depend on a given calculation view.

Hint

In the Data Lineage or Impact Analysis view, you can directly open a calculation view from the dependency tree by double-clicking it.
Lineage and Impact Analysis Across HDI Containers
From QRC 3/2022, SAP HANA Cloud is able to perform a lineage and an impact analysis not only within a given HDI container, but also towards another HDI container.

This feature is useful, when calculation views from a container are consumed by calculation views from another one, to check as exhaustively as possible how a change you make to a calculation view might impact another HDI container. You can also, similarly, track all the dependencies of a calculation view, including the ones located in an external container.

Note

The privileges of the _RT user (the technical user interacting on your behalf with the HDI container) determine which objects are visible outside of the HDI container.

So, if the _RT user has system privilege CATALOG READ, all objects are visible. Otherwise, only the objects for which the _RT user has an object privilege, such as SELECT, are visible.

Tracing the Origin of a Column in a Calculation View Scenario
Another powerful auditing feature is available within a calculation view to show the origin of a column.

Screenshot of a calculation view. A column is selected in the semantics. After selecting the show lineage icon, all the nodes containing this column are highlighted, until reaching the node and the data source where the column has been extracted from..
From the Semantics node, you can choose a column and trace its origin with Column Lineage.

The column lineage shows, within the scenario of the opened calculation view, all the nodes where this column exists. At the bottom of the calculation view scenario, you find the origin of the column. Opening the Mapping tab of this node allows you to identify the source column name in the data source.

Note

In a cube with star join calculation view, the Show Lineage feature only works for private columns, not shared columns from dimension calculation views.
Column Lineage
Column lineage is useful when columns are renamed within the calculation scenario, or when you want to see quickly where a calculated or restricted column originates from.

It also helps you to avoid mixing up columns with the same name, but not necessarily the same data that are present in several nodes of a calculation view.

For example, the NODE_KEY column in an SAP ERP system is used in many tables to join the master data-defining attributes.

You might want to make sure that the NODE_KEY column in the output of a dimension calculation view originates from the correct table (for example, the table SNWD_BP containing business partners) and not from another table, also used in the calculation view, in which a NODE_KEY column is present but does not identify business partners (for example, the NODE_KEY in the table SNWD_AD containing Contact addresses).

Note

The Show Lineage feature stays active until you exit the Show Column Lineage mode by choosing Exit.
The Outline View
Watch this video to learn about the Outline View.

Note

Make sure you don't mix up the Outline feature of the calculation view editor with the general Outline view of SAP Business Application Studio, which displays structured code into a hierarchy.
Where-Used Functionality
In the Calculation View editor, you can quickly identify within a Calculation View where a particular object is used. It allows you to identify superfluous elements that can be deleted or to avoid modifying an object that is used in several places, in case it would break the consistency of your Calculation View.

The Where-Used feature supports the following objects:

Input Parameters
Calculated Columns
Restricted Columns
Screenshot of a calculation view node properties. The Parameters tab shows a Year input parameter. The Where Used icon has been selected and the different places where the input parameter is used are listed.

...

Knowledge quiz
It's time to put what you've learned to the test, get 2 right to pass this unit.

1.
Which of the following tools is used to analyze dependencies between models?
There are two correct answers.

Data lineage

Column lineage

Impact analysis

Outline
2.
Which type of semantics can you extract using the option Extract Semantics?
There are two correct answers.

Column Labels

Calculated Columns

Hierarchies


......

Unit 17
Working in a Modeling Project
After completing this unit, you will be able to:

Explain the structure of a project.
Deploy modeling content to the runtime environment.
Manage modeling content.
Project Structure
Deploying Models
Manage modeling content

...

Project Structure
Objective

After completing this lesson, you will be able to explain the structure of a project.
Workspace, Projects, Modules, and Folders
A workspace in SAP Business Application Studio is a file structure where you can work on one or several projects. Each user has their own workspace, and can create additional ones if needed.

The workspace of a user can contain as many projects as needed, but each project must have a different name. Practically, this is done by storing the content of different projects into different root folders. It's possible to open a single project/folder at a time, or to create a multiroot workspace.

Relationship of Workspace, Projects, and Modules. A user's workspace contains projects. A project contains modules with folders and objects.
When you create a new project, the creation wizard will ask you to choose a project template. SAP Business Application Studio comes with several templates and you choose the one most suitable for the type of application you want to create. Depending on the template you choose, you will be prompted for various settings, which are then used to create the basic project shell.

We'll focus on the project template SAP HANA Database as this is the most straightforward for developing calculation views and other modeling artifacts.

Modules
Within a project there are modules. There are different types of modules. For example, there's a module for Java, a module for HTML, and a module for database objects.

The first step is to create a module of the type you require and then begin developing the source code in that module. You can create as many modules as you need for your project even of the same type. Typically, for a database module, you probably only need one. You should provide a meaningful name for the module so that it's easy to identify the module type and what it contains. Don't create empty modules that you won't use.

Within a module, we have folders. Folders are used to subdivide your source files into meaningful groups. For example, you may choose to create two folders: one for tables and one for calculation views.

HANA Database Modules
A HANA Database module, also know as an HDB module, contains all the design-time files that define the various database objects that must be deployed together with the application. From a project structure perspective, it consists of a folder to which you give a name when creating the HDB module, for example, db or dbmodule.

This folder contains, among others, a folder src, generated by default, which is where you store modeling content, but also additional content related to the HDI deployment (.env file, node_module folder containing the node.js HDI Deployer and its dependencies).

Note

The HDB module is referenced by the project descriptor file, mta.yaml, which also contains some settings about the module, such as the database id or schema name (not all of them are mandatory).
Main Database Artifacts Defined in an HDB Module
Tables

Calculation views

Functions

Procedures

Analytic privileges

Synonyms

Persistence Layer: Inside the HDB Module or Outside?
Depending on the type of application, the main data managed by the application can be stored in different places.

A major design option for an application is to decide whether this data is stored in a persistence layer defined by the application itself, or if it's stored in another location, such as a classical database schema. Let’s consider the two following examples to illustrate these approaches:

Internal or External Persistence Layer

Objects Identifier and Namespace
In the SAP HANA Cloud database, each runtime object has an object identifier, which is used to reference this object within the container or schema. By default, the runtime name is the same as the design-time (source) file name. That keeps things simple.

A runtime name can also have an optional namespace as a prefix to provide even more meaning.

So the structure of a complete runtime object is as follows:

Code Snippet

Copy code

Switch to dark mode
<Namespace>::<Runtime Object Name>
The Runtime Object Name is mandatory.

The Namespace is an optional part of the object identifier, and is mainly used to organize logically the runtime objects within the container.

Note

In this course, the namespace convention we've chosen for the main application is to have a prefix HC, applied uniformly to all objects, regardless of the folder location of design-time objects. In other words, the namespace is the same for all the runtime objects defined in all folders of our HDB module. So, these objects all have the identifier pattern HC::<Runtime Object Name>.
Runtime Objects Identifiers
The object identifiers (including the namespace) are always specified in the design-time objects. That is, the design-time objects must define how the runtime objects will be identified.

Let’s take an example of a modeling object, a calculation view CVC_SalesOrders, with its design-time version in the SAP Business Application Studio workspace, and its runtime version (created during deployment) in the container schema.

During the build process, the name of the deployed runtime objects is created based on a namespace and the object name. Both are derived from the content of the design-time file in the user's workspace. Namespace and object name are separated by :: (two colon signs)
The figure, runtime Objects Identifiers, explains how folders are used to organize the design-time files in the HDB module of your application, and shows what the runtime object identifier could be. In this scenario, the calculation view identifier has a namespace. Let’s now discuss the main rules and possible options to define the namespace.

Namespace
The following rules apply to the namespace:

The namespace is optional. Some objects can be defined with a namespace in their identifier, and others without.

An HDB module can have no namespace defined at all, or can specify any number of different namespaces.

The namespace is always identical for all the design-time objects stored in the same folder.

The namespace must always be explicitly mentioned inside the design-time files, and must correspond to the namespace defined explicitly in the containing folder, or implicitly cascaded from the parent folders.

The .hdinamespace File
The definition of a namespace is possible in the src folder of an HDB module, and also in any of its subfolders. This is done with a file called .hdinamespace.

When you create an HDB module, a .hdinamespace file is created automatically in the folder <hdbmodulename> → src and contains a default value "<application_name>.<HDB_module_name>". You can modify this file to apply different rules than the default to your project’s namespaces.

Note

It's recommended to make such a change at the beginning of the project to avoid breaking dependencies. Similarly, you must decide early on whether a single namespace should be used for all your modeling artifacts' runtime objects, or if you want to use several ones.
A .hdinamespace file within the structure of a project, showing the content of the file, namely the name and subfolder keys used to define the namespace rule. The corresponding values are MyApp1.db for the name key, and append for the subfolder key.
Elements Of .hdinamespace File
The content of any .hdinamespace file is always comprised of two elements, name and subfolder.

Element	Possible values	Default Value
"name"	
"<Any syntactically correct namespace of your choice>"
"" [no namespace specified]
"<application_name>.<HDB_module_name>"
"subfolder"	
"append"
"ignore"
"append"
To structure your runtime modeling content in a way that suits your needs, you can add a .hdinamespace file to any subfolder of the src folder. The "subfolder" setting determines whether the subfolder names should be added implicitly to the namespace for objects located in subfolders (append) or not (ignore).

As a consequence, your runtime modeling objects can be referencing several namespaces. For example, in your HDB module, you could decide the following convention:

Tables are prefixed with db.data.table, and
Calculation are be prefixed with db.models.dim and db.models.cube.
Caution

When you modify the specifications of the .hdinamespace file (or create a new .hdinamespace file in a folder), this change has an immediate effect on the identifiers assigned to your design-time objects.

However, when it comes to deploying these objects, the new namespace rules are considered only after the .hdinamespace file has been deployed. Before the deployment of the new .hdinamespace file, deploying these objects, for example, a calculation view with the new namespace specification, will fail because, on the HDI container's side, the former rules are still used to validate the runtime object identifier.

As a general recommendation, the namespace rules should be defined before you create modeling content. Indeed, if you modify these rules after creating modeling content, you have to adjust the object identifiers (which includes the namespace) before you can deploy your models. And this is even more complicated if dependencies exist between models.

Object Name
The way to define a runtime object name in design-time files is governed by one of the two following rules, depending on the object type.

Defining the Runtime Object Names
Type of modeling object	Number of objects per design-time files	Naming rule
Calculation views, table functions, procedures, analytic privileges	Each design-time file defines only one runtime object (*)	The runtime object name is defined inside the object content and should match the design-time file name (without file extension).
Synonyms, table data import definition, roles	Each design-time file can define one or several runtime object(s)	The runtime object name is defined only in the object content and is not linked to the design-time file name.
(*) Here, we mean, only one core runtime object. As you might have observed, even a simple calculation view generates a core column view, plus other child column views for the attributes, hierarchies, and so on.

Note

In this table, we aren't covering the entire typology of models, but only the main ones that are discussed in this course.
Let’s take two examples, one for each of the two approaches.

Object Name Example: Calculation View
Screen capture of the object name in the code of the design-time file and in the database explorer. both places show the namespace and the object name. The object name and the file name should match to find more easily a design-time file from the runtime object name.
Technically, the design-time file name for a calculation view can be different from the runtime object name.

However, we recommend that you always keep these names in sync to make it easier to find a design-time object based on the runtime object name.

Object Name Example: Synonyms
Screen capture of a .hdbsynonyms file which is used to define references that comply with the namespace rules applicable to the folder where the synonym definition file is located.
This second configuration example shows that a design-time file for synonyms can define several synonyms. In that case, there's no specific rule or recommendation for the design-time file name.

...

Deploying Models
Objective

After completing this lesson, you will be able to deploy modeling content to the runtime environment.
The SAP HANA Deployment Infrastructure
The SAP HANA Deployment Infrastructure (HDI) is a set of services that allows the deployment of database objects into isolated HDI containers.

HDI containers are simply database schemas, but they include additional features such as a dedicated owner of the container plus metadata to manage their database objects. Think of a container as a smart schema.

The objects included in the containers are defined in the HDB modules of your project, using source files and during deployment, the runtime objects are created and added to the container.

HDI takes care of dependency management and determines the order of deployment; it also provides support for upgrading existing runtime artifacts when their corresponding design-time artifacts are modified.

One key concept of HDI containers is the fact that the entire lifecycle (creation, modification, and deletion) of database objects is performed exclusively by the HDI. Therefore, you aren't able to directly create database artifacts, such as tables or views, with Data Definition Language (DDL) statements using SQL.

Data Sources for Calculation Views
When you develop calculation views, the data sources you choose must exist as design-time files in the container of your HDB module. So they must be of one of the following types:

"Real" database objects (table, view, table function, virtual tables, and so on) existing inside the container and having a corresponding design-time file in the HDB module
Synonyms referencing database objects located in any other schema than the container itself. You’ll learn about synonyms later on.
Caution

A calculation view that refers to a data source that was created directly using SQL in your container (so, without a source-file) can't be built. The build fails and issues an error message, such as:
Code Snippet

Copy code

Switch to dark mode
The file requires "db://HA300::TABLE_SQL" which is not provided by any file
Key Properties of HDI Containers
A container is create automatically when you deploy your database module for the first time.

A container generates a database schema the moment that a container is first created.

Database objects are deployed into that schema.

Source file definitions must be written in a schema-free way.

Direct references to objects outside your container aren't allowed. You must use synonyms.

HDI Container Configuration File
Within each HDB module of a project you'll find a very important and mandatory file that exists under the src folder with the suffix: .hdiconfig. This is the HDI container configuration file that is used to bind the design-time files of your project to the corresponding installed build plug-in.

Screen capture of the configuration file for the container. It contains the required plugins, and in this example the minimum plug-in version that applies to all plug-ins.
Without the build plug-in it isn't possible to deploy the runtime objects from the design-time files. This configuration file directs each source file in your project to the correct build plug-in.

Usually there's only one .hdiconfig file in an HDB module, and this must be located in the src folder of the HDB module. This file contains a list of all bindings for all source file types.

The minimum_feature_version entry of this file is there to prevent building your HDB module objects with a version of HANA Cloud that is lower than what is specified and/or what your code requires. For example, if you've designed and tested your HDB module with version 1004 of the build plug-ins, this will not deploy on an SAP HANA Cloud instance where version 1002 is in place.

Inside the container configuration file you'll find, for each source file type, two entries:

Suffix name, for example, "hdbcalculationview"

Plug-in name, for example, "com.sap.hana.di.calculationview"

It’s very important to remember that when you import a project into your landscape, it brings with it its own .hdiconfig file that refers to the feature version that was used when it was first developed. If you then plan to update the source file using newer features of SAP HANA (for example, you want to add a new feature to a calculation view that just became available with the newer version of SAP HANA) you will not see the new feature in the source editor if the .hdiconfig file hasn't been adjusted to use the later feature version.

Caution

The HDI container configuration file is an unnamed file and must only consist of the suffix .hdiconfig. If you try to add a name, the build of the .hdiconfig file will fail. Leave it as it is.
The SAP HANA PROJECTS View
In SAP Business Application Studio, when a project includes an SAP HANA DB module, a dedicated view SAP HANA PROJECTS is available (alongside the Explorer view).

Screen capture of the SAP HANA PROJECTS view in Business Application Studio. Use it to deploy a single object, a folder, or the module. You need to add a connection to an existing HDI container or one that you create as part of the binding.
The SAP HANA Projects view/panel is the materialization of the HDB modules defined in your projects. The upper part represents the folder structure of the HDB module. The root element corresponds to the path to the HDB module in the project. To create the runtime objects inside the connected HDI container schema, you can execute a deployment. This deployment can be triggered at different levels: a single file, a folder, or the entire HDB module.

Note

You can't manipulate (move, copy, delete, and so on) your design-time files from the SAP HANA PROJECTS view, but you can open them in their respective editor.
In the file structure, Pending Deployment identifies where a deployment is required for new or modified files. For deleted design-time files, you need to activate the run-time object deletion by deploying the folder that contained them.

The lower part represents the Database Connections between the HDB module and the HDI containers or services providing access to external resources.

A project with an SAP HANA Database module needs at least a corresponding HDI Container to deploy its runtime objects. This is what is shown in the image, The SAP HANA PROJECTS View.

In the connection part, you can bind an existing HDB module to a new or existing HDI container. You can also add new connections (for example to another container), and

The Database Explorer
From SAP Business Application Studio, you can launch the Database Explorer application to view the database artifacts (tables, views, procedures, column views) of one or several containers.

When you do this, even if you're logged on to SAP Business Application Studio with your usual SAP Business Application Studio user, access to the container is done by a technical SBSS (Service Broker Security Support) user that is created transparently when you add the container to the Database Explorer. This technical user interacts with the database objects on your behalf, for example, to view the content of a table or preview the data of a calculation view.

If your container consumes data from an external schema, the technical user must also be granted authorizations to the external schema objects, for example, to view the data of an external table referenced by a synonym. You'll learn more about this in the unit, Security in SAP HANA Modeling.

Deploying Modeling Objects
During development, you need to create or update the runtime objects from the design-time objects inside the container schema. For that, you use the Deploy function available in the SAP HANA PROJECTS view.

Deploying relies on the HDI container service (a Cloud Foundry service), which manages the interaction between SAP Business Application Studio, the HANA Deployment Infrastructure, and in turn the SAP HANA database.

When you bind an HDB module to an HDI container service, this binding creates the corresponding container schema, if it does not exist. After that, design-time objects can be deployed to the HDI container.

Structure of Building Modeling Content
When you deploy a HANA database module, the HDI deployer populates the HDI container, that is a schema located in the SAP HANA cloud database, with the runtime objects defined in the module.
A deploy operation can be executed at the following levels of a project structure:

An entire HDB module

A subfolder of the HDB module

One or several individual objects

Note

The deploy operation described above shouldn't be confused with what is also called Deploy in Cloud Foundry. In Cloud Foundry, the deploy command (cf deploy) applies to an entire application (for example, an entire MTA, including potentially more than just the HDB module). It’s used mainly when you want to transport and deploy/activate your project to another environment. For example, from the DEV to the QA or PROD environment.

Regardless of what you deploy (HDB module, subfolder, or single object), there's always an end-to-end dependency check applied to each object that is part of the build scope. So, even if you build a single object, there will be a recursive check of all the objects it references to make sure these objects already exist as runtime objects or are defined in a design-time object and built at the same time.

Deploy Errors
When you deploy models defined in an HDB module, you might come across different issues, which are described in the log that you can consult in the console. Some are related to the dependency checks we've just discussed. Others have to do with the consistency of design-time objects (for example, the namespace settings of design-time folders). Another classical build error is when two different design-time files provide a definition for the same runtime object.

Here's a list of frequent root causes for deploy errors.

Most Common Errors During Build Operations
The definition of an object on which another object depends isn't provided, or it's provided but this object hasn't been deployed yet.

The definition of a runtime object is provided by several design-time files.

There's a namespace inconsistency between the content of a design-time file and the namespace property of the folder in which it's located.

The service to access external schemas isn't defined or not available in the target space.

The synonyms for external schema access aren't deployed yet.

There's an object-specific design error.

For example, no measure is defined in a Cube calculation view.

There's an inconsistency between the object you build and a referenced object.

For example, there's a mismatch in a column name between two objects with a dependency.

Putting a Design File Aside to Work Around Build Issues
When a Calculation View cannot be built for whatever reason, it generally prevents the build of its containing folder and any upper folder. There are several approaches you can use to solve this issue temporarily and go on working on the rest of your models, without losing completely the existing design-time file.

Export the Calculation View to a safe place, and then delete the Calculation View from your workspace.
When executing the build, the removed Calculation View won't cause any issue. When you want to resume working on this view, you just need to import it back to its original location.

Add .txt to the design-time file. For example, SALES.hdbcalculationview will become SALES.hdbcalculationview.txt.
In this scenario, you don't remove the file from the workspace, but modify its extension so that it's (temporarily) not considered as a Calculation View design-time file but a plain text file. No specific check is performed on a .txt file upon build, so it will have the same effect as removing the file, but it's then much easier to undo: you just need to remove the additional .txt extension.

...

Manage modeling content
Objective

After completing this lesson, you will be able to manage modeling content.
Import and Export of Modeling Content
SAP Business Application Studio offers import and export features within the development space.

These features can be used to keep several versions of your modeling content, and re-import all or part of this content if needed. They're also useful for quick sandboxing, moving content from one development space to another.

Additionally, import/export can be used to share in a simple way your modeling content with another developer. For example, when a series of synonyms to external database objects have been defined in your project, you can easily share the synonyms definitions with another developer by sending the relevant design-time files for synonyms.

Correspondingly, you can import an individual file or an archive.

Exporting Modeling Content
An export of modeling content can be executed at different levels of a project structure, and applied to either a single file or a folder. After selecting a file or folder, you use the File → Download feature. You can also right-click the file or folder and choose Download in the context menu.

A single design-time file is exported/downloaded as is, with the source file name and extension.

A folder within the project structure (up to the project folder itself) is exported/downloaded as a .tar archive.

To export an entire project opened as a workspace, you can right-click the blank area below the file structure and choose Download. You can also choose all the files and folders under the root of the project and choose File → Download.

Several files and/or folders can be selected and exported/downloaded as a single .tar archive.

Importing Modeling Content
Corresponding to the export, you can either import an individual file or a .zip or .tar archive. Three features are available for that:

Drag and drop from your file system into the Explorer view. Archives such as .zip and .tar files are not extracted. This can be done in command line in a terminal window.
Upload Files (from the File menu) and Explorer view context menu. Archives are not extracted.
Import (from the Welcome page) or Import Project(from the File menu). .zip and .tar archives will be extracted.
Caution

If the content you import is an entire project, for example a proj1.tar archive, you need to open the projects folder first, so that the imported project becomes a child of that folder: projects/proj1
After importing a project, you need to open its root folder as a workspace, or to add to a multiroot workspace. Please refer to the lesson, Creating a Project in SAP Business Application Studio.

Copy, Rename, Move, and Delete Modeling Content
The structure of a project in SAP Business Application Studio looks like a classical file structure, but the changes you make to this structure by copying, renaming, moving, or deleting files (or folders) can have strong impact on the consistency of your project. This impact doesn't necessarily show up immediately during the file structure modification. It generally appears when you deploy the modeling content.

Whenever you copy, rename, move, or delete modeling content, you must keep in mind the key rules that govern the deployment of database artifacts:

Main Rules for a Consistent Management of Modeling Content Files
In an entire HDB module, the definition of a given runtime object (<namespace>::<object_name>) can't be provided more than once.

The namespace defined in the design-time file of a database object must correspond to the namespace setting applied to the folder in which it's located.

A deploy operation always checks the end-to-end dependency between modeling content across all the HDB module, but only deploys the design-time files you've selected for the deploy operation.

During a deploy operation, the checks apply to all the runtime objects that are already built, and also all of the objects included in the deploy scope (that is, in case of a partial deploy, the design-time files you've selected).

A design-time file that has never been built and isn't part of the deploy operation is ignored.

Copying Modeling Content
When you copy/paste a design-time file within the same folder, you must provide a different name for the copy. This isn't the case if you copy/paste a file to a different folder where there isn't a file with the same name.

However, in any case, the content of the design-time file isn't modified at all. Which means you have in your HDB module two design-time files with identical content.

At least, you must ensure that you rename all the runtime objects defined in the design-time file to make these objects unique in the whole HDB module. And, if needed, you must also change the namespace according to the target folder namespace settings.

Note

If you copy a design-time file that was already opened in a code or graphical editor, keep in mind that copy/pasting doesn't open the new file (the copy). The file visible in the editor is still the original one, when – in many circumstances – you might want to actually modify the copy. This especially true if the File → Auto Save feature isn't active.
Moving Modeling Content
When moving modeling content, one key rule that you must think about is the namespace setting of the target folder.

If the namespace settings for source and target folder are the same, moving the file has no particular impact on objects that reference the moved object.

However, a deployment of the moved object can be successful only if the deploy scope includes the source folder, in order to execute/acknowledge the deletion of the moved file in its source folder. If not, the deployment will fail.

If the namespace settings for source and target folder are different, you must at least adapt the namespace. In this case, you must first perform an impact analysis to check whether some existing objects reference the one you're about to move.

Renaming Modeling Content
For the HDI deployer, renaming a design-time file is like deleting a design-time file and creating a new one with the same content. You must be aware of the following rules and recommendations:

A design-time file that creates only one core runtime object (for example, a calculation view or a table function) should always have the same name as the runtime object it defines. For example, after renaming the design-time of a calculation view, it's recommended to also change its ID in the code by opening the ID of the runtime object.

This isn't a technical requirement, but rather a best practice to keep your design-time content readable.

You must check if database objects reference the object you plan to rename. Generally speaking, renaming objects that have dependencies requires modification to the references to this object in other objects. Ideally, you should do this only on exception, because it's error-prone, or limit this practice to test objects, typically when you copy – and rename – an existing object in order to test changes you make to this object while keeping the source object.

You must be particularly careful with partial deployments. If you rename an object that was already built and had dependencies, the renamed file is seen by the HDI deployer as a new design-time. The original design-time file will be seen as deleted by the HDI deployer only when you deploy either the entire HDB module, or any of it subfolders that contains the renamed file.

Caution

If you rename a source file but choose not to rename the runtime view (which is bad practice), when you build this individual source file you'll have a build error. This is because the builder considers the renamed source file as a new design item and so doesn't delete the original runtime. This means that the new source file tries to deploy a runtime view using an already used name, and this is the error (deployment can't create a duplicate runtime object name). To overcome this issue, you must build at the folder level so that the original runtime view is first deleted and then a new runtime view (with the same name) can be deployed.
Deleting Modeling Content
Deleting modeling content can have surprising effects, especially in case of partial deployments. Indeed, the deleted design-time file is only seen as deleted by the HDI deployer when you deploy the entire HDB module, or any of the subfolders that contained the deleted file.

In other words, the smallest deploy scope you can think of, in order to validate the deletion of a design-time file, is the folder in which the deleted file was located. So, even if it's empty, this folder is extremely precious.

Indeed, if you delete this folder as well, you can only validate the deletion of the design-time files it contained by deploying a parent folder. Suppose that this parent folder issrc folder and it contains one (or several) models that you know can't be built at the moment (for whatever reason).

Therefore, when deleting modeling content, you should apply the following best practices:

Before deleting a design-time file, always perform an impact analysis.

Before deleting a folder, always perform a partial deployment at this folder level. If the deployment is successful, it means that all the runtime objects that were defined in design-time files from this folder have actually been (successfully) deleted.

Caution

As already mentioned, you should NEVER delete a database object located in an HDI container with a plain SQL statement.
Using the Search/Replace Feature
SAP Business Application Studio offers several find/search and replace features that are useful to manage the renaming of objects and the code adjustments that must be performed manually.

Within the Code Editor, you can use Edit → Find (Ctrl+F) or Edit → Replace (Ctrl+H) to locate easily (and replace if needed) the ID of an object.

You can also use the Search view to look for a particular text string within the content of the design-time files. You can directly open a design-time time from the search results (not from the file name, but from one of the search hits within the file content).

The Toggle search details button (…) allows you to define file/folder patterns to include and/or exclude.

From the Explorer view, you can also right-click a folder and choose Find in folder. This will invoke the same Search view, but this time the folder you choose is defined as the "root" for the search.
Another useful search feature is available for the file names inside the Explorer view. Just select an element (folder, file) in the workspace, or the blank area below, and start typing a string to search. A fuzzy search tool shows up, allowing you to highlight or filter matching file names.
Rename Columns
In SAP Business Application Studio it's possible to rename one or several columns of a Calculation View from the Semantics node.

Renaming a Column in an Intermediate Node
It's possible to rename a column in any node of the calculation scenario.

Note

The case of the top node is specific and is discussed later on.

Inside a calculation view node, you can rename the output columns only, and not the source columns. Before you can do this, a column must first exist in the output in the Mapping tab. Then you have two main options to rename the column:

From the Mappings tab:
Select the column in the Output Columns area. Then, in the PROPERTIES area, modify the Name field.

From the Columns tab:
The Name column allows you to change the name of several columns at once.

Note that the impact of renaming a column on the upper nodes of the calculation view depends on the upper mapping status.

If the column wasn't yet mapped in upper nodes, the new column name will be used when adding it to the output of upper nodes, up to the top node.
If the column was already mapped in upper nodes, renaming the column applies only locally: At the next level, the column is mapped back to a column with the original name.
Renaming a Column at the Top Node Level
This case is different from the one above, because the column names in the output of the top node (similar to the ones shown in the Semantics) are used to consume the calculation view by front-end tool, SQL queries, and so on, but also by other calculation views.

Technically, columns of the top node can be renamed in the same way as discussed before: from the Mapping or Columns tabs. In addition, the Semantics allow you to maintain a number of column properties, including the Name and Label. You can also use the Rename and Adjust References tool in the Columns tab of the Semantics.

For a Calculation Views that is consumed by another Calculation View, a change in column name could prevent the build of the consuming calculation views. So the consuming views must first be modified so that they reference the actual column name of their data source.

From SAP HANA Cloud QRC 3/2023 onwards, it is possible to use the option Propagate Recursively in Consuming Views of the Rename and Adjust References tool, for a single column at a time. This column name is then renamed recursively, up to the last calculation view of any calculation view stack.

Caution

Calculation Views exposed externally must be handled with special care in order to guarantee they can be used without disruption. This includes, among others, column names.

Generate Properties File for Calculation Views
For a calculation view, you can generate a properties file that contains the name and description of all calculation views objects such as columns, input parameters and variables.

You first generating the properties file for the calculation view and then you must build the SAP HANA Database Module that contains this generated file so that the names and description values are then stored in the BIMC_DESCRIPTION table.

One of the key reason for doing this is to enable translation of the name and description values to multiple languages by updating the BIMC_DESCRIPTION table.

Client tools can read the BIMC_DESCRIPTION table and display values in the reporting tools respectively.

To generate the properties file, select a calculation view and choose the menu option Modeling → Generate Properties File.

Deprecate Calculation Views
You can deprecate a calculation view when it should not be used any longer. This is done in the Semantics, in the View Properties → General tab.

Two different settings are available, depending on whether you want to keep the deprecated calculation view available for queries or not:

Deprecated and Queries Permitted
The view is deprecated but queries can still be executed on the view. In the calculation view editor, a warning icon and a tooltip informs the user if a calculation view node references deprecated calculation views.

Deprecated and Queries Blocked (new in SAP HANA Cloud QRC 1/2024)
In addition to the information provided in the calculation view editor, queries on the view are also blocked and a customizable message can be defined to notify the end users, together with the default error Instantiation of calculation model failed;Model '<calculation_view_name>'.

To mark a calculation view as deprecated:

Open the calculation view in the graphical view editor
In the Semantics, choose View Properties → General.
In the Deprecation Option dropdown list, select one of the possible options.
(Optional) If you chose the Deprecated and Queries Blocked option, define a customized message in the Deprecation Note field. For example: Use Calculation View <new_calculation_view_name> instead.

...

Knowledge quiz
It's time to put what you've learned to the test, get 3 right to pass this unit.

1.
A runtime object in a container must always have a corresponding source file?
Choose the correct answer.

True

False
2.
In a project structure, which of these appears directly beneath the project?
Choose the correct answer.

Folder

Group

Module
3.
Why do you set the deprecate flag in a calculation view?
Choose the correct answer.

To prevent business users from accessing the calculation view.

To prevent developers from consuming the calculation view.

To provide a warning to the business user suggesting they shouldn't use the calculation view.

To provide a warning to the developer suggesting they shouldn't use the calculation view.

......


Unit 18
Managing the Lifecycle of a Modeling Project
After completing this unit, you will be able to:

Create a modeling project in SAP Business Application Studio
Set up access to external data.
Manage source files using Git.
Manage the lifecycle of a project.
Creating a Project
Enabling Access to External Data
Using Git to Manage Source Code
Deploying an Application

...

Creating a Project
Objective

After completing this lesson, you will be able to create a modeling project in SAP Business Application Studio
Database Project
SAP HANA Cloud provides templates for different types of projects. In the context of modeling, we usually use the template for an SAP HANA Database Project.

Use the template wizard to create your project. During the creation steps, you'll be prompted to choose various settings. HDB module can define a local persistence (a persistence layer that is defined and deployed in the application itself). When you build a virtual data model based on calculation views – even if the source data is stored outside of the application – the HDB module contains the database artifacts that define this virtual data model.

When you create a new HDB module, you must provide a number of properties for the module. Most of these properties are stored in the descriptor file of the application: (mta.yaml), except for a few. In particular, the namespace property of the module isn't stored in the descriptor file but is materialized by the .hdinamespace file located in the src folder.

Key Settings of an HDB Module
Setting	Description	Example
Project Name	The name of the project	sales_analytics_project
Module Name	The name of the module. It should be unique within the entire descriptor file (mta.yaml).	core-database-module
Type	hdb is the reserved module type for an HDB module.	hdb
Path	The relative location of the module root within the file structure of the project.	db
Namespace	Optional: this will be the prefix to all generated object names	sales
Schema Name	Optional: Choose a specific name for the HDI container schema used for deployment. See below for more details.	SAMPLE_SCHEMA
SAP HANA Database Version	 	SAP HANA Service (legacy service in SAP BTP) or HANA Cloud.
Default Schema Name of the HDB Module Container
During the creation of an HDB module, specifying a schema name is optional.

If you don't specify a schema name, it's generated during the deployment of a project with the following name structure:

Code Snippet

Copy code

Switch to dark mode
<project_name>_<hdi_service_name_in_MTA>_<numeric_increment>
For example: MYPROJECT_HDI_DB_1

To identify the name of the generated database schema, open an SQL console connected to the container and execute the following SQL query:

Code Snippet

Copy code

Switch to dark mode
select CURRENT_SCHEMA from DUMMY;
Specifying a Schema Name for the HDI Container
If you prefer not to have the schema name generated and would like to use a specific schema name, this is possible. This is done in the descriptor file mta.yaml of your project, and more specifically in the parameters of the hdi-container service that is defined as a resource used by the HDB module.

The following sample shows the additional parameters section of the mta.yaml file where the schema name is defined:

Code Snippet

Copy code

Switch to dark mode
resources:
 - name: hdi-cont
   parameters:
     config:
       schema: <YOUR_SCHEMA_NAME>
   properties:
      hdi-container-name: ${service-name}
   type: com.sap.xs.hdi-container
Then, the schema generated during the first build of the HDB module is named as follows:

Code Snippet

Copy code

Switch to dark mode
<YOUR_SCHEMA_NAME>_<numeric_increment>
Caution

Although it's possible to force the schema name to be exactly what you specify in the mta.yaml file, by adding a parameter in the config section, this can generate schema naming conflicts in case several projects use the same schema name, so is usually not recommended, and you should allow the schema name to be generates automatically as a unique value.

Create a Project with an HDB Module
Demo
Start Demo
Bind an Existing HDB Module to an HDI Service
Demo
Start Demo
HDI Cockpit
Containers play a key role in database development and provide an efficient method of isolating the database runtime artifacts and encouraging modularization. Containers are a key component of HDI development in SAP HANA Cloud. User and roles are granted privileges on containers so that they can access the resources they contain.

An SAP HANA Cloud database will usually have many containers, which have been automatically generated when developers deploy database artifacts to the runtime. Each container will have various combination of users and role privileges assigned. This can soon become difficult to manage and so troubleshooting and monitoring can become difficult.

Screen capture of the HDI Administration cockpit. For details, refer to the surrounding text.
The HANA Deployment Infrastructure (HDI) Cockpit is a tool that provides visibility of all containers. You can add them into container groups and display the users and roles that have been granted access to them.

...

Enabling Access to External Data
Objective

After completing this lesson, you will be able to set up access to external data.
External Data Access Setup
The SAP HANA Deployment Infrastructure (HDI) relies on a strong isolation of containers and a schema-free definition of all the modeling artifacts.

The isolation principle: By default, the Object Owner user of one container has access to all objects of his container, but no access to external data, neiter to another containers nor to a classic schama.
By default, a container has full access to all of the database objects it contains (tables, calculation views, and so on), but has no access at all to other database schemas, whether it’s another container's schema or a classic database schema.

But it's possible to enable access if it's required.

Defining Access to an External Schema – End-to-End Scenario
Let’s consider the end-to-end setup of an access to an external schema.

The object owner gets access to a classic schema via an .hdbgrants file that references a user-provided service to which a technical user with relevant authorization is assigned.
Create the User Provided Service
To enable access to external schema you need a user-provided service. This can be created in SAP Business Application Studio.

You simply choose the name of the service and a user id and password that has privileges to all external objects that you wish to access through the service. These privileges in turn will be granted by this service user, to the container's technical user and application users using a .hdbgrants file.

Hint

After you create the service, open the mta.yaml file, and notice how the user-provided service has been automatically added as a resource assigned to your HDB module.
Watch this video to learn how to prepare to setup a User-Provided Service.

Watch this video to learn how to define the User-Provided Service.

Create the HDBGRANTS File
Before we describe the .hdbgrants file, we need to describe two types of user that are referenced in the file.

Object Owner:
When a database artifact is deployed to a database container, it is owned by the technical user of the container, NOT the developer who created the source file. Only the object owner can alter and drop the object. During deployment, the object owner must have privileges to access any external objects that have been referenced in any calculation view.

Application User:
The application user is used to bind the HDI container to a consuming application. In the context of calculation view modeling, it is typically the user <container_schema_name>...RT who performs a number of modeling operations, including data preview on top of the HDI container connection, for example.

The privileges granted to these two roles are generally different. In particular, the object owner always needs the authorization to grant the privileges he has (for example, to include them in a role you define inside the HDI container or a default one), whereas the application user generally does not need the right to grant the privilege he is given.

The user-provided service's user may have a lot of privileges on external objects. Technically, it would be possible to grant all the database privileges of that user to the object owner and application user. But this gives away too much access. Instead, we should grant only the privileges that are needed, in other words, a subset of the service user's privileges.

For example, if the service users has access to an entire schema, the object owner and application user might only need access to a few tables or views in that schema. Remember, the service user might be shared with other development projects with different privilege requirements.
We define the privileges required by the object owner and application user using a special file with the suffix .hdbgrants. In that file, we specify the name of the user provided service and the privileges that should be granted to both the object owner and application user. When this file is deployed, the privileges are granted.

Watch this video to learn how to create the HDBGRANTS file.

Revoking Privileges and Roles
The .hdbgrants design-time files are special compared to other files types (such as calculation views, functions, or roles) because they do not materialize as database objects when you build the database module.

For this reason, if a certain privilege or role has been granted to the object owner and/or application user when you last built the .hdbgrants file, this will not be automatically reverted if you remove the privilege/role from the .hdbgrants file and build the file again.

Instead, you need to create a .hdbrevokes file, with the same structure, listing (only) the privileges and/or roles that you want to revoke. You must also remove these privileges and/or roles from the .hdbgrants. This will be effective after you build the .hdbrevokes file.

Caution

During deployment, the .hdbrevokes are processed before the .hdbgrants files, so a privilege or role listed in a .hdbgrants file will always be granted, whatever a .hdbrevokes specifies.
To avoid this complexity, a common practice is to define fine-grained authorizations to external objects in two roles (one for the object owner, another for the application user), out of the HDI container, and to grant these two roles to the user-provided service's user. Then, the .hdbgrants file only needs to reference these roles. If needed, additional privileges and/or nested roles can be granted to (or revoked from) these two roles and are then automatically granted to (or revoked from) the object owner and/or application user, without even building the local HDB module.

Creating Synonyms
The final step in the setup of external schema access is to create synonyms that point to the target objects of the external schema. The synonyms declaration is done in a .hdbsynonym file. This file type can be edited either with the text editor, as in the example below, or a dedicated synonym editor.

Example of a Synonym Source File
Code Snippet

Copy code

Switch to dark mode
 {
  "HA300::SALES_DATA": {
    "target": {
      "object": "SALES_DATA",
      "schema": "TRAINING"
    }
  },
  "HA300::SALES_DATA_NEW": {
    "target": {
      "object": "SALES_DATA_NEW",
      "schema": "TRAINING"
    }
  },
  "HA300::HANA_SEARCH_DATA": {
    "target": {
      "object": "HANA_SEARCH_DATA",
      "schema": "TRAINING"
    }
  },
  "HA300::CUSTOMER": {
    "target": {
      "object": "CUSTOMER",
      "schema": "TRAINING"
    }
  }
}
When defining a synonym, there are three key parameters:

Name of synonym: You will refer to this name whenever you need to access the target object from the HDI container
Object: The actual object name in the target schema, such as a table name.
Schema: Where the target object is found.
Watch this video to learn how to define Synonyms and Test Access.

Cross-Container Access
The object owner gets access to another container via an .hdbgrants file that references a role with relevant authorization for the external objects. In addition, a synonym is needed for each external object.
When you need to access data from another HDI container, the setup is relatively similar to what you've just learned for an external (classic) database schema. Let’s point out the main differences:

There's no need for a user-provided service if the external container service is running in the same space as the one your project is assigned to.

You can add the external HDI container service to your project (which automatically adds it to the mta.yaml file).

You must create roles inside the external container that contain the relevant privileges to all objects that could be accessed by the service.

The .hdbgrants file doesn't refer to database object privileges of the technical user assigned to the user-provided service, but to the dedicated roles created inside the external container (see the previous point).

...

Using Git to Manage Source Code
Objective

After completing this lesson, you will be able to manage source files using Git.
Overview of Git
Why Should You Use a Version Control System?
It's very likely that there will be multiple developers working on the same modeling project. The developers need to work together, and this means not overwriting each other's work, making sure they use the latest version of any file, and also being able to return to a previous version of any file if errors are found. To handle these requirements a source code version control system is needed.

The source code version control system needs to be able to handle parallel developments. Let's take a look at a common scenario:

So, how do you handle this request? At the moment, your current development isn't finished, not tested, but you need to patch your version 2.1.0. Of course, you want to start from the last release (you don't want to include any part of the future functionality into the patch), but your patch might affect some files that you already modified as part of the development of new features.

This is where a version control system comes into play. It allows you to keep a complete change history by using kind of milestones during the development of your code, at a very fine-grained level if needed. And you can also branch your code, which means, you can develop and test different features in different parallel development threads (branches). If a feature branch is good to go, you can merge this branch with the main branch. If it isn't, you can continue your development, or even get rid of this branch if you realize that a development option for a feature wasn't relevant and you need to think about it again.

Key Benefits of a Version Control System
Source code backup

A complete change history

Branching and merging capabilities

Traceability (for example, connecting a change to project management software)

Git in a Nutshell
Git is a Distributed Version Control System (D-VCS) used to manage source code in software development.

It allows one or several developers to work locally with their own copy of the Git repository.

Note

Git can also be used even out of a collaboration context, to help you control the development of a project on which you work alone, thanks to a number of capabilities.
The architecture of Git is distributed. It's somewhere between purely local and purely central. A local architecture would make it difficult for several developers to collaborate. A centralized one allows collaboration, but in some cases, a developer need to block a piece of code on the central repository while working on it.

Instead of this, Git is designed so that every developer can keep the entire history of a project (or only a part of it, depending of their needs), locally on their computer.

Git is a free software distributed under GNU GPL (General Public License).

Note

The initial purpose of Git is NOT code review or automated deployment of applications. For this, other tools exist – often with advanced connectivity to Git – such as Gerrit for code review or Jenkins for automated integration and deployment.
Git Architecture
Note

In Git, it's even possible for a developer to define the local repository of another developer as a remote repository and to synchronize their development. This requires a network access and the relevant authorizations.
The shared Git repositories can be hosted internally on a company’s IT infrastructure, or on public Git hosting services such as GitHub.

Lifecycle of Files in Git
When you work with files locally in Git, this involves three major logical areas.

The working directory.

The staging area, also known as INDEX.

The (local) Git repository.

These aren't all real areas, in the sense that a given file isn't necessarily materialized in each of them. Let’s explain this with a diagram.

The way to manage files in a local Git repository is very straightforward, relying on a small number of actions.

The staging area is the virtual place where you put all the modifications that you want to include in your next commit.

Note

When your working directory contains changes that you do NOT want to include in your next commit, you just need to make sure you do NOT stage the corresponding files before committing.
The Git History
Git is very good at representing the history of a project in a simple way, as the succession of commits.

Over time, all the commits you execute in your project are added to the history, and each commit (except the initial one) references its parent commit.

With each commit, Git keeps record of the commit date, the identity of the developer who executed it, and useful information about which files where affected (added, modified, or deleted).

The Branch Concept in Git
Branching is one of the core concepts of Git, which provides a huge flexibility at a very low cost. So what is a branch?

From a conceptual standpoint, a branch is a series of commits. Technically, a branch is just a pointer that references a particular commit of the project history.

Each Git repository always has at least one branch. The default branch is generally called master.

For many different purposes, you can create a new branch and commit your changes to an existing branch.

Let’s describe the figure, The Branch Concept. After commit C3, a new branch Feature32 has been created to support the development of a new feature. Two commits, C5 and C6, have been made to this branch. In the meantime, additional changes have been committed in the master branch (commit C4).

Git Usage
Now that you are familiar with the Fundamentals of GIT lets have a look at the options available for GIT with SAP Business Application Studio.

SAP Business Application Studio has a local repository for GIT the recommendation is always to have the work saved in remote repositories. Remote repository can be public or private aka Corporate GIT.

For this course we will focus on using the Public GIT in particular github https://github.com/. As mentioned earlier you can use any other if you already have an account or are familiar with the tool.

SAP Business Application studio comes with predefined set of development environments, auto tools, plugins to support Github so no additional tool or installs are required just a few prerequisite that have to be met.

Prerequisites
Account on https://github.com :
Log on to https:// github.com and follow the account creation process step by step.

Repository on https://github.com :
Once Account is created navigate to repositories in the menu and create a repository.

Once this prerequisites are met you can start to link the Local Repository to Github repository process in SAP Business Application Studio in your Dev Space.

Setup your Git user name, email :
git config --global user.email "git_email@company.com"

git config --global user.name "user_name"

Note

This process is carried out on the root folder Terminal → New Terminal → cd projects
git config –list is used to retrieve the user credentials saved during configurtaion.

Initialize the local directory as a Git repository :
git --init

Here we initialise the Local directory as a Git Repository. This point it is local and not written to linked with the repository
Add the files in your new local repository :
git add .

Once we have initialised the repository we then can add the files to this repository ready for the first commit.

Commit the files that you've staged in your local repository :
git commit -m "First commit(Free Text)"

This step commits the stages files to the Local Repository.

Create a new :Mainbranch.
git branch -M main

This will create a new Branch named Main. This can be any name.

Provide the Remote Repository Link :
git remote add origin https://<Git-Host-URL>/<full-path-to-repository>

Push to Remote Repository :
git push origin main

User ID and Password :
User Name and Password for Git is asked you can enter it here and proceed, to avoid this step for every push we have below options.

Using Token :
SAP Business Application studio support Personnel access tokens instead of passwords

Personal access tokens (PATs) are an alternative to using passwords for authentication to GitHub when using the GitHub API or the command line i.e Personal access tokens function like ordinary OAuth access tokens. They can be used instead of a password for Git over HTTPS

Personal access tokens can only be used for HTTPS Git operations. If your repository uses an SSH remote URL, you will need to switch the remote from SSH to HTTPS

You should create a personal access token to use in place of a password with the Github UI or with the command line

To create a token, follow the instructions described in the GitHub documentation

Creating personal access token

This slide show how to create a personal access token.
In the upper-right corner of Github page,click Settings
Click Developer settings
Click Personal access tokens
Click Generate new token
Provide a name for the Token
Select Scopes, Permissions, to use your token to access repositories from the command line, select repo.
Using a token on the command line

git clone https://github.com/username/repo.git

Username: your_username

Password: your_token

Caching the Password / Token : .
This avoid the efforts to key in the credentials on every push.
Git Credential helper :
Enter below commands once you login successfully when you push first time from Terminal

git config credential.helper cache –timeout=3600000 (time in milliseconds)

git config credential.helper store

To erase credentials use below command i.e to disable this cached username/password for your current local git folder user

git config credential.helper ""

Note

Other option exists but not covered here. Example usage of .netrc
Git using UI option :
In addition to the Command Line option SAP Business Application Studio provides a graphical user interface for executing Git commands and managing your source control and versioning.

Screen capture of the UI for git and the workflow steps. First, initiate git. Second stage all. Third, commit. Fourth, push to the repository.
Working with Files in a Git Project
Modifying your project content (with or without Git) means creating, modifying, and deleting files. With Git, all of these changes are tracked in your working directory as soon as they are made, for example, when you save a file you have just modified and listed in the File Status area.

Then, for each file, you have the following possibilities:

Stage the modification so that it will be included in the next commit

Leave the modification unstaged (it will not be included in the next commit)

Discard the change

Staging or discarding can also be done for the entire set of modifications.

The next step is to commit your changes, which will add a next commit to the history of the branch.

Note

The concept of branches in Git, already introduced, will be discussed in more details later on. For now, let’s just consider that you are working on a single local branch, for example, master.
Schema of the workflow steps. First, check out files from the local git repository. Then, create or modify files and save them. Then, stage all files. Then, commit and push to the repository.
To materialize this workflow, each file is assigned a Git status. This status is represented by an icon in SAP Business Application Studio workspace and/or the File Status area of the Git pane. The list of possible file statuses is as follows:

Meaning of the git status of files. A is new and staged, U is unstaged. M is modified and unstaged. Unstaged changes can be reverted. D is deleted (staged or unstaged). A red file with C is a conflict during a merge or rebase operation. A blue file with C is a copy.
Note

The Conflict (C) status will be discussed later on.
Committing Changes
When you commit changes, you add these changes to the Git history and provide a commit description. From this moment on, the Git History pane will include this commit, and provide the identity of who committed, the date, the commit description, and details on which files were affected by the commit. Note that committing changes does not modify your working directory. The committed files are still available for further modification, and the new or modified files you haven’t committed yet are still here for an upcoming commit. You can also discard the changes to these uncommitted files.

The commit description provides important information to allow the readability of your changes, in particular when you collaborate with other developers. You can find a number of blogs and tutorials on how to write a good commit message. The following are recommendations for writing a commit message:

Start with a relatively short subject (50 characters max), using imperative mood

Separate the subject and body with a blank line

Provide info about what the change does, and why (for example, what issue it solves)

If relevant, provide the URL of a related issue or specification in another system (for example, JIRA)

It is possible to amend a previous commit. This allows you to replace the very last commit by a new one, after you have staged additional files. The original commit description is displayed so that you can modify it before committing again.

Caution

You must not amend commits that have been shared with other developers, because this would modify a (shared) history on which others might have already based their work.
More information on Git capabilities for SAP Business Application Studio can be found at https://help.sap.com/viewer/9d1db9835307451daa8c930fbd9ab264/Cloud/en-US/265962e20eee43f499516de9011ac2e3.html

Set up Git in SAP Business Application Studio
Demo


...

Deploying an Application
Objective

After completing this lesson, you will be able to manage the lifecycle of a project.
Application Deployment
You've already learned how to manage some important stages of a modeling project’s lifecycle.

Create a project and define key settings.

Import and export modeling content.

Deploy selected objects or an entire HDB module.

Let’s now discuss the deployment of an application. We'll consider the deployment of a simple application made up of one or several HDB modules. Remember, applications often include other types of modules such as HTML, NodeJS, Java, and so on. These full-stack applications are known as multitarget applications (MTA). The same deployment principles described here apply.

Application Deployment Versus Database Objects Deployment
Let’s first make a clear difference between deploying database objects and deploying an entire application.

The workflow of model deployment tasks. First, deploy (build) database objects in a module in a DEV landscape. This creates a container (database schema) with database objects. Then, build the whole project. This creates an MTA archive. Then, export the archive. Then, import the MTA archive in another landscape, such as PROD. This creates a container there.
When you work on a project, you deploy modeling content to the HDI container on a regular basis, in order to test it, check dependencies, and so on. This is something you trigger from SAP Business Application Studio. To do that, you need to have a modeler role for the space in which the HDI container bound to your HDB module sits.

When you deploy an HDB module (or some of its individual source files), if the deployment is successful, the corresponding runtime objects are generated in the HDI container, running in the corresponding development space.

On the other hand, deploying an application is something you do in a target space at a specific point in time, for example, to test the application in a QA environment or set the application to production. Deploying an application can be done outside of SAP Business Application Studio. The user who deploys the application doesn't need to have a developer role in the target space.

To deploy your modeling content to a target environment, such as QA or PROD, you must do the following:

Build the HDB modules.
Build the entire project in order to generate an MTA archive file (.mtar).
Export the MTA archive.
Deploy the MTA archive to the target landscape.
Between steps 3 and 4, you must, of course, make the MTA archive file available to the target landscape, for example, by copying the file to a specific folder or using a dedicated transport tool.

Note

As you see from the figure, Deploying Models, SAP Business Application Studio isn’t involved during the deployment (step 4) on the target landscape. In particular, you don't need to create and build the project in the target landscape.
Source Files for Deployment
The key source file for application deployment is generated by a build operation at the project level in SAP Business Application Studio. This creates a .mtar (MTA archive) file, which is technically a .rar compressed file. This file is named <Application ID>_<version>.mtar and can be found in your workspace, in the folder mta_archives. This archive contains the file mtad.yaml, which is an equivalent of the mta.yaml file but specific to application deployment.

Another important (though, optional) file used for deployment is the .mtaext file. This is an extension file in which the space administrator who deploys the application can specify additions or modifications to the mtad.yaml file in order to pass the relevant parameters for the target environment during deployment. Typical examples of what you can specify in the extension file include the following:

The name you want to give to the HDI container schema

The actual name of a user-provided service in the target environment

The actual name of another HDI container service in the target environment

To generate the .mtar file you open a Terminal session in SAP Business Application Studio and type the command mbt build. Make sure you navigate to the root folder of the project you want to build before executing this command. From the root project folder, use the command cd to change directories, for example, cd my_app.

The name of the HDI containers created during the deployment of your application is also an important parameter. This is the name of the HDI container resource declared in the mta.yaml file (by default, when you create the HDB module, hdi-<HDB_module_name>).

...

Knowledge quiz
It's time to put what you've learned to the test, get 3 right to pass this unit.

1.
Git is used to automate the deployment of runtime objects in SAP HANA Cloud?
Choose the correct answer.

True

False
2.
Which database artifact do you define to access external schema objects?
Choose the correct answer.

Calculation view

Synonym

User-provided service
3.
The name of the schema that corresponds to a container is generated and can't be changed.
Choose the correct answer.

True

False
4.
What is the type of file that is generated when you deploy a complete application?
Choose the correct answer.

.zip

.mtar

.tar

......

Unit 19
Implementing Security in SAP HANA Modeling
After completing this unit, you will be able to:

Define analytic privileges.
Create a design-time role to secure data access.
Restrict access to columns containing sensitive data.
Defining Analytic Privileges
Defining Roles
Masking Sensitive Data

...

Defining Analytic Privileges
Objective

After completing this lesson, you will be able to define analytic privileges.
Analytic Privileges
Analytic privileges are used to enable data access in calculation views by filtering the data based on the values of one or more attributes.

The rationale for analytic privileges is to allow the use of calculation views by different users who might not be allowed to see the same data. This approach means we don't have to create copies of the same calculation views with different filters defined.

For example, different regional sales managers who are only allowed to see sales data for their regions could use the same calculation view, but they would have different analytic privileges assigned to their user ID that specify the regions they're supposed to see.

An analytic privilege can be assigned to multiple users who have the same data access requirements.

Analytic Privileges — The End-to-End Scenario
To secure a calculation view with analytic privilege, the main steps are as follows:

To Create and Assign an Analytic Privilege
Create a source file with the extension .hdbanalyticprivilege.

Assign the calculation views that you want to secure with this analytic privilege.

Choose the type of restrictions that you want to use and define the restrictions.

Set the secured calculation views to check analytic privileges.

Deploy the analytic privilege.

Assign the analytic privilege to a role.

Assign the role to a user.

The next section breaks down the steps in detail:

1. Create the Source File
Select a folder in your project.

Choose New File.

Provide a name and be sure to use the extension .hdbanalyticprivilege.

Select Rename, and enter a label.

2. Select Calculation Views
An analytic privilege definition contains the list of calculation views to which it will apply. You can include any type of calculation view.

Screen capture of the second step: By clicking on +, you add views to which the analytic priviilege will apply.
3. Define the Restrictions
There are three methods available to define the restrictions in an analytic privilege. These methods are exclusive; that is, in one analytic privilege, you use only one of these methods.

Restriction Types in Analytic Privileges
Restriction type	How to Use	Restriction Example
Attribute	With the restriction editor, select one or several attributes from the secured views. For each of them, define restrictions.	
REGION: EMEA

YEAR: Between 2015 and 2017

SQL Expression	Create a valid static SQL expression that refers to the attributes and the authorized values. This is useful when the Attribute restriction type does not fulfill the requirement.	("REGION"=’EMEA’ AND "YEAR"=’2015’) (valid SQL expression)
Dynamic	Use a procedure to derive a dynamic SQL expression to restrict the data set. This expression must be similar to a WHERE clause in a select statement.	P_DYNAMIC_AP_FOR_REGION (name of the procedure)
Defining a Restriction of Type Attribute
Screen capture of the third step: In details, you add an attribute and restriction or an SQL expression.
Defining Values in the Restriction Filter
To define values in the restriction filter, you can use the following operators:

Between <scalar_value_1> <scalar_value_2>

ContainsPattern <pattern with *>

Comparison operators: =, <=, <, >, >= with <scalar value>

IsNull and Is Not Null

Note

All filter operators, except IsNull and Is Not Null, accept empty strings (" ") as filter operands. For example:

In (" ", "A", "B")

Between (" ", "XYZ") (as lower limit in comparison operators)

Only columns of type Attribute (NOT Measure) can be specified in dimension restrictions.

If a DIMENSION calculation view contains a parent-child hierarchy and the hierarchy is enabled for SQL access, it's also possible to define the restriction on a hierarchy node.

Combining Several Attribute Restrictions
Several restriction filters within an analytic privilege are combined in the following way:

Within one attribute column, several restrictions are combined with a logical OR.

Within one analytic privilege, all dimension restrictions are combined with a logical AND.

For example, if an analytic privilege includes two restrictions on two different attributes (YEAR=2017, COUNTRY=US), the user is allowed to see only data fulfilling the compound condition YEAR=2017 AND COUNTRY=US.

Combination of restrictions for the same attribute are combined with OR. Combination of restrictions for different attributes, such as COUNTRY and YEAR are combined with AND.
Several Analytic Privileges Applying to the Same View
If two analytic privileges (or more) are defined to apply to the same view, SAP HANA combines the corresponding conditions with a logical OR.

If a user has two privileges, they are combined with OR. The privileges act as a filter on the result set.
Caution

If the two restrictions from the previous example were defined in two different analytic privileges applying to this user and this view, the user would see more data. Namely all the rows for which YEAR=2017 OR COUNTRY=US (that is, any year for COUNTRY=US and any country/region for YEAR=2017).

Defining a Validity Period
You can decide to make an analytic privilege of the type Attribute valid for only a certain period of time. This restriction applies to the date when users are querying the secured view (CURRENT_DATE).

Caution

This isn't related to the time attributes (year, month, date, and so on) defined in your views. To limit the access to the data based on such attributes, you use a classic attribute-based restriction, for example on a DATE or YEAR column.

Restricting Value with a SQL Expression
This second type of restriction allows you to define a filtering expression that can't be obtained with restrictions of type Attribute. For example, when the precedence of OR and AND logical operators doesn't correspond to what you want to define.

Suppose you want to allow a user to view all the data from the country/region he’s responsible for (US), but also all the historical data for any country/region. You could create a restriction of type SQL Expression as follows:

Code Snippet

Copy code

Switch to dark mode
("COUNTRY" = 'US' OR "YEAR" <= '2016')
Hint

Instead of writing your SQL expression from a blank page, you can start defining a restriction of the type Attribute, and then convert the restriction type to SQL expression. The corresponding SQL code will already be generated for you, and you'll just need to adjust it to your requirements.
Defining a Validity Period
Restrictions of the type SQL Expression also support a validity period, by adding to the SQL expression the relevant SQL code to filter the CURRENT_DATE.

For example:

Code Snippet

Copy code

Switch to dark mode
(CURRENT_DATE BETWEEN '2017-07-28 00:00:00.000' AND '2017-07-28 23:59:59.999')
AND < SQL expression to define the restriction >
Switching Restriction Type
Screen capture of the details of the definition of an analytic privilege: You can switch between attribute based, SQL expression, or dynamic which means, procedure based).
Note

A restriction of type Attribute can be automatically converted into a SQL Expression restriction, but the other way round is NOT possible.

To define a restriction of type Attribute, you can select among the list of shared and private attributes from the secured calculation views. You can define as many restrictions as needed to define the correct data set.

Creating a Dynamic Restriction Type
In an analytic privilege, in addition to static values filtering conditions, it's possible to determine the filtering conditions in a dynamic way, which means that the filtering condition isn't defined once for all, but is evaluated dynamically when the view is executed. This is called Dynamic Analytic Privilege.

With a dynamic analytic privilege, one analytic privilege can be defined that assigns different restrictions to different users.
With a dynamic restriction, the filtering conditions that apply for a specific user are determined at runtime. This allows a more scalable approach, where the same analytic privilege can be granted to several users who have different authorization requirements.

For example, the COUNTRY attribute in one or several calculation views can be filtered dynamically based on the actual list of countries/regions that each is allowed to access, depending on their position in the geographical organizational structure.

Technically, to implement a dynamic restriction, you assign to the analytic privilege one procedure, which returns a SQLScript expression to filter data, like in the WHERE clause of a SQL statement.

For example, a procedure could return ("COUNTRY"='US') for User1 and ("COUNTRY"='UK' OR "COUNTRY"='FR') for User2.

This procedure must have the following properties:

Dynamic Restriction – Procedure Properties
Procedure must be read-only

Security mode must be DEFINER

No input parameters

Only one scalar output parameter of type VARCHAR(256) or NVARCHAR(256)

4. Set the Secured Calculation Views to Check SQL Analytic Privileges
To actually secure a calculation view with an analytic privilege, you must set the Apply Privileges property of the calculation view to SQL Analytic Privileges.

Screen capture of fourth step: In the Semantics details, set the Apply Privileges view property to SQL Analytic Privilege to ensure that privileges are checked.
5. Deploy the Analytic Privilege
During the deployment of calculation views and analytic privileges, a specific dependency check is triggered to avoid errors that would lead to unsecured calculation views. You get a deployment error in the following cases:

If you try to deploy a calculation view after activating the check for SQL analytic privileges but NO analytic privilege has this view in its Secured Models list.

If you try to deploy an analytic privilege but some of the (runtime) calculation views it secures do NOT have the property Apply Privileges set to SQL Analytic Privileges.

If you try to deploy a calculation view after deactivating the check for SQL analytic privileges but there's still one or several (runtime) analytic privileges that have this view in their Secured Models list.

Note

The best approach is to build in parallel analytic privileges and the calculation views that they secure.
6. Assign the Analytic Privilege to a Role
Once an analytic privilege is deployed, the calculation views it applies to can't be viewed until the privilege is granted to the end user.

To do so, in your project, you create a design-time role and grant the new analytic privilege to this role.

7. Assign the Role to a User
The last step is to grant the role to the end user.

Definition of Restrictions on Hierarchy Nodes in Analytic Privileges
SAP HANA supports attribute restrictions based on a hierarchy node (rather than a list of values, and interval) in analytic privileges.

This is useful in scenarios where a user who is enabled to a certain node of the hierarchy must also be enabled to all the descendants of this node.

With a geographical hierarchy, for example, the manager of the North America area will see all the countries in his area if the analytic privilege sets the attribute restriction to the NA node. You do not have to list all the countries from this area.

To Use a Hierarchy Node in an Attribute Restriction
Enable the calculation view for analytic privilege check and also for SQL access to hierarchies.

Create an analytic privilege:

Add the calculation view to the list of secured models.

Create an Attribute restriction type.

From the section Hierarchical Privilege select one of the available hierarchies..

Choose a hierarchy node value.

Caution

This feature is supported:

With calculation views of the type CUBE With Star Join that are designed to check SQL analytic privilege and enable SQL access to hierarchy.

Only for parent-child Hierarchies.

Defining Data Access Security with Nested Calculation Views
There are many business cases where calculation views contain references to one another.

The figure, Nested calculation views – Data Access Security Principles, explains how data access is handled when calculation views are nested.

For each calculation view, the following criteria are considered:

Does the user have SELECT privilege on the column view (this is the actual database object generated from a calculation view) in the container schema?

Does the calculation view check analytic privileges?

Is the user granted analytic privileges for the view?

Nested Calculation Views – Data Access Security Principles
Object privileges are required only for a top level view in a hierarchy of calculation views. Views can be explicitly registered for checking Analytic Privilege. Access is granted only if the user was granted Analytic Privileges for all views in a hierarch registered for the Analytic Privilege check. The output of an underlying view defines the maximum possible output of higher-level views.
The key rules that govern the access to data are as follows:

Object privileges

There's no need to grant SELECT privileges on the underlying views or tables. The end user only needs to be granted SELECT privileges on the top column view of the view hierarchy.

Analytic Privileges

The analytic privileges logic is applied through all the view hierarchy.

Whenever the view hierarchy contains at least one view that is checked for analytic privileges but for which the end user has no analytic privilege, no data is retrieved (not authorized).

Note

In the figure, Nested Calculation Views – Data Access Security Principles, this is the case for View 5, so View 4 won't retrieve any data.

This is also the case for View 6, and, in addition, the end user is not granted a SELECT privilege on View 6. So View 6 will retrieve no data (not authorized).

Note that the end user always needs an explicit SELECT privilege on a calculation view to be able to query its data. That is, granting an Analytic Privilege to this user doesn't also grant an implicit SELECT authorization on the views that this Analytic Privilege secures.

Secure Calculation Views
Enable Calculation View for Privilege Check
Watch this video to learn how to enable Calculation View for privilege check.

Define Analytic Privilege
Watch this video to learn how to define Analytic Privilege.

Define Role
Watch this video to learn how to define role.

Assign Role to User and Test
Watch this video to learn how to assign role to User and Test.



...


Defining Roles
Objective

After completing this lesson, you will be able to create a design-time role to secure data access.
Design-Time Roles
Security Concepts
Before we cover roles, there are some important concepts that you must understand.

A database user can be the owner of database objects. A role is a collection of privileges and can be granted to a user or another role. A privilege enables specific operations on one or several objects.
In SAP HANA Cloud, we define roles and users, and assign privileges. Privileges can be assigned to users directly. They can also be assigned to users indirectly by using roles.

Roles help you to structure access using reusable business-related roles. Roles can be nested, enabling the implementation of a hierarchy of roles.

It's highly recommended that you manage authorizations for users by using roles. Assigning a privilege directly to a user isn't a good practice and creates a lot of maintenance.

Some key point regarding security concepts of SAP HANA Cloud:

All the privileges granted directly or indirectly to a user are combined.

Whenever a user tries to access an object, the system performs an authorization check based on the user's roles and directly allocated privileges (if any).

It isn't possible to explicitly deny privileges. All privileges grant access.

The system doesn't need to check all the users roles. As soon as all the privileges required for a specific operation on a specific object have been found, the system ends the check and allows the operation without checking if the same privileges appear again in another role.

Several predefined roles exist in the SAP HANA Cloud database.

Some of them are templates (and must be customized), and others can be used as they are.

Defining Roles
In the SAP HANA Cloud database, there are two ways to create roles:

As pure runtime objects (with no source file) that are created using SQL or SAP HANA cockpit. These objects are called Catalog Roles. You assign privileges to these roles using SQL grant statements.

By means of source files that you create in the HDB module of a project. These are called Design-Time Roles, and the source file describes the privileges that are immediately granted when the role is deployed.

The design-time files used to create roles must have the extension .hdbrole in order to be recognized as design-time role files.

Note

Each role must be defined in its own .hdbrole design-time file.

It isn't possible to create several roles within the same .hdbrole file.

The role ID (including a valid namespace if applicable) must be unique in the HDB module, as for any other object (calculation view, synonym, and so on).

Types of Privileges in a Design-Time Role

The .hdbroleconfig File
The .hdbrole file can't contain references to real schema names, but only logical references to schemas that are resolved in another type of design-time file: the .hdbroleconfig file.

The purpose of the.hdbroleconfig file is to maintain the actual name of the external schemas in a dedicated file, instead of having many occurrences of the schema names in the .hdbrole files themselves. It makes the maintenance of a project easier when you're able to maintain the references to external schemas in just one place.

An .hdbrole file can contain a reference, such as Ref1. A .hdbroleconfig file defines the corresponding schema for Ref1. For purpose and details, refer to the surrounding text.
You can create the .hdbroleconfig file manually and then specify this file when you create your .hdbrole file. Or you can generate the .hdbroleconfig file automatically from within the .hdbrole editor and then optionally adjust the generated file if necessary.

Using the Role Editor
The .hdbrole and .hdbroleconfig files can be created as text files or you can use the Role Editor.

By default, when you create a new .hdbrole file, the Role Editor opens.

The following video shows how the Role Editor can be used to generate the corresponding files.

Enabling Access to an External Schema
When your application requires access to an external schema, an administrator must define a dedicated user-provided service. A technical user is assigned to this service, and brings its own authorizations to the database objects.

As part of the security implementation in your project, it's necessary to define which of these authorizations will be granted to the different roles. For example, some users might need insert/update/delete privileges on a particular set of tables, when other users only need select privileges.

Note

When creating calculation views, the main authorization you need is a SELECT privilege on the data sources.

The .hdbgrants File
A dedicated file, with extension .hdbgrants, is used to define the set of authorizations that will be given to two specific users, the Object Owner and the Application User.

In a project, only one .hdbgrants file is allowed. The object_owner section lists privileges granted to the Object Owner. The application_user section lists privileges granted to the application user. For details, refer to the surrounding text.
The <filename>.hdbgrants file is structured into three levels:

The name of the user-provided service

The users to whom the privileges are granted

There are two possible values for users:

object_owner is the technical user that owns all the objects of the container schema.

application_user represent the users who are bound to the application modules.

The set of privileges granted

The syntax of this third level is very similar to the syntax of what you find in a .hdbrole file.

Note

A single .hdbgrants file can list authorizations from more than one user-provided services.
It's essential to give the application_user a correct set of authorizations. For example, if a SELECT privilege on an external table is granted to the object_owner, this will allow the creation of a synonym for this table. But if the same SELECT privilege isn't granted to the application_user, you won’t be able to display the content of the target table.

...

Masking Sensitive Data
Objective

After completing this lesson, you will be able to restrict access to columns containing sensitive data.
Column Masking
When calculation views might expose sensitive data to the end user, it's possible to define a mask on the corresponding column(s) so that only authorized users will be allowed to see the actual data. For other users, the mask will be applied in order to hide part or all of the column when previewing the calculation view data or querying it with a front-end tool.

This slide shows how to mask a column.
Defining a Mask Expression
This slide shows how to define a mask expression.
A mask expression is defined in a calculation view as follows:

In the Semantics node, choose the Columns tab.
Select a column, and choose the Data Masking icon in the toolbar
Define the masking expression using SQL
Only columns of certain data types can be masked in a calculation view:

VARCHAR
NVARCHAR
CHAR
SHORTTEXT
Note

Masking is supported for both table types (ROW tables and COLUMN tables).
The mask expression can be either a constant string (for example: XXX-XXX-XXX) or a SQL expression using string functions and/or data from the source column.

For example, you can mask the middle part of a credit card number stored in column credit_card with the following mask expression:

Code Snippet

Copy code

Switch to dark mode
LEFT(credit_card,4) || '-XXXX-XXXX-' || RIGHT(credit_card,4)
With this expression, the credit card number 1111–2222–3333–4444 will be masked as 1111-XXXX-XXXX-4444.
Authorizing Access to a Masked Column
When a column of a calculation view is assigned a mask expression, the data of this column is masked unless the user has the UNMASKED privilege for this calculation view. This privilege is generally included in one or several roles defined in the HDB module, and this role is in turn granted to the end user or, most often, to a role assigned to the end user.

The UNMASKED privilege can be granted at two different levels, which corresponds to two different role definition syntaxes:

At the schema level:

The authorization to view unmasked data (the actual value) is given for an entire schema. In particular, if no schema is specified, the authorization affects all the calculation views defined in the container schema where the role is defined, which is the most common case.

Code Snippet

Copy code

Switch to dark mode
{
"role":
  {
    "name":"db::UNMASK_ENTIRE_SCHEMA",
    "schema_privileges":[{
      "privileges":["UNMASKED"]}]
  }
}
Note that the authorization at the schema level allows to unmask columns in all types of objects that support masks, namely: tables, SQL views, and graphical calculation views.

At the object level:

The authorization to view unmasked data is given for a specific object. You have to specify the object type and object runtime name (with namespace).

Code Snippet

Copy code

Switch to dark mode
{
"role":
  {
    "name":"db::UNMASK_EMPLOYEES_PAYMENT",
    "object_privileges":[{
      "type":"VIEW",
      "name":"db::CVD_EMPLOYEES_PAYMENT",
      "privileges":["UNMASKED"]}]
  }
}
Note

Like for any other calculation view, the end user will receive an authorization error if they don't have a SELECT authorization on the view. In other words, the UNMASKED privilege doesn't bypass the SELECT privilege, which is mandatory to display a calculation view’s data.
Choosing a Mask Mode
From SAP HANA Cloud QRC 4/2021 onwards, two different mask modes are available.

Default
Masking is done based on the user calling the calculation view with the masking definition.

Session User (new)
Masking is done based on session user running the SQL query

This distinction doesn't affect the top-most calculation view of a stacked scenario because the applied privileges are the ones from the session user. But it's especially relevant for underlying views, where the Session User mask mode allows you to mask data from these views by applying the mask (or unmasking) based on the privileges of the Session User, that is, the user executing the query on the top-most view. By contrast, with the Default mask mode, the underlying views are executed with the privileges of the Object Owner, who always has unmasked privileges.

This slide compares the different masking modes.
Caution

The mask mode setting is view-global. Besides, in a stacked scenario, upon execution of the top-most calculation view, the Mask Mode option defined is this view does NOT overwrite the ones defined in the underlying views.
Current Limitations to Column Masking


...

Knowledge quiz
It's time to put what you've learned to the test, get 3 right to pass this unit.

1.
Which approach is recommended to create roles that grant privileges to access local objects generated in your container?
Choose the correct answer.

Catalog Roles

Design-time Roles
2.
What is a dynamic analytic privilege?
Choose the correct answer.

A reusable analytic privilege that can be used for several users who need to access different data.

An analytic privilege that takes its filter values from variables defined in the calculation view.

A temporary analytic privilege that has a defined time validity set for its use.
3.
What is data masking?
Choose the correct answer.

Obscuring column values by hiding some or all characters with replacement characters.

Replacing column values with a higher level group, such as replacing a person's name with their team name.

Removing complete rows of data that should not be seen by users.

......

Unit 20
Getting Started with SAP HANA Cloud Knowledge Graph Engine
After completing this unit, you will be able to:

Describe the concept Knowledge Graph.
Discover the SAP HANA Cloud Knowledge Graph Engine.
Present use cases with the power of SAP HANA Cloud KGE.
Overview of Knowledge Graphs
SAP HANA Cloud Knowledge Graph Engine
Use Cases and Demo

...


Overview of Knowledge Graphs
Objective

After completing this lesson, you will be able to describe the concept Knowledge Graph.
Knowledge Graphs
When we ask a virtual assistant about tomorrow’s weather forecast or use Google to search for the latest news on climate change, knowledge graphs serve as the foundation for today’s cutting-edge information systems. In addition, knowledge graphs have the potential to elucidate, evaluate, and substantiate information produced by deep learning models such as Chat-GPT and other large language models. Knowledge graphs have a wide range of applications, including improving search results, answering questions, providing recommendations, and developing explainable AI systems.

A knowledge graph is a structured representation of facts about the world, describing entities and their interrelations, organized in a graph format. Here are some key aspects of a knowledge graph:

Aspect	Definition
Entities	In a knowledge graph, nodes represent entities (like people, places, things, concepts), and edges represent relationships or connections between these entities.
Relationships	Define the connections between entities. For example, in a knowledge graph about movies, a relationship might be "acted_in" between an actor and a movie.
Attributes	Describe the characteristics of entities and relationships. For example, an entity "movie" might have attributes like "director," "release_date," and "genre."
RDF	
The Resource Description Framework (RDF) is a standard model for data interchange on the web. It extends the linking structure of the web to use URIs to name the relationship between things as well as the two ends of the link (this is usually referred to as a "triple").

This linking structure forms a directed, labeled graph, where the edges represent the named link between two resources, represented by the graph nodes

.
Triples	The basic unit of data in a knowledge graph is a triple, which consists of a subject, a predicate, and an object. For example, "Paris (subject) - is the capital of (predicate) - France (object)".
Triple-Based Structure: RDF represents data as triples, each consisting of:​
Subject: The entity being described (e.g., a Paris).​
Predicate: The relationship or attribute (e.g., "is the capital of ").​
Object: The value or related entity (e.g., France).​A diagram showing the concept of triple (subject, predicate, object) together with the example of Paris, the capital of France)
Semantic Meaning	Knowledge graphs often use standardized vocabularies or ontologies to define the types of entities and relationships, giving them semantic meaning. This makes knowledge graphs highly suitable for AI applications, such as question answering and recommendation systems.
Sources of Data	Data in a knowledge graph can come from various sources, including structured data (like databases), semi-structured data (like XML or JSON), and even unstructured data (like text, using techniques like Named Entity Recognition and Relation Extraction).
Examples	Some prominent examples of knowledge graphs include DBpedia, Wikidata, Freebase (now part of the Google Knowledge Graph), and proprietary graphs developed by companies like SAP, Amazon, Facebook, and Microsoft.
Applications	Knowledge graphs are used in various applications, such as search engines, chatbots, digital assistants, recommendation systems, and data integration tasks.
Ontologies	Provide a formal structure for the knowledge graph, defining the types of entities, relationships, and attributes. Ontologies are needed to ensure the consistency and interoperability of your model.A diagram showing an illustration of an ontology including town, country, country population, and continent.
In essence, a knowledge graph provides a way to represent, store, and query structured knowledge, enabling machines to understand and process human-readable information more effectively.

Ontologies
Ontologies are formal representations of knowledge, providing a structured way to describe domains of interest. They define the types, properties, and relationships of entities within a specific domain, creating a common vocabulary and understanding that can be shared among users and applications. Ontologies play a crucial role in the field of knowledge representation, semantic web, and artificial intelligence.

Key Components of Ontologies
Key Components	Description
Classes (or Concepts)	
Represent categories or types of entities within the domain. For example, in a medical ontology, classes might include "Patient," "Doctor," and "Disease."

Properties (or Attributes)	
Describe the characteristics or attributes of the classes. For example, a "Patient" class might have properties like "name," "age," and "medical history."

Relationships (or Relations)	
Define how classes are interrelated. For example, a "Doctor" class might be related to a "Patient" class through a relationship like "treats" or "diagnoses".

Individuals (or Instances)	
Represent specific objects or entities that belong to a class. For example, "John Doe" might be an individual of the "Patient" class.

Axioms	
Logical statements that constrain the interpretation of the classes, properties, and relationships. Axioms define rules and restrictions that must be satisfied within the ontology.

Purpose and Benefits of Ontologies
Purpose and Benefits	Description
Shared Understanding	
Provide a common vocabulary and structure that can be understood and used by both humans and computers.

Data Integration	
Facilitate the integration of data from different sources by mapping them to a common ontology.

Reasoning	
Enable automated reasoning and inference about the entities and relationships within the domain. For example, if a doctor treats a patient, and the patient has a particular disease, the ontology can infer that the doctor is involved in the treatment of that disease.

Knowledge Reuse	
Allow for the reuse of knowledge across different applications and domains.

Semantic Interoperability	
Promote interoperability between different systems and applications by ensuring that data is consistently interpreted and understood.

Examples of Ontologies
Examples	Description
Gene Ontology (GO)	
A collection of terms that describe gene product attributes across species. It is widely used in bioinformatics for annotating genes and gene products.

FOAF (Friend of a Friend)	
An ontology used to describe people and their relationships, commonly used in social networks.

SNOMED CT (Systematized Nomenclature of Medicine--Clinical Terms)	
A comprehensive clinical terminology used in healthcare for encoding clinical data.

Schema.org	
A collaborative initiative that provides a common vocabulary for marking up web pages to improve search engine results.

Ontology Languages
Languages	Description
RDF (Resource Description Framework)	
A standard used for representing information about resources on the web.

RDFS (RDF Schema)	
An extension of RDF that provides a vocabulary for describing classes and properties.

OWL (Web Ontology Language)	
A richer and more expressive language that builds on RDF and RDFS, providing a framework for defining and reasoning about ontologies.

SKOS (Simple Knowledge Organization System)	
SKOS is a W3C recommendation designed for representing knowledge organization systems such as thesauri, classification schemes, taxonomies, and subject heading systems. It is less formal than OWL and is focused on applications where informal relationships and loose definitions are sufficient.

Example: skos:Concept, skos:broader, skos:narrower, skos:related.

DAML+OIL	
A predecessor to OWL, DAML+OIL is a combination of DAML, developed by DARPA, and OIL, developed by EU research projects. It provides constructs for complex class descriptions and axioms. After the development of OWL, DAML+OIL was largely superseded, but it played a crucial role in the evolution of ontology languages.

SHACL (Shapes Constraint Language)	
SHACL is a language for validating RDF graphs against a set of conditions. It allows for the definition of constraints on RDF data and provides mechanisms to identify and report violations.

Example: sh:NodeShape, sh:PropertyShape, sh:minCount, sh:maxCount.

Frame Logic	
Frame Logic is an older ontology language, based on the concept of frames, which are data structures for representing stereotypical situations. Frames include slots and fillers, where slots are the properties or attributes of the frame and fillers are the values of these properties.

Creating and Managing Ontologies
Creating and Managing	Description
Ontology Editors	
Tools like Protégé that provide a graphical interface for creating, editing, and visualizing ontologies.

Reasoning Engines	
Software that can perform automated reasoning and inference based on the ontology. Examples include HermiT and Pellet.

Conclusion
Knowledge graphs are powerful tools for organizing, integrating, and leveraging knowledge. They provide a structured and semantically rich representation of information, enabling sophisticated queries, reasoning, and contextual understanding. By using knowledge graphs, organizations can enhance their data management, improve decision-making, and develop more intelligent systems.

Ontologies are essential for structuring knowledge in a way that is understandable and usable by both humans and machines. They provide a foundation for semantic interoperability, data integration, and intelligent systems. By using ontologies, organizations can enhance the quality and efficiency of their knowledge management processes, leading to better decision-making and innovation.

...

SAP HANA Cloud Knowledge Graph Engine
Objective

After completing this lesson, you will be able to discover the SAP HANA Cloud Knowledge Graph Engine.
SAP HANA Database Knowledge Graph Engine
The SAP HANA database includes a sophisticated Knowledge Graph Engine designed to facilitate advanced analytics and graph processing.

A diagram showing components of the SAP HANA Knowledge Graph Enging and how it interacts with the SAP HANA Cloud Index Server, Session Management and SAP HANA clients.
The SAP HANA database Knowledge Graph Engine is a component within the SAP HANA in-memory database platform designed to support advanced graph processing and analysis. It extends SAP HANA Cloud's capabilities to handle complex, interconnected data structures, making it well-suited for a variety of modern analytical tasks.

The Knowledge Graph Engine allows users to model, query, and analyze graph data efficiently. By leveraging the in-memory computing power of SAP HANA Cloud, it provides high performance and scalability, facilitating the processing of large and complex datasets.

Below are key points that provide a comprehensive understanding of the SAP HANA Knowledge Graph Engine:

Key Features
Keys	Description
Graph Data Processing	
The Knowledge Graph Engine allows you to model complex relationships between data points, making it easier to perform queries that involve traversing these relationships, supports both directed and undirected graphs with nodes and edges. Nodes can represent entities, and edges can represent relationships between those entities.

In-Memory Processing	
Leverages SAP HANA's in-memory capabilities to deliver fast query performance and scalability.

Schema Flexibility	
Supports a schema-less structure, which is beneficial when dealing with heterogeneous or evolving data formats.

Scalability	
The engine leverages the in-memory capabilities of SAP HANA, providing high performance and scalability for large datasets.

Integration with SQL	
Seamless integration with other SAP HANA components, such as the SQL engine, allowing for a unified data model, simplified access to both tabular and graph data and allowing users to combine graph queries with traditional SQL queries.

Graph Algorithms	
Built-in graph algorithms for common tasks such as shortest path, connected components, and centrality measures, which can be executed directly within the database.

Query Language	
Supports graph query languages like Cypher (used by Neo4j) and Property Graph Query Language (PGQL), making it accessible to those familiar with graph databases.

Conclusion
The SAP HANA database Knowledge Graph Engine is a robust tool for handling complex, interconnected data structures. By leveraging the power of SAP HANA's in-memory capabilities, it provides efficient graph processing and supports a wide range of analytical tasks. This makes it a valuable addition for organizations looking to gain deeper insights from their data and make more informed decisions.

Interfaces
The SAP HANA Cloud Knowledge Graph Engine provides a range of interfaces and APIs that facilitate interaction with the system for data ingestion, querying, and management. These interfaces enable seamless integration with other systems and workflows, making it easier to leverage graph-based data management and analytics within an enterprise environment.

Interfaces and APIs	Description
SPARQL	
The primary interface for querying the graph data is through the SPARQL endpoint. This interface allows users to submit SPARQL queries to retrieve and manipulate data in the graph.

SPARQL Update allows users to perform CRUD (Create, Read, Update, Delete) operations on the graph data, enabling dynamic updates to the graph structure.

SPARQL stands for SPARQL Protocol and RDF Query Language.

RESTful API	APIs are provided for data ingestion, allowing users to upload RDF data in various formats, such as Turtle, RDF/XML, N-Triples, and JSON-LD.
Management Operations	
These APIs support administrative tasks such as creating, deleting, and managing graph data stores, as well as configuring access controls and other settings.

Graph Algorithms	
APIs are available to execute built-in and custom graph algorithms, providing advanced analytics capabilities.

JavaScript API	
The Knowledge Graph Engine can be accessed through the SAP HANA client APIs, which include support for JavaScript. This enables developers to integrate graph capabilities into web applications and other JavaScript-based solutions.

SQL Interface	
While the primary querying interface is SPARQL, the Knowledge Graph Engine also supports SQL extensions that allow integration with traditional SQL-based applications. This facilitates the use of relational data alongside graph data.

Event Streaming Interfaces	
The engine supports interfaces for streaming data, enabling real-time data ingestion and processing. This is useful for use cases such as real-time analytics and event processing.

BI and Visualization Tools	
Integration with SAP Analytics Cloud enables the creation of graph-based visualizations and dashboards, providing a user-friendly interface for exploring and presenting graph data.

Third-Party Tools	
The engine can integrate with various third-party BI and visualization tools, allowing users to leverage their preferred tools for data analysis and presentation.

SAP Data Intelligence	
The Knowledge Graph Engine can be integrated with SAP Data Intelligence, enabling the use of machine learning algorithms for advanced analytics on graph data. Interfaces are provided to integrate custom machine learning models, allowing users to tailor analytics to their specific needs.

Cloud Foundry and Kubernetes	
The engine provides service brokers for Cloud Foundry, allowing it to be seamlessly integrated into Cloud Foundry-based environments.

Kubernetes Operators	
Kubernetes operators can be used to manage the lifecycle of the Knowledge Graph Engine, facilitating deployment and scaling in Kubernetes environments.

OData Services	
The engine can expose data and services through OData, enabling integration with a wide range of enterprise applications that support the OData protocol.

These interfaces and APIs allow users to interact with the SAP HANA Cloud Knowledge Graph Engine in a variety of ways, supporting a wide range of use cases and integration scenarios. Whether you're building custom applications, performing real-time analytics, or integrating with existing systems, the Knowledge Graph Engine offers the flexibility and power needed to leverage graph data effectively.

Working with Knowledge Graph
SAP HANA Cloud Knowledge Graph Engine is a component within the SAP HANA Cloud platform, designed to help organizations derive insights from interconnected data using graph-based data management and analytics. The architecture of the Knowledge Graph Engine facilitates the storage, retrieval, and analysis of complex relationships within enterprise data.

Architecture	
Overview

Data Ingestion and Storage	
The Knowledge Graph Engine supports the ingestion of data from various sources, including structured data from relational databases, semi-structured data from files (like JSON or XML), and unstructured data.

Data is stored in a graph-based storage system that is optimized for graph queries. This layer ensures efficient storage and retrieval of nodes and edges, supporting large-scale graph databases.

Data Modeling with RDF/OWL Support	
The engine uses Resource Description Framework (RDF) and Web Ontology Language (OWL) to model data. This allows for a flexible and semantically rich representation of data and relationships. Users can define ontologies that describe the structure of the data and relationships between objects, enabling a consistent and semantic understanding of the data.

Graph Querying: SPARQL Query Language	
The Knowledge Graph Engine supports SPARQL, a powerful query language for RDF data. This enables complex queries that can traverse the graph, finding patterns and relationships within the data. Leveraging SAP HANA's in-memory processing capabilities, graph queries can be performed with high speed and efficiency, offering low-latency responses.

Computation and Analytics	
The engine includes a library of graph algorithms that can be used for various analytical tasks such as shortest path analysis, community detection, and centrality measures. Users can also define and execute custom algorithms tailored to their specific use cases.

Integration and APIs	
The Knowledge Graph Engine is tightly integrated with other components of the SAP HANA Cloud platform, allowing seamless data movement and processing. APIs are provided to enable interaction with the Knowledge Graph Engine from external applications, facilitating easy integration with other systems and workflows.

Security and Governance	
The engine enforces data security policies, including access controls, encryption, and role-based permissions to ensure data privacy and compliance. Tools and frameworks are provided to support data governance, including data lineage, metadata management, and quality checks.

User Interface and Visualization	
Built-in or integrated visualization tools help users explore and understand the graph data visually, making it easier to derive insights and communicate findings.

Dashboard and Reporting: integration with BI tools allows for the creation of dashboards and reports that present graph-based insights in a meaningful and actionable format.

Scalability and Performance	
The architecture supports the horizontal scaling of resources to handle large and growing datasets. Performance optimization techniques, including query optimization and indexing, ensure that graph queries are executed efficiently even on large graphs.

Getting Started
Installing and setting up the SAP HANA Cloud Knowledge Graph Engine involves several steps, and since SAP HANA Cloud is a managed service, many of the traditional installation steps are abstracted away.

Here are the prerequisites to get you started with the Knowledge Graph Engine on SAP HANA Cloud:

Prerequisites	Task
SAP HANA Cloud Instance	
Ensure you have an active SAP HANA Cloud instance.

SAP HANA Cloud Cockpit	
Access to the SAP HANA Cloud cockpit or SAP HANA Cloud Central.

Permissions	
Ensure you have the necessary permissions to manage services and configure the Knowledge Graph Engine.

Here are the general steps to get you started with the Knowledge Graph Engine on SAP HANA Cloud:

Steps
Access SAP HANA Cloud Central:
Log in to SAP HANA Cloud Central (formerly known as SAP HANA Cloud Cockpit) using your SAP credentials.

Create or Select an SAP HANA Cloud Instance:
If you haven't already, create an SAP HANA Cloud instance.

Select your SAP HANA Cloud instance to manage its configurations.

Enable the Knowledge Graph Service:
In the SAP HANA Cloud Central, navigate to the Services section.

Locate the Knowledge Graph service. If it's not enabled by default, you might need to enable it via the instance settings.

Connect to Your HANA Cloud Database:
Use SAP HANA Studio, DBeaver, or any SQL client that supports JDBC/ODBC to connect to your SAP HANA Cloud database.

Use the connection details provided in the SAP HANA Cloud Central.

Create Graph Tables and Graphs:
Use SQL commands to create the necessary tables and graphs. Here is an example of creating a simple graph:

Code Snippet

Copy code

Switch to dark mode
-- Create a vertex table
CREATE COLUMN TABLE VertexTable (
    id INT PRIMARY KEY,
    name NVARCHAR(100)
);

-- Create an edge table
CREATE COLUMN TABLE EdgeTable (
    id INT PRIMARY KEY,
    source INT,
    target INT,
    relationship NVARCHAR(100)
);

-- Create the graph
CREATE GRAPH MyGraph (
    VERTEX TABLE VertexTable,
    EDGE TABLE EdgeTable
);

-- Insert data into vertex and edge tables
INSERT INTO VertexTable VALUES (1, 'Alice');
INSERT INTO VertexTable VALUES (2, 'Bob');
INSERT INTO EdgeTable VALUES (1, 1, 2, 'knows');
Use the Knowledge Graph Engine:
After setting up your graph, you can use the Knowledge Graph Engine to perform graph operations.

Below is an example query to retrieve nodes and edges:

Code Snippet

Copy code

Switch to dark mode
SELECT * FROM MyGraph;
Additional Configuration
Ontology Management: Depending on your needs, you might want to define and manage ontologies to define the structure of your graph data.

Data Integration: Integrate your graph data with other SAP HANA Cloud services and applications as needed.

Documentation and Resources
SAP HANA Cloud Documentation: Refer to the official SAP HANA Cloud documentation for detailed instructions and advanced configuration options.

SAP Community: Engage with the SAP Community forums for troubleshooting and sharing best practices.

SAP Tutorials: Check SAP's tutorials and learning resources for hands-on guides and examples.

By following these steps, you should be able to install and configure the SAP HANA Cloud Knowledge Graph Engine for your data-intensive applications.

...

Use Cases and Demo
Objective

After completing this lesson, you will be able to present use cases with the power of SAP HANA Cloud KGE.
Use Cases
SAP HANA Cloud Knowledge Graph Engine provides a robust platform for managing and leveraging comprehensive knowledge graphs, enabling advanced analytics, data integration, and knowledge-driven applications. Here are several use cases where SAP HANA Cloud Knowledge Graph Engine can be effectively applied:

Master Data Management
Customer 360 View: Integrate data from various systems to create a unified customer profile, enabling better customer segmentation, personalized marketing, and improved customer service.

Product Master Data: Manage complex product hierarchies, attributes, and relationships to enhance product information management and cataloging.

Supply Chain Management
Supplier Relationship Management: Create a comprehensive knowledge graph of suppliers, their capabilities, and relationships to optimize procurement and risk management strategies.

Demand Forecasting: Integrate data from multiple sources to improve demand forecasting accuracy, enabling better inventory management and supply chain planning.

Risk and Compliance Management
Financial Risk Assessment: Combine financial data, risk indicators, and regulatory information to perform comprehensive risk assessments and ensure compliance.

Regulatory Compliance: Track and manage compliance requirements, regulations, and their impact on business processes and operations.

Customer Intelligence and Analytics
Sales Forecasting: Integrate historical sales data, customer demographics, and market trends to generate more accurate sales forecasts.

Churn Prediction: Analyze customer behavior and interactions to predict customer churn and identify retention strategies.

Healthcare and Life Sciences
Patient Data Integration: Create a unified patient profile by integrating data from electronic health records, clinical trials, and other sources to improve patient care and outcomes.

Clinical Research: Manage and analyze clinical trial data, research findings, and scientific literature to advance medical research and drug discovery.

Enterprise Search and Knowledge Discovery
Document Management: Create a semantic search and discovery framework for organizational documents, enabling better access to enterprise knowledge and resources.

Employee Skill Mapping: Map employee skills, expertise, and experiences to enhance resource allocation, talent development, and succession planning.

IoT and Smart Manufacturing
Predictive Maintenance: Integrate data from IoT devices and enterprise systems to predict equipment failures and optimize maintenance schedules.

Real-time Analytics: Analyze real-time data from manufacturing processes to improve operational efficiency, quality control, and supply chain management.

Customer Service and Support
Help Desk Knowledge Base: Create a comprehensive knowledge base for customer service and support by integrating product information, troubleshooting guides, and customer feedback.

Chatbot Enhancement: Enhance chatbot capabilities by providing a rich knowledge graph that includes customer data, interaction history, and product information.

Retail and E-commerce
Product Recommendation: Improve product recommendations by analyzing customer preferences, behavior, and external trends.

Inventory Management: Optimize inventory levels by integrating sales data, customer demand, and supply chain information.

Research and Development
Innovation Management: Integrate research data, patents, and market trends to identify opportunities for innovation and new product development.

Scientific Literature Analysis: Analyze scientific literature and research findings to uncover insights and trends in specific domains.

Public Sector and Governance
Citizen Services: Enhance citizen services by integrating data from various government agencies and departments to create a unified view of citizen interactions and needs.

Policy Analysis: Analyze the impact of government policies on different sectors, regions, and populations to inform decision-making and policy formulation.

By leveraging SAP HANA Cloud Knowledge Graph Engine, organizations can gain deeper insights, enhance decision-making, and drive innovation across various domains.

Knowledge graph data
Case Study Implementing SAP HANA Cloud Knowledge Graph Engine for Pharmaceutical Research
Background
A customer is a leading pharmaceutical company engaged in research and development (R&D) for innovative drug therapies. Due to the vast amount of data generated from various sources like clinical trials, research papers, and patents, they face challenges in efficiently managing and leveraging this data to gain insights and accelerate drug discovery.

Objective
Implement SAP HANA Cloud Knowledge Graph Engine to integrate, process, and analyze diverse data, facilitating advanced queries and knowledge discovery to support drug R&D.

Scope
Data Integration
Knowledge Graph Creation
Advanced Querying and Analytics
Collaboration and Data Sharing
Implementation Steps
Data Integration
Ingest structured and unstructured data from various sources such as clinical trial databases, research publications, patent databases, and gene/protein databases (e.g., PubMed, ClinicalTrials.gov, GenBank).

Utilize SAP HANA Data Lake and SAP Data Intelligence to enable seamless data integration and preparation.

Knowledge Graph Creation
Define a comprehensive ontology for the pharmaceutical domain, including classes for genes, proteins, drugs, diseases, clinical trials, and patents.

Map data to the ontology and create a knowledge graph using the Knowledge Graph Engine.

Establish relationships between different entities, such as "gene-protein," "drug-disease," and "drug-clinical trial."

Advanced Querying and Analytics
Enable researchers to perform complex queries across integrated data sets using the SPARQL query language.

Implement graph analytics to uncover hidden relationships and generate insights, such as drug repurposing opportunities, potential side effects, and new drug targets.

Use machine learning algorithms to predict drug efficacy and safety based on the knowledge graph.

Collaboration and Data Sharing
Share the knowledge graph with internal teams and external collaborators, promoting collaboration and reducing data silos.

Ensure data governance and security using SAP HANA Cloud's built-in features, such as user access control and data encryption.

Benefits and Outcomes
Benefits and Outcomes	Purpose
Accelerated Drug Discovery	
Enhanced ability to identify and validate new drug targets.

Improved Data Accessibility	Centralized data access for researchers, reducing time spent on data collection and preparation.
Advanced Analytics	Enabled complex queries and graph analytics, leading to novel insights.
Enhanced Collaboration	Facilitated data sharing and collaboration across teams and with external partners.
Better Decision Making	Provided a comprehensive view of data, supporting data-driven decisions in R&D strategies.
Conclusion
By implementing SAP HANA Cloud Knowledge Graph Engine, the customer successfully integrated diverse data sources, created a domain-specific knowledge graph, and facilitated advanced analytics. This resulted in improved research productivity, faster drug discovery, and enhanced collaboration, reinforcing their position as a leader in pharmaceutical innovation.

Demo
Demonstration image KG
Case Study Ontology of a University
The relationships between classes are fundamental in an ontology, as they describe how concepts are connected.

Here is a generic example of an ontology with some classes and their relationships.

Case

Detail
Ontology of a University	Case Study q&a
Structured	Representation
Classes	
Person

Student

Professor

Course

Department

Relationships between classes	
Student is enrolled in Course

Professor teaches Course

Course belongs to Department

Department has Professor



...

Knowledge quiz
It's time to put what you've learned to the test, get 5 right to pass this unit.

1.
It is correct to say that the case study presented "Ontology of a University" is a Knowledge Graph model?
Choose the correct answer.

True

False
2.
Which of the following are ontology languages?
There are three correct answers.

Resource Description Framework (RDF)

Axioms

Web Ontology Language (OWL)

RDF Schema (RDFS)

Gene Ontology (GO)

Outline
3.
What do knowledge graphs often use to define entity types and relationships?
Choose the correct answer.

Standardized vocabularies or ontologies

Properties or Attributes

Individuals or Instances
4.
What is the core component that extends the capabilities of SAP HANA to handle complex and interconnected data structures?
Choose the correct answer.

Graph Model Definition

Knowledge Graph Engine

Social Network Analysis

Integration and APIs
5.
What can be integrated with the Knowledge Graph Engine to enable the use of machine learning algorithms for advanced analytics on graph data?
Choose the correct answer.

Data Ingestion

RESTful APIs

SAP Data Intelligence

Kubernetes
6.
What is the supported language for RDF data in Knowledge Graph Engine?
Choose the correct answer.

REXX

ABAP

SPARQL

Algol

......




