1. Learning how to use the SAP AI Core service on SAP Business Technology Platform

..

Configuring SAP AI Core and SAP AI Launchpad
Objectives

After completing this lesson, you will be able to:
Illustrate the process involved in working with SAP AI Core and SAP AI Launchpad
Connect SAP AI Core and SAP AI Launchpad to user's docker and GitHub repositories
SAP AI Core
What is the added value of SAP AI Core?
Note

This learning journey just focuses on SAP AI core.
Refer to the learning journey Solving Your Business Problems Using Prompts and LLMs in SAP's Generative AI Hub to use SAP's generative AI Hub to solve your business problems using LLMs, SAP AI Launchpad, and generative-ai-hub SDK.

You will also be able to apply techniques for refining and evaluating prompts using different large language models in generative AI hub.

Gain peace of mind with SAP managed deployments
With SAP AI Core, customers gain peace of mind as they benefit from a complete service that packages all dependencies and hides the complexity of building your own training and serving productive environment. Instead, SAP AI Core exposes simple API endpoints to embed AI into your business applications.

Quickly embed AI in SAP applications and business processes
Customers can take advantage of the standardized integration into SAP applications to quickly build and integrate their AI use cases into their business applications.

Strike the right balance between costs and performance
On the one hand, customers benefit from accelerated performance with GPU support to run their most resource-hungry use cases at scale. On the other hand, customers run efficiently while keeping control of their costs by leveraging built-in autoscaling and scale-to-zero as well as by choosing from a broad range of storage, CPU, and GPU service plans.

SAP AI Core uses Kubernetes clusters which provide proven features for running container-based applications for training and serving AI models. Such a cluster can provide different resources according to the current requirements. E.g., GPU nodes for heavy duty AI models or running complex ML AI pipelines that require different resources per step. The Kubernetes infrastructure is very fast and flexible (scale up / down the containers which run the AI code).

Model once and serve multiple customers
Customers benefit from simplified training and inferencing with reusable templates and can deliver model templates for their customers or internal teams, to train the model on their own data.

Operationalize AI with SAP AI Core i.e., integrate AI into SAP and custom applications using AI API. Data can be extracted from SAP HANA Cloud, External Cloud or On-Premise Storage.

To realize those added values, SAP AI Core and SAP AI Launchpad offer enterprise-grade features to productize and operate your AI models. The main capabilities that SAP AI Core provides are the orchestration of AI workflows, such as model trainings and batch inference, as well as serving model inference, so that models can make predictions.

To ease the shipment of new AI scenarios, the service offers continuous delivery capabilities and ensures tenant isolation with multi-tenancy. To safeguard your costs and to scale on demand, you only pay for the resources you used but can tap into scalability and increased performance powered by GPUs.

All of the functionality comes out of the box and is managed for you while retaining openness to any AI framework so that you can ship your AI scenario easily.

To connect to your business applications as well as to operate your AI scenarios in AI Launchpad, SAP AI Core provides the standardized AI interface, AI API, which provides a common framework for consuming and operating your AI scenarios.

During development your AI scenarios, you are supported with development tooling, such as the SAP AI Core SDK, and full flexibility in the choice of storage for your data and models.

The process of building a new AI scenario on SAP AI Core involves the following steps: an initial configuration, the model training, and the inferencing. Below is the end-to-end ML Workflow in SAP AI Core, which shows the various sub-steps in each of these steps. Later on, we will start with the onboarding and initial configuration of your instance of SAP AI Core and SAP AI Launchpad.

Description of the end-to-end ML workflow in SAP AI Core: Onboarding, Data Upload, Training, Model Deployment, Model Serving (Inference) via Rest endpoint.
SAP AI Launchpad
SAP AI Launchpad is a service on SAP Business Technology Platform (SAP BTP) to transparently manage AI models across the enterprise. Connect to AI API-enabled runtimes, including SAP AI Core, and centralize AI lifecycle management for your AI scenarios with a convenient user interface. The application acts as the single access point for all AI content across your SAP landscape. From an MLOps perspective, SAP AI Launchpad provides customers with the capabilities to capture and analyze metrics that has been created by supported AI runtimes - for example, SAP AI Core.

Users can compare and visualize those metrics. The integration between SAP AI Launchpad and supported AI runtimes is facilitated by a standardized interface called "AI API". Furthermore, the application is focusing on supporting the full lifecycle management and operations of AI processes by providing a holistic view of all metrics, artifacts etc. made available through the integrated AI runtimes and let you analyze and evaluate critical productive AI KPIs. Customers can also productize existing training models of supported AI runtimes or trigger jobs and deploy models directly using SAP AI Launchpad.

Description of SAP AI Launchpad, which is a web application for operating and monitoring the SAP AI Core platform as well as other Cloud AI platforms.
What is the added value of SAP AI Launchpad?
Centralize AI lifecycle management
With SAP AI Launchpad, customers can connect to all AI API-enabled services and streamline the AI lifecycle management of their AI scenarios SAP AI Launchpad. Benefit from uniform model status as well as uniform training and deployment, regardless of the underlying technology. As such, SAP AI Launchpad will not only increase transparency but will also enable to drive innovation by re-using AI content.

Monitor and continuously improve model performance
Customers benefit from simplified model retraining to continuously improve their model performance. They can also productize existing training models of supported AI runtimes or trigger jobs and deploy models directly using SAP AI Launchpad. The different resource groups (tenant) are displayed in SAP AI Launchpad.

AI Core Configuration
Watch the following video to learn about AI Core Configuration.

Resource Groups
The SAP tenant concept provides a separation of data and workspaces of customers in an SAP system.

As SAP AI Core is based on a Kubernetes cluster, the technical separation is achieved by using Kubernetes Resource Groups. You can create resource groups to isolate ML workloads, for example, to separate users/ tenants.

There are 2 use cases:

An SAP LOB provides embedded AI for different customers: The LOB is administrating the BTP tenant for SAP AI Core. In order to make sure that customer data and AI assets like models and trainings workflows are safe/secured, each "subtenant" i.e. customer is managed in AI Core using a separate resource group.

A customer is using SAP AI Core directly: As the customer is using his own BTP Organisation (i.e. Tenant), data and AI assets are safe and cannot be accessed by other customers. The Customer can still use resource groups to separate data and AI assets for different departments by assigning a resource group for each department of the company.

https://help.sap.com/docs/AI_CORE/2d6c5984063c40a59eda62f4a9135bee/8aae6cbe2c0e4290954b8f61b4b355b7.html?locale=en-US

Connect Tools and Manage Your Credentials
Creating secrets for your tools means that you can connect external programs and tools without compromising your account. The tools can be used to incorporate version control, cloud storage, and portable containers.

Using SAP AI Core alongside external tools such as GitHub, Docker, and Amazon Web Services S3 storage leverages the benefits of version control, containerization and cloud storage. As a result, your content is made available remotely, where you have a stable internet connection.

This section is a one-time procedure. It is a dev ops based configuration that connects these tools to your SAP AI Core. You may repeat the steps, if necessary - for example, to remove or replace access to an external tool.

Note

You must have completed tasks in Initial Setup before configuring your SAP AI Core instance.

Prerequisites
Steps
Manage your Git Repository.

You can use your own Git repository to version control your SAP AI Core templates. The GitOps onboarding to SAP AI Core instances involves setting up your Git repository and synchronizing your content.

Managing Your Git Repository | SAP Help Portal

Manage Resource Groups.

A resource group represents a unique workspace environment where users can create or add entities such as configurations, executions, deployments, and artifacts.

Managing Resource Groups | SAP Help Portal

Manage your Object Store Credentials.

Connect SAP AI Core to a cloud object store and manage access using an object store secret. The connected storage is used as storage for your dataset, models, and other cache files of the Metaflow Library for SAP AI Core.

Managing Your Object Store Credentials | SAP Help Portal

Manage docker credentials.

Docker facilitates the packaging and running of an application in a remote container. Connect SAP AI Core to a Docker repository and manage access using a Docker registry secret.

Managing Docker Credentials | SAP Help Portal





Training an ML model
Objective

After completing this lesson, you will be able to trigger training of an ML model based on customer data
Model Training in SAP AI Core
Steps for Model Training: Data Upload to e.g. S3, Submit Training Argo Workflow, Access resulting Model artifact.
Note

You can add multiple datasets for Training. Switching between training datasets can be achieved using template parameters.

In order to train a model, a dataset needs to be provided and made available to SAP AI Core. You can achieve this by storing the datasets in a hyperscaler object store and providing the required access rights to SAP AI Core.

Hint

A hyperscaler object store represents the most cost effective, robust, and durable way of managing datasets and models since it is characterized by the ability to massively expand under a controlled and efficiently managed software-defined framework. Moreover, as SAP AI Core is designed to work with containers, an object store makes data access convenient for Argo in a transparent, automatic fashion by copying data in/out of pipeline containers and assuring secure credential storage.
Step 1
In the model train block connects an object store to SAP AI Core and loads the training dataset.

Step 1 also segregates the AI artifacts, which is achieved by balancing the creation of SAP AI Core instances and resource groups. Clearly, different tenants can isolate different AI artifacts, but it is also possible to define namespaces to group and segregate related resources within the scope of one SAP AI Core tenant.Tenant Separation in SAP AI Core via “Resource Groups”. Assets like Data, Training and Serving Templates cannot be accessed by other tenants.
AI Core: Register the Input Dataset
One example of an asset that can be segregated within a resource group is the input dataset used to train an ML model. The registration of a dataset in SAP AI Core assumes the creation of a resource group. That resource group is only visible to those people allowed to access that specific resource group and is consumed by all those components belonging to the same resource group.

Note

Although the segregation of assets using Res.grps is valid, the important point here is to provide different datasets that are injected into the same training pipeline in different runs, however, resulting in different models.

This can be done periodically, or by shuffling one large dataset into different variants to get the best generalizing model.

AI Core: Create a Configuration
As mentioned before, you can use SAP AI Core to design their own training pipeline and specify it into a training template, which is used to define an executable object in SAP AI Core. For the sake of convenience, it falls under the umbrella of a specific scenario that acts as an additional namespace and groups all the executables together that you need to solve your specific business challenge. The executables, together with the input dataset, are the key components that you need to bind together to create a training configuration and then to train an ML model in SAP AI Core.

Note

For the sake of convenience, it falls under the umbrella of a specific "scenario" that acts as an additional namespace.

Unfortunately, the scenario is not a namespace. It is used for grouping items of a given ML "project."

A configuration is a set of parameters which can be changed for every run.

AI Core: Trigger the Workflow Execution
An executable includes information about the training input and output, the pipeline containers to be used, and the required infrastructure resources. SAP AI Core provides several pre-configured infrastructure bundles called resource plansthat differs from one another depending on the number of CPUs, GPUs, and the amount of memory. Therefore, SAP AI Core can efficiently deal with demanding workloads.

Another important point when working with ML, is evaluating the model’s accuracy. ML models are used for practical business decisions, so more accurate models can result in better decisions. The cost of errors can be huge, but the impact of these mistakes can be mitigated by optimizing the model’s accuracy. This being said, a customer using SAP AI Core might ask: how am I going to evaluate the performance of my model? The answer is that SAP AI Core provides several APIs to register your favorite metrics that can be retrieved and inspected once the training is complete.

AI Launchpad: Check the Status of Execution and Logs
When the model has completed training and has satisfactory metrics, the final step of the model training block consists of harvesting the results. When the model is produced and stored by SAP AI Core in the same connected hyperscaler object store, it is automatically registered in SAP AI Core and is ready for deployment.

Ingest Live Data into your House Price Predictor with SAP AI Core
Build data pipelines and reuse code to train and generate models on different datasets.

Steps
Ingest Live Data into your House Price Predictor with SAP AI Core | Tutorials for SAP Developers


Serving an ML Model
Objective

After completing this lesson, you will be able to deploy an ML model in SAP AI Core and use the deployment URL for inferencing
Model Deployment in SAP AI Core
After a model is trained, that is, it has learned (hidden) patterns from the provided dataset, the model needs to be deployed. By deploying the model, one can send new data to the model and get a "prediction" for the given data record. This is also called Model Serving or Inferencing.

A Kubernetes cluster can be configured to provide both CPU (cheap) and GPU containers for Model Serving.

In addition, a high number of requests can be sent to the Model Server at the same time. In order to process these "inference" requests in a timely fashion, Kubernetes allows to scale the Model Server "on demand".

Here we have 2 cases:

Autoscaling: adding (cloning) new containers on demand.

Scale to Zero: enables cost efficiency and pay per use, by shutting down idle containers.

Deploying a model in SAP AI Core consists of writing a web application that is able to serve the inference requests through an endpoint exposed on the internet, which could be easily scaled on the Kubernetes infrastructure.

Steps for deploying a trained model: submit a Serving Template to obtain the deployment URL, which can be used in any app for model inference.
Serving Application
To serve a model, you code and develop a serving application that will be run in the form of a container.

Everything starts with an inference request sent to an endpoint. Internally, the web application has to interpret the data contained in the body of the call and then it has to retrieve the model from the hyperscaler object store, apply it to the data and pack the prediction into a response that will be consumed by a custom service.

Description of Serving Application workflow: When data is received by the model server, the data can be preprocessed, for example, normalized before feeding into the model and obtaining the inference result.

Note

While there are different ways of inferencing (that is, batch inference), we are mainly looking into online inferencing with an exposed endpoint (AI API) that the end user calls using an http request.
Coding is fundamental, but, as mentioned before, the model server that will be deployed is defined by a specific template. This self-contained template will create an executable with the definition of the required parameters, the container to be executed, the resources needed for starting the web application, and the number of replicas of the model server to be started.

This combination of the proper serving executable, with the reference to the model to be used, will enable SAP AI Core to start your deployment.

When the model server is running and the deployment URL is ready, the very final step of the ML workflow in SAP AI Core is the consumption of the model through the exposed endpoint. The API can be easily integrated in any business application using an http request, such as a Jupyter notebook, Postman, or a CAP application, and so on.

Build a House Price Predictor with SAP AI Core
Steps
Build a House Price Predictor with SAP AI Core | Tutorials for SAP Developers

..........................

2. Navigating Large Language Models fundamentals and techniques for your use case

.....

Describing LLMs
Objectives

After completing this lesson, you will be able to:
Describe LLMs
Identify benefits and risks of using LLMs
Introducing Large Language Models (LLMs)
Imagine that you want to use SAP AI Core to transform business data into intelligent, actionable insights across various sectors, including finance, HR, and supply chain management. Before diving into the world of generative artificial intelligence (AI), it is good to know some of the basic terms.

In this lesson, you will learn about Large Language Models (LLMs), and explore the benefits and risks of implementing LLMs in artificial intelligence systems.

An LLM is a type of artificial intelligence model that specializes in processing, understanding, and generating human language. These models are a subset of machine learning models known as deep learning models, which are designed to handle large-scale data and complex pattern recognition.

See the video to learn about LLMs, its capabilities, and benefits.

In-Depth Definition of LLMs
In this video, you will learn about a more detailed definition of LLMs along with their key aspects.

Benefits and Risks of LLMs
Once you have identified LLMs, you want to use generative AI to resolve your queries. Note that answers to your queries can be incorrect or may not present correct facts. It is advised tounderstand the benefits and risks associated with LLMs before you start using them.

In the context of SAP Business AI, LLMs have both benefits and risks that must be considered for businesses.

Benefits and Risks of LLM
Benefits	Risks
Efficiency: In the context of SAP Business AI, LLMs can significantly improve the efficiency of processes due to their ability to understand and process natural language at a large scale.	Data Privacy Concerns: LLMs process a lot of textual data, potentially causing data privacy concerns. If sensitive data is processed, the system should ensure the confidentiality and anonymity of such data.
Cost Reduction: With LLMs, tasks such as customer support, data analysis, and others can be automated, thus reducing operational costs.	Bias and Fairness: LLMs are trained on vast amounts of data from the internet so they might reflect and reproduce the biases present in those datasets.
Data Analysis: LLMs can analyze and interpret vast amounts of data faster and more effectively than humanly possible, providing businesses with valuable insights.	Misinterpretation of Data: While LLMs can understand language, they can also make mistakes and misunderstand or misinterpret data, causing potential issues.
Improved Customer Experience: LLM-based applications can enhance customer experience by offering personalized assistance and real-time responses.	Dependency: Excessive reliance on LLMs can make a business vulnerable if the system stops working or delivers inaccurate results.
Scalability: LLM can handle increasing amounts of work and interactions due to its deep learning capabilities.	Technical Complexity: Implementing, fine-tuning, and maintaining LLMs requires technical expertise and resources. Mismanagement could lead to unexpected issues and costs.
Further Reading
AI Ethics at SAP
SAP believes that artificial intelligence (AI) has the potential to unlock all kinds of opportunities for businesses, governments, and society. However, AI also has the potential to create economic, political, and social challenges. Many people are concerned about the impact of AI on their lives as well as the associated uncertainty and risks. SAP acknowledges AI’s profound impact on decisions, fairness, transparency, privacy, and human dignity. For this reason, the development, deployment, use, and sale of AI systems at SAP is governed by a clear, ethical set of rules that takes into account technological advancements and the evolving regulatory landscape.

In this free online course, you’ll learn the importance and principles of AI ethics, how SAP set up an ethics governance framework, and which rules SAP employees need to follow during the development, deployment, use, and sale of AI systems.

​SAP Generative AI Cybersecurity Strategy​​ Learn about Generative AI (GenAI) and how it offers tremendous potential to improve the way the world conducts business, but also introduces new and complex security challenges, and how SAP is addressing both its use and its challenges.


Describing SAP's Generative AI Strategy
Objective

After completing this lesson, you will be able to describe SAP's approach towards LLMs
SAP's Approach to LLMs
Now that you are familiar with the benefits and risks associated with Large Language Models (LLMs), let's understand what is SAP's approach towards LLMs.

LLMs in SAP AI Core
Large language models are a crucial part of the generative AI hub in SAP AI Core.

With these models, the SAP AI Core can perform tasks such as drafting e-mails, writing code, creating written content, translating languages, and much more. They enable users to generate coherent, contextually appropriate sentences and paragraphs in response to a given prompt.

SAP's large language models are not just limited to text applications. They can collaborate with other AI models for more complex tasks, helping businesses to streamline their operations, improve their decision-making processes, or create more interactive and personalized customer experiences.

One of the most significant advantages of these models is that they can be trained, enabling them to learn and improve over time, adapt to various use cases, and respond to evolving business needs.

In the context of the generative AI hub, these models are typically used in a software as a service (SaaS) setup, where they are trained and hosted in the cloud, and can be integrated directly into applications, products, or services. Businesses can directly input their unique data into the AI model and receive tailored responses or forecasts, without having to learn how to design and train an AI model.

You can see models and scenarios supported in the generative AI hub here Generative AI Hub | SAP Help Portal

SAP Business AI Approach
See the video to understand how SAP Business AI works to make your business applications more intelligent.



Starting From Ideation to Productization
Objective

After completing this lesson, you will be able to learn about the SAP product management life cycle for generative AI use cases
Steps to Integrate LLMs into Business Applications
After learning about the SAP Business AI approach, you want to use Large Language Models (LLMs) in your business applications. You want to learn about SAP's methodical approach to embedding LLM capabilities in its solutions.

SAP's approach to embedding LLM capabilities is structured and methodical, aiming to harness the potential of AI while navigating the challenges of implementation and commercialization. The strategy not only seeks to enhance SAP's product offerings but also to establish a framework for integrating cutting-edge AI technologies in a scalable and efficient manner.

This lesson will help you identify the "bigger picture" across the different product development phases. It will identify the roles that must be included for each task, involving the following personas:

Data Engineers & Software Architects: Focused on the technical aspects of AI integration.
Product Management & Product Owners: Responsible for the product's road map, vision, and commercial success.
Solution Management: Engages with the technical and business aspects to ensure that the product meets market needs.
User Experience: Concentrates on how users receive products, including feedback and usability.
See the video to learn about the key steps involved in integrating LLMs into business applications.

Let's look at each of these steps in more details.

1. Ideation Phase
The Ideation phase is focused on identifying potential use cases for LLMs within SAP's ecosystem. It emphasizes the importance of technology exploration to assess the feasibility and potential benefits of implementing LLMs. We can highlight diverse applications of LLMs, such as language generation, text completion, chatbots, content summarization, and understanding tasks (for example, sentiment analysis, text classification, translation). An iterative approach to experimentation is recommended, starting with simple ideas, and gradually refining them to optimize LLM potential for specific use cases.

During the Ideation phase, the focus is on laying the groundwork for generative AI applications within SAP's offerings. This involves:

Familiarization: Individuals, particularly Data Engineers and Software Architects, are encouraged to learn about the generative AI technology. This is done through reviewing SAP's AI strategy, accessing learning resources, and experimenting with the technology. For more information, see https://www.sap.com/products/artificial-intelligence.html.
Customer Collaboration: Engaging with customers to explore potential use cases is key. This step is essential for Product Management & Product Owners who need to understand the customer's needs to ideate effectively. For more information, see Design-Led Development Process.
2. Validation Phase
The validation phase assesses the feasibility, desirability, and viability of implementing LLMs. This involves determining whether the problems at hand are suitable for LLMs, based on their capability to understand or generate natural language. There are clear "exclusion criteria" for tasks not suitable for LLMs, such as calculations, and compares the efficacy of LLMs against traditional machine learning techniques for certain problems. Cost-benefit analysis and customer data access are crucial factors in this phase, with specific emphasis on compliance with data protection policies when using third-party LLMs.

In the Validation phase, the aim is to refine the ideas into viable use cases and assess their feasibility:

Use Case Iteration: This includes clearly defining the business problem and solution, and iterating on the value proposition through customer feedback sessions. Ensure that you follow ethical and data protections principles for your use case (mandatory for use cases with personal and/or personally identifiable information (PII) data).
Technical Evaluation: Data Engineers & Software Architects ensure access to data for evaluation, test various models, and follow architectural guidelines defined for their use case.
Commercialization Evaluation: Product Management & Product Owners should review pricing and commercialization and conduct cost-benefit analysis to understand the financial viability.
3. Realization and Productization Phase
The third phase is where the developed use cases are turned into actual products:

Use Case Finalization: Based on customer feedback, the use case is refined to ensure it meets business requirements.
Technical Finalization: Feasibility and security are key focus areas here, ensuring the product can be reliably deployed. Software Architects are primarily responsible for these activities.
Commercialization Finalization: It is crucial to define the pricing model, finalize business cases, and establish a metering concept for consumption objectives . This is where the commercial aspect of the product is solidified, followed by regular productization steps like executing the commercialization model.
4. Operation, Continuous Improvement Phase
The final phase involves the ongoing management and enhancement of your product:

Operations: Regularly gathering customer feedback, monitoring user adoption, and assessing model performance are critical tasks. This is where the User Experience team plays a significant role.
Continuous Improvement: The product is continuously refined based on real-world usage data and customer feedback, ensuring that it remains effective and relevant.
This structured approach ensures that each phase of product development is thorough, collaborative, and geared toward creating robust, market-ready generative AI products that adhere to SAP's standards for software development and operations life cycle.

Integrating LLMs into Business Applications
Objective

After completing this lesson, you will be able to integrate LLMs into business applications
Integration of LLMs into Business Applications
Integrating Large Language Models (LLMs) into business applications using SAP’s methodical approach involves several steps. Some of these steps can be:

Needs Assessment:
Identify the use cases where an LLM application or usage can add value, such as natural language understanding, sentiment analysis, or chatbots, using ideation to validation phases.

Use data frameworks in SAP Business Technology Platform (SAP BTP):
SAP BTP supports leading data frameworks, such as LangChain and LlamaIndex, for LLM development. Just like LLMs, they are used in a SaaS setup, where they are trained and hosted in the cloud and can be integrated directly into applications, products, or services.

LangChain is a framework designed for developing data-aware applications powered by LLMs. It excels in integrating multiple tools and building intelligent agents capable of performing various tasks. Developers can use Python and JavaScript with LangChain.

On the other hand, LlamaIndex, formerly known as GPT Index, focuses on data ingestion, structuring, and access. It shines in smart search, retrieval, and efficient data structuring, offering various engines for natural language access to data.

Select the Right LLM Framework:
You can choose, for example, between LangChain and LlamaIndex. While LangChain provides a broad framework for LLM applications, LlamaIndex specializes in data indexing and retrieval.

Choose LangChain for its versatility in agent-based applications.

Opt for LlamaIndex when efficient data handling is crucial.

You can even consider integrating LlamaIndex into LangChain to optimize retrieval capabilities within SAP BTP.

Identify Some Use Cases for Selected Frameworks:
Complementary Strengths: Combining LangChain’s agent-based capabilities with LlamaIndex’s efficient data indexing creates a powerful synergy for LLM applications.
Tailored Solutions: Whether developing an enterprise chatbot or a data analysis tool, the combination of these frameworks offers comprehensive solutions on SAP BTP.
It is important to note that this integration is not just a technical feat; it’s a strategic orchestration of data, intelligence, and user experience.

Further Reading
Recommended reading for this unit:

Prompt LLMs in the generative AI hub in SAP AI Core & Launchpad

LangChain vs LlamaIndex: Enhancing LLM Applications on SAP BTP

Identifying the Appropriate Optimization Technique for Different LLM Use Cases
Objective

After completing this lesson, you will be able to identify the appropriate optimization technique for different LLM use cases
Reasons Why LLMs Can Underperform on Your Use Case
Large Language Models (LLMs) can have limitations and it's advised that you explore advanced techniques to maximize the performance of LLMs for your specific use case.

In this unit, you will identify the challenges for maximizing LLM performance, and then learn about practical strategies that drive the efficiency and effectiveness of these AI models.

Challenges of Maximizing LLM Performance on Use Cases
See the video to identify challenges of maximizing LLM performance for your use cases.

These challenges need a series of steps to optimize the performance of the model for your use case.

Optimization is rarely linear and often requires an iterative approach, moving back and forth between different techniques like prompt engineering, RAG, and fine-tuning based on ongoing evaluations.

Optimal LLM performance often requires a combination of these techniques. The choice depends on whether the issue is about context, action, or both.

See the following video to know more about the optimization journey using various techniques.

Successful optimization involves consistent evaluation of outputs and iteration between different techniques. It's an ongoing process to find the best approach for a given problem.

Understanding Principles of Prompt Engineering
Objective

After completing this lesson, you will be able to describe prompt engineering principles
Principles of Prompt Engineering
Prompt engineering is the process of crafting inputs for artificial intelligence systems to guide them in generating the desired outputs. It involves a careful selection of prompting phrases to elicit specific information or responses from the model. These prompts help guide the model's behavior and output, allowing users to interact with the model more effectively to get desired results.

See the video to learn about the basic principles of prompt engineering.

Advanced prompting techniques, like Chain-of-Thought (CoT) and Tree-of-Thought (ToT), are designed to enhance the problem-solving capacity of Large Language Models (LLMs). They structure prompts in a way that guides the models through complex reasoning processes.

Chain-of-Thought (CoT):
CoT is a straightforward technique used to generate multiturn responses by extending the input prompt. Instead of providing a single prompt, multiple related prompts are concatenated to form a chain. The model processes this concatenated chain as a single input, allowing it to maintain context and continuity throughout the conversation. By including previous conversation history, the generated responses become more consistent.

Here is an example of explaining how a seed grows into a plant.

Initial Prompt:
"Can you explain how a seed grows into a plant?"

Chain of Thought Prompts:
Prompt 1: "What is the first stage of seed germination?"

Prompt 2: "What happens during the seedling stage?"

Prompt 3: "How does the plant develop leaves?"

Prompt 4: "What is the role of sunlight in plant growth?"

This CoT approach ensures that a complex process is broken down into easy-to-understand parts, making it simpler to grasp the overall concept.

This chaining of prompts helps the model understand the context better and generate more focused responses.

Tree-of-Thought (ToT):
The Tree-of-Thought technique takes the concept of multiple prompts a step further by creating a branching structure. It allows for exploration of different paths in a conversation, enabling a more extensive and nuanced discussion.

Each branch of the tree represents a different possible path or user query, maintaining its own context while staying connected to the main conversation. By organizing prompts in a tree-like structure, the model can handle different strands of thought simultaneously and switch between them more seamlessly.

Considering same example of explaining how a seed grows into a plant. This approach will branch out into different stages and aspects of seed growth.

Initial Prompt:
"Can you explain how a seed grows into a plant?"

ToT branches:
Branch 1: "What is the first stage of seed germination?"
Sub-branch 1.1: "What happens during imbibition?"
Sub-branch 1.2: "How does the seed coat break?"
Branch 2: "What happens during the seedling stage?"
Sub-branch 2.1: "How do roots develop?"
Sub-branch 2.2: "How does the shoot emerge?"
Branch 3: "How does the plant develop leaves?"
Sub-branch 3.1: "What is the role of cotyledons?"
Sub-branch 3.2: "How do true leaves form?"
Branch 4: "What is the role of sunlight in plant growth?"
Sub-branch 4.1: "How does photosynthesis work?"
Sub-branch 4.2: "Why is sunlight important for photosynthesis?"
Both techniques aim to improve the depth and quality of LLM responses, but they do so in different ways that are suited to different types of problems.

CoT is linear, guiding the model through a single chain of reasoning steps to reach a conclusion. ToT is nonlinear, allowing the model to generate and evaluate multiple lines of reasoning simultaneously, similar to branches on a tree.



Identifying RAG Use Cases
Objective

After completing this lesson, you will be able to identify RAG use cases
Retrieval Augmented Generation (RAG)
You are aware of the challenges that Large Language Models (LLMs) can face. You have also learned that successful optimization involves consistent evaluation of outputs and iteration between different techniques, such as Retrieval Augmented Generation (RAG) and fine-tuning.

In this unit, you will identify a process and a typical architecture for RAG. It also includes an introduction to SAP HANA Cloud vector engine, which enables RAG applications. You will also learn about fine-tuning, agents, functions, and tools to optimize the performance of LLMs for your use case.

As we saw, LLMs output can be factually incorrect or irrelevant for your use case. RAG or Retrieval Augmented Generation can help in improving these responses, making them more precise and reliable for your use case.

See the video to identify the RAG architecture.

SAP HANA Vector Engine
To handle complex and unstructured vector data efficiently in enterprise environments, you can use SAP HANA vector engine.

An SAP HANA vector engine is a component of the SAP HANA database designed to handle complex and unstructured vector data, such as embeddings used in machine learning and AI applications. It allows for the storage, analysis, and processing of vector data, enabling the development of intelligent data applications and adding more context in scenarios involving generative AI.

The HANA vector engine is optimized for executing vectorized operations on large datasets, allowing for efficient parallel processing.

Techniques, such as RAG, extends the capabilities of the vector engine by incorporating retrieval operations, such as filtering and sorting, into the generation process itself. This means that the data is retrieved and processed in a single operation, eliminating the need for extra round-trips between the CPU and storage.

Here are some key benefits of using an SAP HANA vector engine.

Performance: It provides a high-performance vector store that can handle large volumes of data, which is crucial for AI applications.

AI Integration: The Vector Engine facilitates the integration of AI models, like those used in RAG, with enterprise-grade databases for enhanced query responses.

Scalability: It supports the development of scalable AI applications that can grow with the needs of the business.

Data Analysis: The engine allows for the seamless processing, comparison, and utilization of vector data, which is vital for building intelligent data applications.

Contextual Understanding: By storing and analyzing vector embeddings, the Vector Engine adds more context to generative AI scenarios, improving the relevance and accuracy of the AI’s output.

Frameworks Integration: With the integration of frameworks like LangChain, it becomes easier to build chat-based applications that can answer technical questions or provide information spread across many pages of a website.

In summary, an SAP HANA vector engine is designed to meet the demands of modern data-driven enterprises, enabling them to apply the power of AI for data analysis and decision-making processes. For example, it facilitates the use of RAG by providing a high-performance vector store that can build scalable and efficient AI applications grounded in factual data.

Further Reading
BTP Reference Architecture for RAG(Reference Architecture): ​This reference architecture accommodates both Cloud Foundry and Kyma runtime, providing adaptability in your endeavor to use generative AI on SAP BTP.

SAP HANA Cloud introduces new vector capability to usher in the future of intelligent data applications and enhance developer productivity Harness the power of AI with intelligent data apps and insights.

SAP HANA Cloud vector engine guide is available at SAP HANA Cloud, SAP HANA Database Vector Engine Guide.

Introducing Fine-Tuning
Objective

After completing this lesson, you will be able to identify the need for fine-tuning
Fine-Tuning Large Language Models (LLMs)
In the optimization journey, you can fine-tune Large Language Models (LLMs) to improve their performance for your use case.

Fine-tuning refers to the iterative process of making minor adjustments to a model's parameters or architecture to improve the model's performance on specific tasks.

See the video to learn more about improving your model's performance.

Compared to prompt engineering and RAG, fine-tuning is a resource-intensive and costly process. It requires substantial labeled data, computational resources, and expertise in deep learning models. It is important to know when to use fine-tuning, and when to avoid using it.

See the video to learn about the considerations of when to fine-tune LLMs, and when to avoid it.


Optimizing LLM Performance Using Agents
Objective

After completing this lesson, you will be able to optimize LLM Performance using agents, functions, and tools
Agents, Functions, and Tools for Optimizing LLM Performance
You can use prompt engineering, RAG, and fine-tuning to gain better response from Large Language Models (LLMs). The LLMs can be used even more effectively when they are paired with agents, functions, and tools that enhance their capabilities.

Agents
See the video to learn about agents and how they can optimize LLMs.

Agents are software programs that act like intermediaries between humans and LLMs. They can take user input, translate it into prompts for the LLM, and interpret the LLM's output back into natural language. Agents can also help manage the interaction between the user and the LLM, ensuring that the conversation is productive and efficient. There are many different types of agents; each with its own strengths and weaknesses. Some agents are designed to be general-purpose, while others are specialized for specific tasks. The best agent for a particular use case depends on the specific needs of the user.

Functions
Functions are small units of code that perform specific tasks. LLMs can be augmented with functions that provide them with additional capabilities, such as access to external data sources or the ability to perform complex calculations. Functions can be written in various programming languages, and they can be easily integrated into LLM applications. Using functions can significantly improve the performance of LLMs on specific use cases. For example, an LLM that is used for generating code could be augmented with a function that can connect to a code repository and retrieve relevant code snippets. This allows the LLM to generate more accurate and complete code.

Tools
Tools are software programs that can be used to interact with LLMs. These tools can help users to create prompts, manage LLM interactions, and interpret LLM output. There are various tools available for LLMs, ranging from simple text editors to complex programming frameworks. The tools used depend on the specific needs of the user. For example, a developer who uses an LLM to generate code might use a text editor to create prompts and a debugging tool to inspect LLM output.

Scenario Using the Agent, Functions, and Tool Interaction
Consider a scenario where a user wants to generate a marketing report. They can use an agent to create a prompt for the LLM that specifies the desired content and format for the report. The agent can then use a function to access a company's data warehouse and retrieve relevant data for the report. Finally, the LLM can generate the report based on the data and the user's prompt.

In this example, the agent acts as a bridge between the user and the LLM, ensuring that the user's request is understood and the LLM's output is relevant and actionable. The function provides the LLM with access to the necessary data, while the LLM does the heavy lifting of processing the data and generating the report. By effectively using agents, functions, and tools, users can streamline their LLM workflows, improve the quality of their results, and gain a deeper understanding of the capabilities of these powerful tools.

Evaluating and Testing Your LLM Use Case
Objective

After completing this lesson, you will be able to identify methods to evaluate and test your LLM use case
Methods and Metrics for Model Evaluation
Once you have explored Large Language Models (LLMs), prompt engineering, fine-tuning, and other techniques to streamline the performance of LLMs for your use case, you want to assess the performance of LLM in your use case.

In this unit, you will learn about the key metrics and methods to assess LLM performance for your use case. You will learn basic concepts, for example, model inference, to provide stable and reliable LLM predictions. You will also identify some best practices to ensure optimal performance for LLM applications.

See the list of common metrics used to assess the LLM performance:

Perplexity:
Perplexity measures how well a language model predicts a given sequence of words. Lower perplexity indicates better performance.
Bilingual Evaluation Understudy (BLEU):
BLEU assesses the quality of machine-generated translations by comparing them to reference translations. It computes precision for n-grams (typically 1 to 4) and averages them. N-grams are set a consecutive n-words like pairs of consecutive words, three consecutive words, and so on, that are extracted from a given sample of text or speech. These are used to analyze the word order or context for tasks like text classification.
While commonly used in machine translation evaluation, BLEU can also be relevant for text generation tasks. For example, it can be used to measure the similarity of generated text (for example, summaries) to human reference text.

Recall-Oriented Understudy for Gisting Evaluation (ROUGE):
ROUGE evaluates the quality of text summarization systems. It measures the overlap between system-generated summaries and reference summaries.
Classification Accuracy:
This metric measures the proportion of correctly predicted instances out of the total. For example, if an LLM classifies customer reviews as positive or negative sentiment, accuracy tells us how often it predicts correctly.
Precision and Recall:
These metrics are essential for binary classification tasks.
Precision quantifies how many of the predicted positive instances are positive. It helps avoid false positives.
Recall (also known as sensitivity) measures how many actual positive instances were correctly predicted. It helps avoid false negatives.
F1 Score:
Often used for text classification tasks, the F1 score balances precision (correctly predicted positive instances) and recall (actual positive instances).
Word Error Rate (WER):
WER assesses the accuracy of automatic speech recognition systems by comparing system-generated transcriptions to human transcriptions.
Semantic Similarity Metrics:
These metrics include Cosine Similarity, Jaccard Index, and Word Mover’s Distance (WMD). They measure semantic similarity between sentences or documents.
The evaluation metric that you must apply depends on your use case. Each metric provides different insights into model performance.

Best Practices for LLM Applications
See the video to learn more about assessing LLM's performance.

Performance metrics like accuracy, explanatory ability, robustness to errors and efficiency can be considered during model inference.

Use Model Inference
Model inference refers to the stage where a trained Large Language Model (LLM) is deployed into production for making predictions on real-world data. It is an essential phase in extracting value from LLMs.

During inference, new input text, documents, or queries are fed into the LLM API. This could be customer conversations, product descriptions, legal contracts, and so on, based on the business use case.

The LLM then uses its learned linguistic patterns and knowledge gained during pretraining and fine-tuning to analyze the new inputs. Common capabilities explored at inference time can include:

Sentiment analysis
Named entity recognition
Text summarization
Question answering
Language translation
Generating content like emails, reports, and others
The central aspect that distinguishes inference is that the LLM is no longer being trained here. Its parameters are fixed after the training phase. The focus in inference is on using the trained model for business applications by harnessing its predictive power on new, unseen data in a low-latency, scalable and cost-efficient manner. The key objective is to provide stable and reliable LLM predictions that translate into business value.

Explore Best Practices
See the list for some best practices to ensure optimal performance for LLM applications for your use case:

Fine-tune on domain-specific data: Pretrain LLMs further on relevant industry or company data like customer support tickets, legal contracts, and others. This enhances accuracy on business terminology.

Incrementally update training data: Add new product specifications, policy documents, and so on, to continually fine-tune LLMs to maintain performance as new data comes in.

Test with production workloads: Evaluate LLMs with real customer queries, transactions, and others instead of synthetic data to measure production readiness.

Set rigorous quality metrics: Define quantitative KPIs like accuracy, latency, explanation capability to benchmark model performance.

Monitor and address errors: Log model failures during inference to identify areas of improvement and feedback into the next training iteration.

Optimize infrastructure costs: Consider infrastructure, for example virtual machine service, or configurations used for training. Model the inference based on efficiency testing to reduce overprovisioning.

Failover policies for downtime: Create a backup configuration and redundancy for mission-critical LLM applications to ensure 24/7 availability.

Leverage MLOps: Standardize benchmarking, model retraining releases for maintainability, and reproducibility of LLM systems over time.

The role of MLOps in testing and evaluating LLM performance for your use case is described in the next topic.

The goal is to make LLMs easy to integrate and enhance continually across dynamic business use cases. Following these best practices aids reliability, cost control as well as supports future innovation.

Leverage MLops for Testing and Evaluating LLMs Use Case
MLOps plays an integral role in systematically testing and evaluating LLM performance for your use case through:

Automated Benchmarking - MLOps pipelines enable running a diverse set of test cases through CI/CD to benchmark capabilities like accuracy, latency, and explainability across different models, versions, and code changes.

Centralized Performance Logging - All evaluation metrics during training, validation, and inference are logged in an aggregated manner for analysis and model comparison.

Smoother Retraining Setups - Old model versions can be retrained using updated datasets in a reproducible way to measure performance improvements.

Error Analysis at Scale - Logs from all running instances are funneled to identify systematic gaps for models to address via feedback loops or architecture tweaks.

Gradual Rollouts - Models are first served to a small percentage of traffic to test stability before rollout to higher production volumes in a safe manner.

Automated Alerting - Integration with monitoring tools like Prometheus, Grafana, ElasticSearch, Kibana allows setting alerts on metrics deviations.

By standardizing LLM testing protocols and using automation, MLOps enables easier comparison between long running experiments, safe model upgrades, and provides rich analytics dashboards to track progress. This is invaluable for business-critical AI where continuity in performance rigor is essential.

Secure Your LLM Applications
You also need to evaluate the safety and security of your LLM applications and protect them against potential risks. Monitor and enhance security measures over time to safeguard your LLM applications. Detect and prevent critical security threats like hallucinations, jailbreaks, and data leakage. Explore real-world scenarios to prepare for potential risks and vulnerabilities.

Summary
The video summarizes the model testing and evaluation.

Further Reading
This blog post discusses a use case involving multiple AI agents in the context of SAP: Multi AI Agents use case. SAP Maintenance Notification creation.

This page provides an overview of how SAP Business AI capabilities can be embedded into applications and scenarios: Artificial Intelligence | SAP Business AI.

This page provides an overview of AI solutions on an SAP Business Technology Platform: AI tools for SAP application development.


.....................

3. Solving Your Business Problems Using Prompts and LLMs in SAP's Generative AI Hub

...

Exploring Generative AI Hub
Objective

After completing this lesson, you will be able to describe AI foundation and generative AI hub
AI Foundation on SAP BTP
In this chapter, you will learn about SAP’s AI foundations and the tools and services that come with it to help you solve your business challenges. Learn about generative AI hub, how to access it, and how to use it.

Our Business AI is embedded across the portfolio. Joule: A copilot that truly understands your business. Embedded AI capabilities: SAP Cloud ERP, SAP Supply Chain Management, SAP ERP Human Capital Management, Spend Management and SAP Business Network, SAP Customer Relationship Management, SAP Business Technology Platform. Customized AI. AI Foundation on SAP Business Technology Platform. AI ecosystem partnerships and investments: Aleph Alpha, Anthropic, AWS, Cohere, Databricks, Google Cloud, IBM, Meta, Microsoft, Mistral AI, NVIDIA.
SAP’s AI Foundation provides the platform to infuse Business AI across applications and processes, making it a vital resource for extending and innovating within and beyond SAP landscapes. It's the center of gravity for developers who want to orchestrate cutting-edge technology with business context for mission-critical scenarios. SAP uses this foundation to build their own applications and also offers it to customers and partners to bootstrap their own customized solutions so that they can benefit from the same technology integrations.

AI Foundation is a comprehensive set of services and tools for AI developers to develop, deploy, and manage powerful custom-built AI applications and AI-powered extensions on SAP Business Technology Platform (SAP BTP).

It offers ready-to-use, customizable, and business data-grounded AI capabilities, supported by the flexible access to all frontier AI models and specifically generative AI foundation models. SAP embeds AI Foundation across its portfolio.

The following video describes AI Foundation on SAP BTP.

Generative AI Hub
Need for Generative AI Hub
The growing landscape of LLMs necessitates a generative AI hub that will help to facilitate systematic and tool-supported model selection tailored to diverse use cases. Generative AI hub, part of SAP AI Core and SAP AI Launchpad on SAP Business Technology Platform, consolidates trusted LLM and foundation model access, grounded on business and context data, and LLM exploration into a single entity. This streamlines innovation, ensures compliance, and offers versatility, benefiting both SAP's internal needs and its broader ecosystem of partners and customers.

Generative AI Hub in SAP AI Core
The generative AI hub, within SAP AI Core, is crucial in boosting business AI capabilities, acting as the main hub for including generative AI into AI tasks in both SAP AI Core and SAP AI Launchpad.

The following video shows how generative AI hub is constructed within SAP AI Core.

SAP leverages diverse models from multiple providers through the hub, reducing dependency on a single vendor. The generative AI hub also supports innovation by providing accessible tools like the AI playground. As a result, it enhances the accuracy and relevance of AI applications that use SAP's data assets.

Generative AI Hub
Describing the development, deployment, and management of AI solutions and extensions of SAP applications, focusing on adaptability and AI lifecycle management.
Generative AI hub covers the AI lifecycle efforts end to end and enables customers to develop, deploy, and manage custom-built AI solutions and AI-powered extensions of SAP applications.

Access: Enables access to frontier AI models, out-of-the-box selection of compute resources, and orchestration modules (like content filtering, data masking, and more).

Exploration and Development: Provides a comprehensive toolset for building of custom AI solutions and model exploration, including a prompt editor, prompt management, prompt registry, libraries, SDKs, and a fine-tuning service. This also includes an AI playground, chat, and prompt management to explore different models, meta data and parameter changes to find the best fitting technology for your needs. All in a secure and safe environment to interact with cutting-edge technology.

Deployment and Delivery: Software deployment includes all the steps, processes, and activities that are required to make a software system or update available to its intended users. Today, most IT organizations and software developers deploy software updates, patches, and new applications with a combination of manual and automated processes.

Support for Bring Your Own Model (BYOM) as well as allocation of compute resources, (re) training, and serving template workflows. This facilitates efficient deployment and delivery of AI models and applications to ensure they are operational and accessible to users.

Orchestration: AI orchestration refers to the process of coordinating and managing the deployment, integration, and interaction of various AI components within a system or workflow. This includes orchestrating the execution of multiple AI models, managing data flow, and optimizing the utilization of computational resources.

AI orchestration aims to streamline and automate the end-to-end life cycle of AI applications. It ensures the efficient collaboration of different AI models, services, and infrastructure components, leading to improved overall performance, scalability, and responsiveness of AI systems.

Coordinating and managing AI compute workflows and scheduling, content moderation, data masking, agent deployment, grounding capabilities, and inference engines to ensure seamless and efficient operation of AI systems.

Governance: Implementing policies and procedures to manage the development, deployment, and operation of AI systems in compliance with regulatory and organizational standards. This includes logs for tracking and auditing purposes, metering, monitoring, multi-tenancy, CaaS flow, as well as roles and responsibilities.

Adaptability (Adaptation): With custom AI model it's crucial to constantly adapt, whether you need new better models, different model architecture, or simply exchange the underlying dataset.

Benefiting from easier interaction with LLMs for grounding, fine-tuning, custom AI models, and AI Agents, you can drive your AI adaptation at your pace without the need to move to other services. Adaptability allows you to:

Adapt easily by switching models when needed or change orchestration configuration on the fly. Adapt your prompts to be more agnostic by saving different variants that call different LLMs.
Switch models, configure orchestration, add further modules, and register prompt variants for different LLMs.
Benefit from configurations, lifecycle, and change of custom models and content packages.
Trust & Security: Ensuring data privacy, data isolation, and robust security measures to protect AI systems and their data:

No automatic saving of prompts or data
Ensuring data masking and content filtering (prompt injection, jailbreak)
SOC 2, NIST, ISO certifications
Vector Engine
With our grounding capability, we've integrated an SAP managed vector engine (powered by SAP HANA cloud) to simplify retrieving your business documents relevant to a question or task and providing them as context for the LLM.

In summary, the generative AI hub offers a comprehensive suite of tools and services to integrate AI into your applications, exploring generative AI capabilities in a safe environment, ensuring trust, control, and seamless access to foundation models and business data.

Generative AI Hub Access
Before deep diving into using generative AI hub in applications, it's important to discover how to access generative AI hub in SAP AI Core and SAP AI Launchpad.

Here are the main steps to gain access to generative AI hub and integrate an LLM into an application:

Set up a BTP global account:

You need an access to a global BTP account.
See the following tutorial here for detailed process: Setup your Global Account of your SAP BTP Enterprise Account
Provision SAP AI Core from the SAP BTP cockpit:

Provision SAP AI Core from the SAP BTP cockpit within the SAP Business Technology Platform. This process generates a service key, which includes the necessary URLs and credentials to access your SAP AI Core instance.
See the following documentation for this process: Intial setup for SAP AI core
Connect to SAP AI Core tools like SAP AI Launchpad:

Generative AI hub can be accessed through SAP AI launchpad. To access SAP AI Launchpad, you need to connect it to SAP AI Core. You can also connect other tools like Postman and python.
See the details of this in the following tutorial: Set Up Tools to Connect With and Operate SAP AI Core
Create a deployment: Create a deployment, programmatically or via the SAP AI Launchpad, to instantiate a use-case-specific LLM configuration. This step involves:

Referencing a model provider-specific executable (for example, models provided via the Azure OpenAI service).
Configuring parameters like model name, model version, and so on.
SAP AI Core will provide a unique URL for each deployment that can be used to access the LLM.
The details about deployment are explained in the following topic.

Create a Deployment
See the following video to know how to deploy LLMs in generative AI hub.

Note

Each model version has a specified deprecation date. When a deployment uses a model version, it will stop working on that version's deprecation date.

To ensure continued functionality, choose one of the following model upgrade options:

Auto Upgrade: Configure or update your LLM to use the latest model version. When SAP AI Core supports a new model version, your existing deployments will automatically migrate to the latest version. This might imply behavior changes in the updated version.

Manual Upgrade: Update your LLM configuration with a replacement model version of your choice. This version will be used in your deployments, regardless of any updates to the models supported by SAP AI Core. Choosing this path, you need to keep track of the deprecation date and update accordingly. Otherwise, after the deprecation date, the endpoint will not deliver a response.

Note

You need to deploy all the needed models for your scenario before you proceed with developing prompts. For example, in this learning journey you want to use models such as meta--llama3-70b-instruct, mistralai--mixtral-8x7b-instruct-v01, gpt-4o, and gemini-1.5-pro. You can deploy all these models using the steps described in the preceding video.

Explaining Generative AI Hub Applications
Objective

After completing this lesson, you will be able to explain generative AI applications beyond chatbots, using software-driven input and output processing scenarios
Applications of Generative AI
You have deployed foundation models (FMs) in generative AI hub. You can start experimenting with deployed models in SAP AI Launchpad. However, these FMs are not limited to just human to human interaction, they have a wider use case. Let’s explore that in this lesson.

Generative AI, particularly FMs , has opened a wide range of potential applications that go beyond the traditional chatbot paradigm. While many people still associate generative AI with applications that follow a specific schema – human input, AI processing, and output for human consumption – the possibilities extend far beyond this.

One exciting application of generative AI is its ability to produce outputs solely based on software, without requiring human instruction. For instance, it can automatically generate reports, create explanations, or translate complex information into easily understandable formats for human readers. This opens opportunities for automating various tasks and making information more accessible.

On the other hand, generative AI can also be used to interpret human instructions and control software systems without necessarily producing output for human consumption. This allows for the development of intelligent interfaces and control systems that can understand and execute human commands effectively.

Furthermore, generative AI can be employed in complex applications where both input and output are handled by software, without any direct human interaction. These LLM-based applications have the potential to automate intricate processes, optimize systems, and solve problems in ways that were previously unimaginable.

It's worth noting that the boundaries between these different application categories are not always clear-cut, as most applications share common components, and advanced prompt engineering techniques can blur the lines between human input and software involvement. However, the key takeaway is that generative AI has the potential to revolutionize a wide range of industries and domains, extending far beyond the realm of chatbots and opening new possibilities for innovation and automation.

See the following video summary of the applications of generative AI.

Use Case for Generative AI hub
Generative AI hub in SAP AI Core can revolutionize your business by leveraging LLMs for advanced email insights and automation, boosting efficiency and productivity.

See the following video for a use case of generative AI hub

Example of a Business Problems Solved Using Generative AI hub
Let's explore an example of using generative AI hub to solve a business problem.

Business Problem: A manufacturing company is facing challenges with maintaining the quality of their product descriptions across multiple platforms. The descriptions are often inconsistent, leading to customer confusion and reduced trust in the brand.

Solution using generative AI hub: The company employs the generative AI Hub to create a unified product description generator. This solution uses a LLM to process existing product information and generate consistent, detailed, and engaging product descriptions that can be used across various sales channels.

Value Proposition for generative AI hub in this Problem: The generative AI hub provides the company with access to advanced AI models that can understand the context and nuances of their products. By generating high-quality content, the company can ensure brand consistency, improve customer satisfaction, and potentially increase sales due to clearer communication. Moreover, the automation of this process frees up valuable time for employees to focus on more strategic tasks.

In this learning journey, we will take a scenario and then provide a detailed solution of a business problem in the next lesson and units.

Developing Basic Prompts for Common Queries
Objectives

After completing this lesson, you will be able to:
Identify business scenario for the learning journey
Develop basic prompts using generative AI hub in SAP AI launchpad
Identify the Facility Management Scenario
Facility Solutions Company is a premier provider of comprehensive facility management, maintenance, and cleaning services tailored for both residential and commercial properties. Their mission is to ensure that your environment remains safe, efficient, and impeccably maintained, allowing you to focus on your core activities without the hassle of facility upkeep.

Their target markets are luxury residential complexes, apartments, and individual homes. They also serve commercial Properties like office buildings, retail spaces, industrial facilities, and educational institutions.

Business Problem:
Facility solutions company receives thousands of mails pertaining to customers’ requests, complaints, and other messages. The company maintains internal applications to address customer requests and complaints and prioritize them to address these requests in a timely and correct manner.

However, categorizing these mails include manual process to transfer data from mails to internal applications, categorizing, and then prioritizing these tasks.

They turn towards generative AI hub to address this problem.

We will provide stepwise solutions to this problem in this learning journey.

Get Started with Generative AI hub in SAP AI Launchpad
The solution entails assigning incoming mails to the correct service category and assign them correct priorities like urgency and sentiment.

Let’s get started with developing prompt.

First task is to get started with Generative AI hub in SAP AI Launchpad and then categorize messages according to the incoming mail.

Develop a Prompt in AI Launchpad
We will now find the urgency and sentiment in an incoming mail .

Developing a prompt to solve a business problem is a stepwise ideation process. This process is explained in the following steps:

1. Develop a basic prompt
We'll start with a basic prompt and then develop the prompt, finding an output that can be used by application within the company.

The first prompt is:

Python

Copy code

Switch to dark mode
Giving the following message:
---
Subject: Urgent HVAC System Repair Needed

Dear Support Team,

I hope this message finds you well. My name is [Sender], and I am reaching out to you from [Residential Complex Name], where I have been residing for the past few years. I have always appreciated the meticulous care and attention your team provides in maintaining our facilities.

However, I am currently facing a pressing issue with the HVAC system in my apartment. Over the past few days, the system has been malfunctioning, resulting in inconsistent temperatures and, at times, complete shutdowns. Given the current weather conditions, this has become quite unbearable and is affecting my daily routine significantly.

I have attempted to troubleshoot the problem by resetting the system and checking the thermostat settings, but these efforts have not yielded any improvement. The situation seems to be beyond my control and requires professional intervention.

I kindly request that a repair team be dispatched immediately to address this urgent issue. The urgency of the matter cannot be overstated, as it is impacting not only my comfort but also my ability to carry out daily activities effectively.

Thank you for your prompt attention to this matter. I look forward to your swift response and resolution.

Best regards,
[Sender]
---
Your task is to extract
- urgency
- sentiment
"
We start with using an open-source model like meta--llama3-70b-instruct.

Open-source LLMs provide several advantages. They reduce costs, offer transparency, and allow for customization. Additionally, they come with community support, ensure data security, and prevent vendor lock-in. These factors make open-source LLMs a smart and practical choice for developing and deploying language models.

It’s a good practice to think about cost and performance optimization before starting to deep dive into using LLMs.

However, as we can see this response is too lengthy. Our objective is to assign urgency and sentiment to the mails that can be used as an input for a software. This lengthy response is not useful for us as of now. Let us continue to develop it.

Note

You may get a slightly different response to the one shown here and in all the remaining responses of models shown in this learning journey.

When you execute the same prompt in your machine, an LLM produces varying outputs due to its probabilistic nature, temperature setting, and non-deterministic architecture, leading to different responses even with slight setting changes or internal state shifts.

2. Assign values to urgency and sentiment
We will now assign some basic urgency and sentiment values and execute the prompt.

Python

Copy code

Switch to dark mode
Giving the following message:
---
Subject: Urgent HVAC System Repair Needed

Dear Support Team,

I hope this message finds you well. My name is [Sender], and I am reaching out to you from [Residential Complex Name], where I have been residing for the past few years. I have always appreciated the meticulous care and attention your team provides in maintaining our facilities.

However, I am currently facing a pressing issue with the HVAC system in my apartment. Over the past few days, the system has been malfunctioning, resulting in inconsistent temperatures and, at times, complete shutdowns. Given the current weather conditions, this has become quite unbearable and is affecting my daily routine significantly.

I have attempted to troubleshoot the problem by resetting the system and checking the thermostat settings, but these efforts have not yielded any improvement. The situation seems to be beyond my control and requires professional intervention.

I kindly request that a repair team be dispatched immediately to address this urgent issue. The urgency of the matter cannot be overstated, as it is impacting not only my comfort but also my ability to carry out daily activities effectively.

Thank you for your prompt attention to this matter. I look forward to your swift response and resolution.

Best regards,
[Sender]
---
Your task is to extract:
- "urgency" as one of `low`, `medium`, `high`
- "sentiment" as one of `positive`, `neutral`, `negative`
See the following video to execute the prompt.

3. Generate JSON output
We need the output of these values in a format that can be used as an input for a software. We decide to use the JSON output.

JSON offers a simple, language-independent, and lightweight solution for efficient data exchange. It can handle complex structures while remaining easy to parse and generate. JSON's widespread use in APIs and web services ensures interoperability, reduces bandwidth and storage requirements, and simplifies the development process.

We use the following prompt:

Python

Copy code

Switch to dark mode
Giving the following message:
---
Subject: Urgent HVAC System Repair Needed

Dear Support Team,

I hope this message finds you well. My name is [Sender], and I am reaching out to you from [Residential Complex Name], where I have been residing for the past few years. I have always appreciated the meticulous care and attention your team provides in maintaining our facilities.

However, I am currently facing a pressing issue with the HVAC system in my apartment. Over the past few days, the system has been malfunctioning, resulting in inconsistent temperatures and, at times, complete shutdowns. Given the current weather conditions, this has become quite unbearable and is affecting my daily routine significantly.

I have attempted to troubleshoot the problem by resetting the system and checking the thermostat settings, but these efforts have not yielded any improvement. The situation seems to be beyond my control and requires professional intervention.

I kindly request that a repair team be dispatched immediately to address this urgent issue. The urgency of the matter cannot be overstated, as it is impacting not only my comfort but also my ability to carry out daily activities effectively.

Thank you for your prompt attention to this matter. I look forward to your swift response and resolution.

Best regards,
[Sender]
---
Extract and return a json with the following keys and values:
- "urgency" as one of `low`, `medium`, `high`
- "sentiment" as one of `positive`, `neutral`, `negative`
Your complete message should be a valid json string that can be read directly and only contain the keys mentioned in the list above.
The prompt is the same as the previous prompt, but the last few sentences now requests a JSON string.

We again execute this prompt in SAP AI Launchpad.

The following screenshot shows the version 3 of the prompt:

A Prompt Editor interface from a Generative AI application. The editor is titled Analyzing mail categories and is on version 3. The prompt message instructs to extract and return a JSON with keys urgency and sentiment with specified values. The response box contains the JSON output: {urgency: high, sentiment: neutral}. The model used is meta-llama3-70b-instruct. The metadata section includes tags and notes, with an Update button. The interface also has options to save, create new, and select prompts.
You can see a JSON output.

4. Ensure that the JSON formatting is correct
This step is needed when the JSON output format is not clear in the previous prompt. You can make it clearer using the following prompt:

Python

Copy code

Switch to dark mode
Giving the following message:
---
Subject: Urgent HVAC System Repair Needed

Dear Support Team,

I hope this message finds you well. My name is [Sender], and I am reaching out to you from [Residential Complex Name], where I have been residing for the past few years. I have always appreciated the meticulous care and attention your team provides in maintaining our facilities.

However, I am currently facing a pressing issue with the HVAC system in my apartment. Over the past few days, the system has been malfunctioning, resulting in inconsistent temperatures and, at times, complete shutdowns. Given the current weather conditions, this has become quite unbearable and is affecting my daily routine significantly.

I have attempted to troubleshoot the problem by resetting the system and checking the thermostat settings, but these efforts have not yielded any improvement. The situation seems to be beyond my control and requires professional intervention.

I kindly request that a repair team be dispatched immediately to address this urgent issue. The urgency of the matter cannot be overstated, as it is impacting not only my comfort but also my ability to carry out daily activities effectively.

Thank you for your prompt attention to this matter. I look forward to your swift response and resolution.

Best regards,
[Sender]
---
Extract and return a json with the follwoing keys and values:
- "urgency" as one of `low`, `medium`, `high`
- "sentiment" as one of `positive`, `neutral`, `negative`
Your complete message should be a valid json string that can be read directly and only contain the keys mentioned in the list above. Never enclose it in ```json...```, no newlines, no unnecessary whitespaces.
This prompt gives clear instruction to provide a clean format without any quotes or whitespaces.

The following screenshot shows version 4 of the prompt:

A Prompt Editor interface from a Generative AI tool. The interface includes sections for the prompt, response, and metadata. The prompt instructs the AI to extract and return a JSON object with keys urgency and sentiment based on specified values. The response box contains the JSON output: {urgency:high,sentiment:neutral}. The model used is meta-llama3-70b-instruct. There are also options to save, create new, and select prompts, as well as a section for metadata tags and notes.
You can see that the JSON output is now clear. This is ready for software consumption.

5. Simple Categories Based on Business Functions
We have associated urgency and sentiment to a mail. However, we also need more tags for each message to categorize them for business needs. We can start with a simple prompt:

Python

Copy code

Switch to dark mode
Giving the following message:
---
Subject: Urgent HVAC System Repair Needed

Dear Support Team,

I hope this message finds you well. My name is [Sender], and I am reaching out to you from [Residential Complex Name], where I have been residing for the past few years. I have always appreciated the meticulous care and attention your team provides in maintaining our facilities.

However, I am currently facing a pressing issue with the HVAC system in my apartment. Over the past few days, the system has been malfunctioning, resulting in inconsistent temperatures and, at times, complete shutdowns. Given the current weather conditions, this has become quite unbearable and is affecting my daily routine significantly.

I have attempted to troubleshoot the problem by resetting the system and checking the thermostat settings, but these efforts have not yielded any improvement. The situation seems to be beyond my control and requires professional intervention.

I kindly request that a repair team be dispatched immediately to address this urgent issue. The urgency of the matter cannot be overstated, as it is impacting not only my comfort but also my ability to carry out daily activities effectively.

Thank you for your prompt attention to this matter. I look forward to your swift response and resolution.

Best regards,
[Sender]
---
Assign a list of matching support category to the message.
The following screenshot shows the output of this prompt:

A Prompt Editor interface from a Generative AI tool. The editor is titled Analyzing mail categories and is at version 5. The interface is divided into three main sections: Prompt, Response, and Metadata.
You can see categories are assigned to the message.

6. Assigning values to categories from a list
As you can see, the categories have multiple values, at times overlapping with earlier values like urgency. Let’s align them to values defined in the company. Aligning the output to business needs is a key step in addressing a business problem.

Python

Copy code

Switch to dark mode
Giving the following message:
---
Subject: Urgent HVAC System Repair Needed

Dear Support Team,

I hope this message finds you well. My name is [Sender], and I am reaching out to you from [Residential Complex Name], where I have been residing for the past few years. I have always appreciated the meticulous care and attention your team provides in maintaining our facilities.

However, I am currently facing a pressing issue with the HVAC system in my apartment. Over the past few days, the system has been malfunctioning, resulting in inconsistent temperatures and, at times, complete shutdowns. Given the current weather conditions, this has become quite unbearable and is affecting my daily routine significantly.

I have attempted to troubleshoot the problem by resetting the system and checking the thermostat settings, but these efforts have not yielded any improvement. The situation seems to be beyond my control and requires professional intervention.

I kindly request that a repair team be dispatched immediately to address this urgent issue. The urgency of the matter cannot be overstated, as it is impacting not only my comfort but also my ability to carry out daily activities effectively.

Thank you for your prompt attention to this matter. I look forward to your swift response and resolution.

Best regards,
[Sender]
---
Assign a list best matching support category tags to the message:
`facility_management_issues`, `cleaning_services_scheduling`, `general_inquiries`, `specialized_cleaning_services`, `routine_maintenance_requests`, `emergency_repair_services`, `sustainability_and_environmental_practices`, `training_and_support_requests`, `quality_and_safety_concerns`, `customer_feedback_and_complaints`
The following screenshot shows the output of this prompt in SAP AI Launchpad.

A Prompt Editor interface from a Generative AI application. The interface includes sections for the prompt, response, and metadata. The Prompt section contains a message instructing the AI to assign support category tags to a message. The Metadata section shows fields for tags and notes, both of which are currently empty. There are buttons for saving, creating new prompts, and selecting options at the top of the interface.
You can see that categories are assigned to values from the list. These are streamlined for business processing.

7. Generate JSON output for categories values
Similar to step 3 earlier, we need the JSON output for processing these values in a software. We also ensure that the format of the JSON output is without any unnecessary symbols or space.

We use the following prompt:

Python

Copy code

Switch to dark mode
Giving the following message:
---
Subject: Urgent HVAC System Repair Needed

Dear Support Team,

I hope this message finds you well. My name is [Sender], and I am reaching out to you from [Residential Complex Name], where I have been residing for the past few years. I have always appreciated the meticulous care and attention your team provides in maintaining our facilities.

However, I am currently facing a pressing issue with the HVAC system in my apartment. Over the past few days, the system has been malfunctioning, resulting in inconsistent temperatures and, at times, complete shutdowns. Given the current weather conditions, this has become quite unbearable and is affecting my daily routine significantly.

I have attempted to troubleshoot the problem by resetting the system and checking the thermostat settings, but these efforts have not yielded any improvement. The situation seems to be beyond my control and requires professional intervention.

I kindly request that a repair team be dispatched immediately to address this urgent issue. The urgency of the matter cannot be overstated, as it is impacting not only my comfort but also my ability to carry out daily activities effectively.

Thank you for your prompt attention to this matter. I look forward to your swift response and resolution.

Best regards,
[Sender]
---
Extract and return a json with the follwoing keys and values:
- "categories" list of the best matching support category tags from: `facility_management_issues`, `cleaning_services_scheduling`, `general_inquiries`, `specialized_cleaning_services`, `routine_maintenance_requests`, `emergency_repair_services`, `sustainability_and_environmental_practices`, `training_and_support_requests`, `quality_and_safety_concerns`, `customer_feedback_and_complaints`
Your complete message should be a valid json string that can be read directly and only contain the keys mentioned in the list above. Never enclose it in ```json...```, no newlines, no unnecessary whitespaces.
The following screenshot shows the output of this prompt in SAP AI Launchpad.

A Prompt Editor interface from a Generative AI tool. The editor is used to analyze mail categories. The interface includes sections for the prompt, response, and metadata. The prompt section contains a message with instructions for generating a valid JSON string containing specific keys. The response section displays a JSON string with categories: routine_maintenance_requests, emergency_repair_services, and facility_management_issues. The model used is meta-llama3-70b-instruct. There are options to save, create new, and select prompts.
You can see categories in the JSON output that can be processed in a software.

8. Combining All the Steps
We have taken a step-by-step approach to arrive at proper values of urgency, sentiment, and categories in JSON format. Now, let us combine all these steps into a single prompt:

Python

Copy code

Switch to dark mode
Giving the following message:
---
Subject: Urgent HVAC System Repair Needed

Dear Support Team,

I hope this message finds you well. My name is [Sender], and I am reaching out to you from [Residential Complex Name], where I have been residing for the past few years. I have always appreciated the meticulous care and attention your team provides in maintaining our facilities.

However, I am currently facing a pressing issue with the HVAC system in my apartment. Over the past few days, the system has been malfunctioning, resulting in inconsistent temperatures and, at times, complete shutdowns. Given the current weather conditions, this has become quite unbearable and is affecting my daily routine significantly.

I have attempted to troubleshoot the problem by resetting the system and checking the thermostat settings, but these efforts have not yielded any improvement. The situation seems to be beyond my control and requires professional intervention.

I kindly request that a repair team be dispatched immediately to address this urgent issue. The urgency of the matter cannot be overstated, as it is impacting not only my comfort but also my ability to carry out daily activities effectively.

Thank you for your prompt attention to this matter. I look forward to your swift response and resolution.

Best regards,
[Sender]
---
Extract and return a json with the following keys and values:
- "urgency" as one of `low`, `medium`, `high`
- "sentiment" as one of `positive`, `neutral`, `negative`
- "categories" list of the best matching support category tags from: `facility_management_issues`, `cleaning_services_scheduling`, `general_inquiries`, `specialized_cleaning_services`, `routine_maintenance_requests`, `emergency_repair_services`, `sustainability_and_environmental_practices`, `training_and_support_requests`, `quality_and_safety_concerns`, `customer_feedback_and_complaints`
Your complete message should be a valid json string that can be read directly and only contain the keys mentioned in the list above. Never enclose it in ```json...```, no newlines, no unnecessary whitespaces.
The following screenshot shows the output of this prompt in SAP AI Launchpad.

A Prompt Editor interface from a Generative AI tool. The interface includes sections for the prompt, response, and metadata. The prompt section contains a message instructing the AI to categorize mail into specific categories and to return the response in JSON format. The response section displays the AI's output in JSON format, indicating high urgency, neutral sentiment, and categories including facility_management_issues, emergency_repair_services, and routine_maintenance_requests. The model used is meta-llama3-70b-instruct. There are options to save, create new, and select prompts at the top right.
You can see the consolidated output that assigns urgency, sentiment, and categories to customer messages that can be used in software.

You have successfully developed a prompt to move towards a solution of the problem.

In the next unit, we'll learn to use generative-ai-hub-sdk for the same prompts and then evaluate the response from this prompt.

Identifying the Need for Using Generative-AI-Hub-SDK
Objectives

After completing this lesson, you will be able to:
Identify the need of using SDKs for LLMs
Describe generative AI hub SDK
Software Development Kits (SDKs) for LLMs
An SDK is a collection of tools, libraries, and documentation that developers use to create software applications for specific platforms or frameworks. SDKs typically include APIs, sample code, and other resources to help developers build and integrate their own applications with the platform or framework.

Business Problem
Consider the problem that we were solving using the generative AI hub.

We saw that the Facility solutions company faces high volumes of customer communications, requiring efficient processing and prioritization within their internal applications to ensure timely and accurate responses.

We used generative AI hub in SAP AI Launchpad to create basic prompts.

We used a basic prompt to generate a structured response for the prompt. However, the business need is to streamline this process to handle large scale queries and enhance the performance of LLMs for the problem. Going forward, you want to create a customized solution and integrate this output in applications.

You can achieve this using SDKs for LLMs.

Use of SDKs
Here are some reasons why employing SDKs for LLMs is advantageous:

Streamlined Processes:
SDKs provide developers with prebuilt tools, libraries, and APIs that simplify the development workflow. This eliminates the need to implement complex infrastructure or low-level details when accessing LLMs.

Enhanced Efficiency:
SDKs facilitate easier integration of LLMs into applications, improving development efficiency. Developers can concentrate on creating solutions instead of managing model deployment and maintenance.

Customization Opportunities:
SDKs often offer customization features, such as generative-ai-hub-sdk, to leverage the power of large language models available in generative AI hub. Customization enables developers to tailor foundation LLMs for specific use cases.

Improved Performance:
SDKs optimize LLMs for particular hardware (for example, GPUs) and cloud platforms. Google's API, for instance, delivers state-of-the-art on-device latency for LLMs on Android and iOS devices.

In conclusion, SDKs simplify LLM access, boost efficiency, and support customization, making them indispensable tools for developers working with large language models.

Generative-AI-Hub-SDK
You have seen a way to access generative AI hub using SAP AI Launchpad. Using generative-AI-hub-SDK you can leverage the power of LLMs available in generative AI for creating customized solutions combining the power of LLMs with and buisness context using SAP data.

With this SDK, you can interact with these models and create natural language completions, chat responses, and embeddings.

Note

It's recommended that you use python kernel 3.11.9 and above to execute all codes in this learning journey. You may refer to python guide here. In case of any errors, ensure that you have properly deployed, configured, and provisioned generative AI hub in SAP BTP.

Here are some key points:

Installation: You can install the SDK using the following pip command:

Python

Copy code

Switch to dark mode
pip install "generative-ai-hub-sdk[all]"
Note

Here [all] implies installing all extra packages including langchain, which is not available by default.

.
Configuration: Ensure that you have access to generative AI hub and deployed models. Refer to the topic generative AI hub in lesson 1 in unit 1. The SDK reuses configuration settings from the AI-core-SDK. These include client ID, client secret, authentication URL, base URL, and resource group. You can set these values as environment variables or via a config file.

To utilize LLMs, you must configure the proxy modules.

We suggest setting these values as environment variables for AI core credentials via a configuration file. The default path for this file is ~/.aicore/config.json for Mac.

You can use the following steps for Mac:

Open Notepad and replace the values in below json with your AI core Service keys that you downloaded from BTP and press Ctrl + S (Command + O for mac) to save file. A pop up will appear on the screen where navigate to ~/.aicore/ and location and save the file as config.json
In case you are using mac, you might not be able to create .aicore folder directly. In that case, create the folder using the command

Code Snippet

Copy code

Switch to dark mode
mkdir ~/.aicore/
Now use the following command to open config.json file in nano.
Code Snippet

Copy code

Switch to dark mode
nano ~/.aicore/config.json
Now paste the following json script on config.json file.

JSON

Copy code

Switch to dark mode
"AICORE_AUTH_URL": "https://* * * .authentication.sap.hana.ondemand.com",
  "AICORE_CLIENT_ID": "* * * ",
  "AICORE_CLIENT_SECRET": "* * * ",
  "AICORE_RESOURCE_GROUP": "* * * ",
  "AICORE_BASE_URL": "https://api.ai.* * *.cfapps.sap.hana.ondemand.com/v2"
}
For windows, use the following steps:

Create .aicore folder in C:\Users\<current user>
Create a config.json file with the below contents
JSON

Copy code

Switch to dark mode
{
"AICORE_AUTH_URL": 
"AICORE_CLIENT_ID": ,
"AICORE_CLIENT_SECRET": ,
"AICORE_RESOURCE_GROUP": 
"AICORE_BASE_URL":
}
You can get these values from AI Core service key in BTP.

Alternatively, you can refer to the https://github.com/SAP-samples/ai-core-samples/blob/main/10_Learning_Journeys/README.md file for detailed configuration steps to execute the notebook for this learning journey in your system.

Usage Examples:

OpenAI-like API Completion:

Python

Copy code

Switch to dark mode
from gen_ai_hub.proxy.native.openai import completions
response = completions.create(
    model_name="tiiuae--falcon-40b-instruct",
    prompt="The Answer to the Ultimate Question of Life, the Universe, and Everything is",
    max_tokens=7,
    temperature=0
)
print(response)
This Python code generates a completion for the given prompt using the "tiiuae--falcon-40b-instruct" model from the OpenAI library. It asks for a short response about the answer to the ultimate question of life and sets parameters for response length and randomness. Finally, it prints the generated completion.

Chat Completion:

Python

Copy code

Switch to dark mode
from gen_ai_hub.proxy.native.openai import chat
messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Does Azure OpenAI support customer managed keys?"},
    {"role": "assistant", "content": "Yes, customer managed keys are supported by Azure OpenAI."},
    {"role": "user", "content": "Do other Azure Cognitive Services support this too?"}
]
kwargs = dict(model_name='gpt-35-turbo', messages=messages)
response = chat.completions.create(**kwargs)
print(response)
This code interacts with the OpenAI API to simulate a chat conversation. By loading predefined messages into the "messages" list and specifying model details in "kwargs", it sends these inputs to the API to generate a response. This allows seamless integration for querying and getting automated replies within your application using any model, for example, "gpt-35-turbo" model from OpenAI in this code.

Embeddings:

Python

Copy code

Switch to dark mode
from gen_ai_hub.proxy.native.openai import embeddings
response = embeddings.create(
    input="Every decoding is another encoding.",
    model_name="text-embedding-ada-002",
    encoding_format='base64'
)
This code imports the "embeddings" module from "gen_ai_hub.proxy.native.openai" and generates an encoded representation of the text "Every decoding is another encoding." by calling the "create" function. It specifies the model "text-embedding-ada-002" and uses "base64" for the encoding format. This helps convert text into a machine-readable format for advanced text processing.

Using generative-AI-hub-SDK to interact with Orchestration Services
Objectives

After completing this lesson, you will be able to:
Describe orchestration feature in generative AI hub
Using generative-AI-hub-SDK to interact with Orchestration services
Need for Orchestration
You have seen how we can use generative AI hub to get an output that can be used in creating custom applications. However, business scenarios usually require more than the bare consumption of Foundation Models for generative tasks. They need to scale, secure, and manage these solutions.

A flowchart illustrating the orchestration process for handling a request. Steps include Grounding, Prompt Templating, Data Masking, Content Filter, and LLM Access, leading to a response.
Access to generative AI models often needs to be combined with other functionalities. These include:

Prompting models using scenario-specific templates from a Prompt Repository.
Ensuring compliance with AI Ethics and Responsible AI through Content Filtering.
Maintaining data privacy by using Data Masking techniques.
Enhancing models with business context through Retrieval-Augmented Generation (RAG) .
You need a service for coordinating and managing the deployment, integration, and interaction of various AI components.

In the Facility solutions company scenario in this learning journey, you've seen that you need to manually update the model each time for each prompt. You'll see that even in generative-AI-hub-SDK code, you need to write different functions for each model.

This can be an erroneous and time-consuming process leading to complex and redundant code and workflows.

This is where orchestration services can be helpful.

AI orchestration is the process of coordinating and managing how various AI components are deployed, integrated, and interact within a system or workflow.

Orchestration services and workflows in generative AI hub are useful in creating sophisticated workflows without complex code.

We can use orchestration services for different foundational models without changing the client code.

This approach reduces maintenance, enhances control, and optimizes efficiency, helping teams focus on innovation rather than integration. It helps you design powerful AI workflows visually and bring your AI vision to life faster with modular capabilities and intuitive interfaces.

In addition, it allows seamless integration and management of diverse components like data pipelines, AI models, and prebuilt modules (content filtering, data masking). It also ensures efficient execution of multiple AI models, optimizes computational resources, and automates the end-to-end AI lifecycle.

Before starting to develop prompts using generative-AI-hub-SDK, let's explore orchestration services in generative AI hub.

Introduction to Orchestration in Business AI
Orchestration in business AI scenarios integrates content generation with several essential functions.

Key Functions:

Templating: Templating allows you to create prompts with placeholders that the system fills during inference.
Content Filtering: Content filtering lets you control the type of content that's sent to and received from a generative AI model.
Orchestration Workflow: In a basic orchestration setup, you can combine different modules into a pipeline executed with a single API call. Each module's response serves as the input for the next module.
Configuration and Execution: Orchestration defines the execution order of the pipeline centrally. You can configure each module's details and exclude optional modules by passing a JSON-formatted orchestration configuration in the request body.
See the following video for an example of the execution order.

Note

The screens shown in the demo refer to one of the possible models.
Harmonized API: The harmonized API enables the use of different foundational models without changing the client code. It uses the OpenAI API as a standard and maps other model APIs to it. This includes standardizing message formats, model parameters, and response formats. The harmonized API integrates into the templating module, model configuration, and orchestration response.
You'll see an implementation in the next lesson to use multiple models in a seamless manner.

Using Orchestration Service
You need to perform the following steps to use Orchestration service:

Get an Auth Token for Orchestration
Create a Deployment for Orchestration
Consume Orchestration
You can refer to the detailed steps here.

Using Orchestration Service Through Generative-AI-hub-SDK
Here are some codes to demonstrate how generative-AI-hub-SDK can interact with the Orchestration Service.

You need to perform the following steps:

Initializing the Orchestration Service: Before you use the SDK, set up a virtual deployment of the Orchestration Service. After deployment, you'll receive a unique endpoint URL.
Python

Copy code

Switch to dark mode
YOUR_API_URL = "..."
Define the Template and Default Input Values: You can use the URL to access orchestration service.
Python

Copy code

Switch to dark mode
from gen_ai_hub.orchestration.models.message import SystemMessage, UserMessage
from gen_ai_hub.orchestration.models.template import Template, TemplateValue

template = Template(
    messages=[
        SystemMessage("You are a helpful translation assistant."),
        UserMessage(
            "Translate the following text to {{?to_lang}}: {{?text}}"
        ),
    ],
    defaults=[
        TemplateValue(name="to_lang", value="German"),
    ],
)
We are considering an example of a translation assistant here. The code sets up a template for a translation assistant. It defines system and user messages to guide the translation process. The system message establishes the assistant's role, while the user message includes placeholders for the target language and text to translate. It also sets a default target language, making the template ready for immediate use in translating text to German.

Define the LLM:
Python

Copy code

Switch to dark mode
from gen_ai_hub.orchestration.models.llm import LLM

llm = LLM(name="gpt-4o", version="latest", parameters={"max_tokens": 256, "temperature": 0.2})
The code imports the LLM class from the gen_ai_hub module, then creates an instance of an advanced language model, specifically "gpt-4o". This instance is configured with the latest version and tailored parameters like maximum token limit and temperature setting to control response variability. It sets the stage for scalable AI-driven text generation tasks.

Create the Orchestration Configuration:
Python

Copy code

Switch to dark mode
from gen_ai_hub.orchestration.models.config import OrchestrationConfig

config = OrchestrationConfig(
    template=template,
    llm=llm,
)
The code here imports the OrchestrationConfig class from the gen_ai_hub.orchestration.models.config module. It then initializes an instance of OrchestrationConfig using the template and LLM variables. This setup is necessary to configure the orchestration process, ensuring the system uses the specified template and language model parameters.

Run the Orchestration Request:
Python

Copy code

Switch to dark mode
from gen_ai_hub.orchestration.service import OrchestrationService

orchestration_service = OrchestrationService(api_url=YOUR_API_URL, config=config)
result = orchestration_service.run(template_values=[
    TemplateValue(name="text", value="The Orchestration Service is working!")
])
print(result.orchestration_result.choices[0].message.content)
This code leverages the OrchestrationService to run a predefined task using specific template values. It connects to the service through the provided API URL and configuration, executes the task by supplying a text value, and then prints the resulting content. The code ensures streamlined communication with the orchestration system and retrieval of results.

You can further use modules for content filtering and other tasks. See a notebook that demonstrates how to use the SDK to interact with the Orchestration Service.

You will also see an example in the Facility solutions company scenario in the next lesson.

Using Generative-AI-hub-SDK to Leverage the Power of LLMs
Objective

After completing this lesson, you will be able to develop basic prompts using generative AI hub SDK
Load Packages and Data
You've installed and configured generative-ai-hub-sdk. You've also configured orchestration services. We begin with installing packages and then loading data.

Python

Copy code

Switch to dark mode
!pip install -U "generative-ai-hub-sdk>=3.1" tqdm
This code installs the necessary software packages. By using !pip install -U "generative-ai-hub-sdk>=3.1" tqdm, it ensures you have the latest version of the generative-ai-hub-sdk (version 3.1 or later) and the "tqdm" library. The "tqdm" package is a progress bar library that helps display the progress of tasks in the console.

Python

Copy code

Switch to dark mode
from typing import Literal, Type, Union, Dict, Any, List, Callable
import re, pathlib, json, time
from functools import partial
EXAMPLE_MESSAGE_IDX = 10
This code begins by importing necessary modules and types from various Python libraries, including typing, re, pathlib, json, time, and functools. It then defines a constant, EXAMPLE_MESSAGE_IDX set to 10 to select a random 10th message from a dataset.

Next, you need to load and preprocess a dataset of emails stored in a JSON file. You can split dataset into development and testing sets with a smaller test set created for more focused evaluation. You can also process email data, to extract and organize categories, urgency, and sentiment into sets for further analysis.

Helper Functions
Before developing prompts using generative-ai-hub-sdk, you need to enable the use of AI models in generative AI hub. This is done through helper functions. You can see detailed functions in the repository here.

One of the function interfaces with the AI Core SDK to manage deployment tasks efficiently. It includes a spinner function to provide real-time updates during long operations, maintaining user engagement. The function checks for an existing deployment and either retrieves it or creates a new one, coordinating configurations and ensuring smooth execution within a specified timeout. Refer to the detailed code in the repository here.and the relevant readme file.

In case of any errors or issues while executing the code, you may refer some of the troubleshooting tips.

The following function sets up and sends requests to an AI orchestration service.

Python

Copy code

Switch to dark mode
import pathlib
import yaml

from gen_ai_hub.proxy import get_proxy_client
from ai_api_client_sdk.models.status import Status

from gen_ai_hub.orchestration.models.config import OrchestrationConfig
from gen_ai_hub.orchestration.models.llm import LLM
from gen_ai_hub.orchestration.models.message import SystemMessage, UserMessage
from gen_ai_hub.orchestration.models.template import Template, TemplateValue
from gen_ai_hub.orchestration.models.content_filter import AzureContentFilter
from gen_ai_hub.orchestration.service import OrchestrationService

client = get_proxy_client()
deployment = retrieve_or_deploy_orchestration(client.ai_core_client)
orchestration_service = OrchestrationService(api_url=deployment.deployment_url, proxy_client=client)

def send_request(prompt, _print=True, _model='meta--llama3-70b-instruct', **kwargs):
    config = OrchestrationConfig(
        llm=LLM(name=_model),
        template=Template(messages=[UserMessage(prompt)])
    )
    template_values = [TemplateValue(name=key, value=value) for key, value in kwargs.items()]
    answer = orchestration_service.run(config=config, template_values=template_values)
    result = answer.module_results.llm.choices[0].message.content
    if _print:
        formatted_prompt = answer.module_results.templating[0].content
        print(f"<-- PROMPT --->\n{formatted_prompt if _print else prompt}\n<--- RESPONSE --->\n{result}")   
    return result
It imports necessary libraries and modules, sets up a proxy client, and retrieves or deploys an orchestration configuration. It defines a function, "send_request", which configures and sends prompts to an AI model, then formats and prints the response. The function can access all the configured models in your generative AI hub instance using orchestration deployment. This setup aims to streamline AI-driven tasks using a consistent and reusable approach.

Develop a Prompt Using generative-AI-hub-SDK
We will now find the urgency and sentiment in an incoming mail.

Use the following code to read a random mail with the ID assigned earlier.

Python

Copy code

Switch to dark mode
mail = dev_set[EXAMPLE_MESSAGE_IDX]
As we saw earlier, developing a prompt to solve a business problem is an iterative process. This process is explained in the following steps:

Develop a basic prompt: We'll start with a basic prompt and then develop the prompt, finding an output that can be used by applications within the company. The first prompt is:
Python

Copy code

Switch to dark mode
prompt_1 = """Giving the following message:
---
{{?input}}
---
Your task is to extract
- urgency
- sentiment
"""

f_1 = partial(send_request, prompt=prompt_1)

response = f_1(input=mail["message"])
We start with using an open-source model like meta--llama3-70b-instruct using the ‘send_request’ function. The default model in this function is meta--llama3-70b-instruct.

Note that we don’t need to copy the entire mail or model details all the time because we are using generative-ai-hub-sdk.

You can see the following output:

A typed message requesting urgent HVAC system repair. The sender describes severe malfunctions affecting daily life and requests immediate professional intervention. The tone is urgent and polite.
You will see the output prompt. You have used SDK to generate a prompt and response.

You can see prompt part of the output.

An analysis of a message's urgency and sentiment. Urgency is rated high due to phrases like urgent repair needed and unbearable. Sentiment is neutral/negative, showing frustration and distress.
You can see response part of the output.

You see the result upon execution. You can utilize different models using the same function. This is a benefit of the orchestration feature of generative AI hub.

The current lengthy response is not useful for our objective of assigning urgency and sentiment to emails for software input, so we should continue to develop it.

Note

You may get slightly different responses to the one shown here and in all the remaining responses of models shown in this learning journey.

When you execute the same prompt in your machine, an LLM produces varying outputs due to its probabilistic nature, temperature setting, and non-deterministic architecture, leading to different responses even with slight setting changes or internal state shifts.

Assign values to urgency and sentiment: We'll now assign some basic urgency and sentiment values and execute the prompt.
Python

Copy code

Switch to dark mode
prompt_2 = """Giving the following message:
---
{{?input}}
---
Your task is to extract:
- "urgency" as one of {urgency}
- "sentiment" as one of {sentiment}
"""

f_2 = partial(send_request, prompt=prompt_2, **option_lists)

response = f_2(input=mail["message"])
The code defines a prompt for extracting "urgency" and "sentiment" from a given message. It uses the "partial" function to configure a request with predefined options. Finally, it processes the email message by passing it through the configured request to get the needed details. This ensures consistent and accurate data extraction from messages.

You can see the following output:

A prompt asking to extract the urgency and sentiment from an email requesting urgent HVAC system repair. The email describes the issue, troubleshooting attempts, and requests immediate assistance.
You can see the prompt generated in the output.

A text snippet analyzing a message's urgency and sentiment. It concludes the urgency as high due to phrases like Urgent HVAC System Repair Needed and the sentiment as neutral.
You get a response. You can see that the urgency and sentiment of the mail is generated.

You can see that the values for urgency and sentiment are based on assigned values.

Generate JSON output: A software requires input in a specific format, and JSON is chosen for this purpose. JSON is a suitable choice because it's a lightweight, language-independent, and easy-to-parse format that enables efficient data exchange and simplifies development.
We use the following code:

Python

Copy code

Switch to dark mode
prompt_3 = """Giving the following message:
---
{{?input}}
---
Extract and return a json with the following keys and values:
- "urgency" as one of {{?urgency}}
- "sentiment" as one of {{?sentiment}}
Your complete message should be a valid json string that can be read directly and only contain the keys mentioned in the list above.
"""
f_3 = partial(send_request, prompt=prompt_3, **option_lists)

response = f_3(input=mail["message"])
This code prepares and sends a request to extract specific information from a message. It uses a predefined prompt template to instruct the system to extract "urgency" and "sentiment" and return them in JSON format. The "partial" function sets up this request with fixed options, and then it sends the request using the message content.

You can see the following output:

A prompt asking to extract urgency and sentiment from a message about an urgent HVAC repair. The response JSON indicates urgency as high and sentiment as neutral.
You can see a JSON output.

Ensure that the JSON formatting is correct: This step is needed when the JSON output format is not clear in the previous prompt. You can make it clearer using the following code:
Python

Copy code

Switch to dark mode
prompt_4 = """Giving the following message:
---
{{?input}}
---
Extract and return a json with the follwoing keys and values:
- "urgency" as one of {{?urgency}}
- "sentiment" as one of {{?sentiment}}
Your complete message should be a valid json string that can be read directly and only contain the keys mentioned in the list above. Never enclose it in ```json...```, no newlines, no unnessacary whitespaces."""
f_4 = partial(send_request, prompt=prompt_4, **option_lists)

response = f_4(input=mail["message"])
This code defines a prompt template called "prompt_4" that specifies instructions for extracting "urgency" and "sentiment" from an input message and returning it as a JSON string. This prompt gives clear instruction to provide a clean format without any quotes or whitespaces.

You can see the following output:

A prompt asking to extract urgency and sentiment from a message about an urgent HVAC system repair. The response should be in JSON format, indicating high urgency and neutral sentiment.
You can see that the JSON output is clear now. This is ready for software consumption.

Simple categories based on business functions: We have associated urgency and sentiment to a mail. However, we also need more tags for each message to categorize them for business needs. We can start with a simple code.
Python

Copy code

Switch to dark mode
prompt_5 = """Giving the following message:
---
{{?input}}
---
Assign a list of matching support category to the message.
"""
f_5 = partial(send_request, prompt=prompt_5)

response = f_5(input=mail["message"])
This code helps categorize customer support messages by matching them to relevant support categories. It creates a prompt template with the message text, then uses a function, "f_5", to send the formatted request to a categorization service.

The following screenshot shows the output of this code.

A prompt asking to assign support categories to a message about an urgent HVAC repair. The response suggests three categories: Facilities/Maintenance, Urgent/Emergency, and HVAC/Heating and Cooling.
You can see that categories are assigned to the message. The response assigns the support categories to the original message, streamlining customer support processes.

Assigning values to categories from a list: The categories generated above have overlapping values, such as urgency, and should be aligned with company-defined values to address business needs effectively.
Python

Copy code

Switch to dark mode
prompt_6 = """Giving the following message:
---
{{?input}}
---
Assign a list best matching support category tags to the message:
{{?categories}}
"""
f_6 = partial(send_request, prompt=prompt_6, **option_lists)

response = f_6(input=mail["message"])
This code assigns appropriate support category tags to the email message. When executed with the specific input message, it sends the request, automating the tagging process to improve support efficiency.

The following screenshot shows the output of this code.

A prompt asking to assign support category tags to an urgent HVAC repair request. The response suggests emergency_repair_services, facility_management_issues, and routine_maintenance_requests.
You can see that categories are assigned to values from the list. These are streamlined for business processing.

Generate JSON output for categories values: Similar to step 3 earlier, we need JSON output for processing these values in a software. We also ensure that the format of JSON output is without any unnecessary symbols or space.
We use the following code:

Python

Copy code

Switch to dark mode
prompt_7 = """Giving the following message:
---
{{?input}}
---
Extract and return a json with the follwoing keys and values:
- "categories" list of the best matching support category tags from: {{?categories}}
Your complete message should be a valid json string that can be read directly and only contain the keys mentioned in the list above. Never enclose it in ```json...```, no newlines, no unnessacary whitespaces.
"""
f_7 = partial(send_request, prompt=prompt_7, **option_lists)

response = f_7(input=mail["message"])
This code creates a precise prompt template to extract specific category tags from an input message and return them in a valid JSON format. It then uses a predefined function, "send_request", with the constructed prompt and some options to generate the necessary response from the given input message. This ensures consistency and accuracy in data extraction.

The following screenshot shows the output of this code.

A prompt asking to extract support categories from a message about an urgent HVAC repair. The response is a JSON object with categories: emergency_repair_services, facility_management_issues, and routine_maintenance_requests.
You can see categories in JSON output that can be processed in a software.

Combining all the steps: We have used generative-ai-hub-sdk to arrive at proper values of urgency, sentiment, and categories in JSON format step-by-step. Now, let's combine all these steps into a single prompt.
Python

Copy code

Switch to dark mode
prompt_8 = """Giving the following message:
---
{{?input}}
---
Extract and return a json with the follwoing keys and values:
- "urgency" as one of {{?urgency}}
- "sentiment" as one of {{?sentiment}}
- "categories" list of the best matching support category tags from: {{?categories}}
Your complete message should be a valid json string that can be read directly and only contain the keys mentioned in the list above. Never enclose it in ```json...```, no newlines, no unnessacary whitespaces.
"""
f_8 = partial(send_request, prompt=prompt_8, **option_lists)

response = f_8(input=mail["message"])
The code combines all the previous steps in one prompt. It helps processing an email message to identify and return key information. By using a predefined template, it extracts details like urgency, sentiment, and categories, then formats them as a JSON string. It ensures that the output is clean and directly usable by invoking a function with the given parameters.

The following screenshot shows the output of this code.

A prompt asking to extract urgency, sentiment, and categories from a message about an urgent HVAC repair. The response JSON indicates high urgency, neutral sentiment, and relevant categories.ncy, neutral sentiment, and categories including emergency repair services, facility management issues, and routine maintenance requests.
You can see the consolidated output that assigns urgency, sentiment, and categories to customer messages that can be used in software.

You have successfully developed a prompt to move towards a solution of the problem using generative-ai-hub-sdk.

In the next lesson, we'll learn to use generative-ai-hub-sdk to evaluate the response from this prompt.

Using Generative-AI-hub-SDK to Evaluate Prompts
Objective

After completing this lesson, you will be able to evaluate prompts for a larger data set using functions in sdk
Need to Evaluate Prompts Using Generative-AI-hub-SDK
You have developed a basic prompt that assigns urgency, sentiment, and categories to customer messages in JSON format. However, the management wants accurate answers because Facility solutions company will use these outputs in building custom applications that will directly impact customer experience.

You need automated and consistent evaluation of prompts across various scenarios, ensuring reliability and efficiency.

For this you need to create custom evaluation functions using generative-ai-hub-sdk.

Using an SDK for evaluating prompts offers the following benefits:

Reliable Testing: SDKs automate testing of prompts across various scenarios, ensuring consistent and efficient results.
Measuring Performance: They provide objective metrics such as relevance, coherence, and fluency to quantitatively assess response quality.
Tailored Evaluations: SDKs allow you to create custom evaluators that meet specific needs, enabling more precise and relevant assessments.
Scalable Results: They support large-scale evaluations, making it easier to test prompts on extensive datasets.
Implementing Evaluation Functions
We start the evaluation with the import of packages.
Python

Copy code

Switch to dark mode
from tqdm.auto import tqdm
import time


class RateLimitedIterator:
    def __init__(self, iterable, max_iterations_per_minute):
        self._iterable = iter(iterable)
        self._max_iterations_per_minute = max_iterations_per_minute
        self._min_interval = 1.0 / (max_iterations_per_minute / 60.)
        self._last_yield_time = None

    def __iter__(self):
        return self

    def __next__(self):
        current_time = time.time()

        if self._last_yield_time is not None:
            elapsed_time = current_time - self._last_yield_time
            if elapsed_time < self._min_interval:
                time.sleep(self._min_interval - elapsed_time)

        self._last_yield_time = time.time()
        return next(self._iterable)
The code defines a "RateLimitedIterator" class to control the rate at which you can iterate over an iterable. By specifying a maximum number of iterations per minute, it ensures that the iteration process adheres to a defined speed, preventing hitting rate limits. It uses the "tqdm" library for progress visualization and the "time" module for timing control.

Next we define an evaluation function.
Python

Copy code

Switch to dark mode
def evaluation(mail: Dict[str, str], extract_func: Callable, _print=True, **kwargs):
    response = extract_func(input=mail["message"], _print=_print, **kwargs)
    result = {
        "is_valid_json": False,
        "correct_categories": False,
        "correct_sentiment": False,
        "correct_urgency": False,
    }
    try:
        pred = json.loads(response)
    except json.JSONDecodeError:
        result["is_valid_json"] = False
    else:
        result["is_valid_json"] = True
        result["correct_categories"] = 1 - (len(set(mail["ground_truth"]["categories"]) ^ set(pred["categories"])) / len(categories))
        result["correct_sentiment"] = pred["sentiment"] == mail["ground_truth"]["sentiment"]
        result["correct_urgency"] = pred["urgency"] == mail["ground_truth"]["urgency"]
    return result
evaluation(mail, f_8)
This code evaluates the predictions made by a function processing an email message. It uses a provided extraction function to analyze the email's content and compares the results against predefined ground truth data, checking for valid JSON, correct categories, sentiment, and urgency. This ensures that the extraction function performs accurately and consistently.

The last sentence evaluates the combined prompt function that we developed in the previous lesson.

You can get the following outputs:

A prompt asking for a JSON response based on an urgent HVAC repair request email. The response JSON includes urgency, sentiment, and categories. The correct response is also shown.
You can see that the evaluation shows that all predictions are correct except sentiment.

We'll implement an evaluation function for a large number of mails to support large-scale evaluations, making it easier to test prompts on extensive datasets.
Python

Copy code

Switch to dark mode
from tqdm.auto import tqdm

def transpose_list_of_dicts(list_of_dicts):
    keys = list_of_dicts[0].keys()
    transposed_dict = {key: [] for key in keys}
    for d in list_of_dicts:
        for key, value in d.items():
            transposed_dict[key].append(value)
    return transposed_dict

def evalulation_full_dataset(dataset, func, rate_limit=100, _print=False, **kwargs):
    results = [evaluation(mail, func, _print=_print, **kwargs) for mail in tqdm(RateLimitedIterator(dataset, rate_limit), total=len(dataset))]
    results = transpose_list_of_dicts(results)
    n = len(dataset)
    for k, v in results.items():
        results[k] = sum(v) / len(dataset)
    return results


def pretty_print_table(data):
    # Get all row names (outer dict keys)
    row_names = list(data.keys())

    # Get all column names (inner dict keys)
    if row_names:
        column_names = list(data[row_names[0]].keys())
    else:
        column_names = []

    # Calculate column widths
    column_widths = [max(len(str(column_name)), max(len(f"{data[row][column_name]:.2f}") for row in row_names)) for column_name in column_names]
    row_name_width = max(len(str(row_name)) for row_name in row_names)

    # Print header
    header = f"{'':>{row_name_width}} " + " ".join([f"{column_name:>{width}}" for column_name, width in zip(column_names, column_widths)])
    print(header)
    print("=" * len(header))

    # Print rows
    for row_name in row_names:
        row = f"{row_name:>{row_name_width}} " + " ".join([f"{data[row_name][column_name]:>{width}.1%}" for column_name, width in zip(column_names, column_widths)])
        print(row)

overall_result = {}
This code now performs evaluation on the entire dataset, evaluates each entry through a function with rate limiting, transposes the results for better aggregation, and then pretty-prints the final evaluation metrics in a tabular format. It uses the "tqdm" library to show a progress bar, making it easier to track processing status. The entire flow ensures streamlined, efficient processing and clear presentation of results.

Implement the final function to the final combined function.
Python

Copy code

Switch to dark mode
overall_result["basic--llama3-70b"] = evalulation_full_dataset(test_set_small, f_8)
pretty_print_table(overall_result)
You can get the following output:

A progress bar shows 100% completion of 20 tasks in 31 minutes. Below, a table displays metrics for basic--llama3-70b: 100% for is_valid_json, 85% for correct_categories, 25% for correct_sentiment, and 65% for correct_urgency.
You can see the results for the basic prompt. The output varies significantly when you rerun the code with different input examples. This sets the baseline for further improvement in prompt accuracy and relevancy.

The key take away till now in this learning journey is that we can create a basic prompt and then evaluate the prompt on the dataset to set a baseline for further enhancement.

Describing Techniques for Refining Prompts
Objective

After completing this lesson, you will be able to explain advanced prompting methods
Need for Refining Prompts
Continuing with the scenario discussed previously, we created basic prompts that assign urgency, sentiment, and categories to customer messages that can be used in software.

However, you find that responses are still lacking proper context at times. You need to refine prompts to achieve better results.

The evaluation results also indicate a scope for improvement in the prompt.

Refining prompts in generative AI is important for achieving better, more specific results. In generative AI hub, prompts are used to guide the model's responses; hence, improving them is crucial to successfully achieve the intended purpose of the model.

Here's why refining prompts is important:

Higher specificity: Carefully crafted prompts can help focus the AI's outputs on a specific problem or question, rather than a broad range of topics.

Improved accuracy: Better prompts can help to increase both the precision and relevance of the responses given by the AI model.

Advanced user experience: When the prompts are attuned to give highly specific and accurate responses, users get a much richer and satisfying interaction with the AI system.

Adapting various scenarios: Different scenarios may require different types of responses. By refining prompts, users can customize the AI's responses to fit various use cases, from plain responses to specific queries to detailed analysis of text and classification of text.

So, the better your prompts, the better the AI system will function in terms of delivering precise and useful responses, which in turn will enhance the overall user experience.

Advanced Prompting Methods
Advanced prompting techniques enhance the performance and reliability of LLMs by providing structured and context-rich prompts.

One-Shot Prompting
One-shot prompting involves guiding the model with a single example to generate a response. This technique is useful for demonstrating tasks with minimal examples and allows the model to produce relevant responses based on the provided example.

Few-Shot Prompting
Few-shot prompting builds on one-shot prompting by providing multiple examples to guide the model. This approach is particularly effective for complex tasks, as it helps the model understand nuances and generate more accurate responses.

Metaprompting
Metaprompting involves crafting or refining prompts to effectively guide AI model responses. This technique uses a prompt to generate or complete subsequent prompts, streamlining the process. For instance, we employ a metaprompt to produce a guide for classification tasks based on an example. The goal is to let the LLM instruct itself and condense the information from multiple examples into concise instructions.

Often, combining multiple techniques can also lead to better results.

We'll implement these techniques in the Facility solutions scenario in the next lesson.

Implementing Advanced Prompt Engineering Techniques
Objective

After completing this lesson, you will be able to design a systematic approach to develop and evaluate prompt engineering from a simple baseline
Few-shot Prompting
Let's implement promoting techniques and then evaluate the results to see improvement in the prompt results.

We use the following code:

Python

Copy code

Switch to dark mode

prompt_10 = """Your task is to extract and categorize messages. Here are some example:
---
{{?few_shot_examples}}
---
Use the examples when extract and categorize the following message:
---
{{?input}}
---
Extract and return a json with the follwoing keys and values:
- "urgency" as one of {{?urgency}}
- "sentiment" as one of {{?sentiment}}
- "categories" list of the best matching support category tags from: {{?categories}}
Your complete message should be a valid json string that can be read directly and only contain the keys mentioned in the list above. Never enclose it in ```json...```, no newlines, no unnessacary whitespaces.
"""

import random
random.seed(42)

k = 3
examples = random.sample(dev_set, k)

example_template = """<example>
{example_input}

## Output

{example_output}
</example>"""

examples = '\n---\n'.join([example_template.format(example_input=example["message"], example_output=json.dumps(example["ground_truth"])) for example in examples])


f_10 = partial(send_request, prompt=prompt_10, few_shot_examples=examples, **option_lists)

response = f_10(input=mail["message"])
The code aims to create a prompt template to extract and categorize messages according to their urgency, sentiment, and support category tags. By using randomly selected examples from a development set, it generates a formatted few-shot learning prompt. The prompt is sent to a language model to process and categorize a given input message, and the overall performance of the model is then evaluated and displayed in a table format.

Here’s an expanded explanation for a few parts of the code:

Setting the Random Seed: It sets a random seed using "random.seed(42)" to ensure that the random sampling of the examples is reproducible. This helps in maintaining consistency in experiments and evaluations.
Sampling Examples: The variable "k" is set to 3, indicating the number of examples to sample from the "dev_set" dataset. The "random.sample(dev_set, k)" function selects three random examples from the development set.
Formatting Examples: The selected examples are formatted into a template "example_template". Each example includes the input message and the expected output in JSON format. This formatted string is then joined using "\n---\n" to create a cohesive set of examples.
Partial Function Application: The "partial" function is used to bind the generated prompt and examples to the "send_request" function, creating a function "f_10" that can be called with just the input message. This streamlines the process of sending requests to the model with the necessary context.
Sending Request and Evaluating: The script sends the request using "f_10(input=mail["message"])" with the input message from "mail["message"]". The result is stored and evaluated against a small test dataset "test_set_small". The evaluation results are stored in "overall_result["few_shot--llama3-70b"]".
Output Display: Finally, the "pretty_print_table(overall_result)" function is used to display the evaluation results in a formatted table, making it easier to interpret the results.
You can get the following output prompts:

A prompt for extracting and categorizing messages, along with an example message and its categorized output. The example message is an inquiry about training programs for an in-house maintenance team. The output categorizes the message under general_inquiries and training_and_support_requests, with a neutral sentiment and low urgency.
You can see an example prompt here:

An email template and its corresponding output in JSON format. The email is a request for help with training programs from a support team. The JSON output categorizes the request and indicates its sentiment and urgency.
This is another prompt example here:

An email requesting urgent HVAC system repair, highlighting the issue's impact on comfort and daily activities, and asking for immediate professional intervention.
You can see another example prompt and the response here.

A progress bar and a table with evaluation metrics for two models.
This is the output for evaluation after implementing few-shot prompting.

You can see improvement in sentiment and urgency assignment.

We established a baseline earlier, and now we can evaluate and compare the results of the refined prompts with the baseline using the test data.

Metaprompting
Here we'll implement metaprompting to create detailed guides for prompts for various tags like urgency, sentiments, and so on.

We use the following code:

Python

Copy code

Switch to dark mode

example_template_metaprompt = """<example>
{example_input}

## Output
{key}={example_output}
</example>"""

prompt_get_guide = """Here are some example:
---
{{?examples}}
---
Use the examples above to come up with a guide on how to distinguish between {{?options}} {{?key}}.
Use the following format:
```
### **<category 1>**
- <instruction 1>
- <instruction 2>
- <instruction 3>
### **<category 2>**
- <instruction 1>
- <instruction 2>
- <instruction 3>
...
```
When creating the guide:
- make it step-by-step instructions
- Consider than some labels in the examples might be in correct
- Avoid including explicit information from the examples in the guide
The guide has to cover: {{?options}}
"""

guides = {}

for i, key in enumerate(["categories", "urgency", "sentiment"]):
    options = option_lists[key]
    selected_examples_txt_metaprompt = '\n---\n'.join([example_template_metaprompt.format(example_input=example["message"], key=key, example_output=example["ground_truth"][key]) for example in dev_set])
    guides[f"guide_{key}"] = send_request(prompt=prompt_get_guide, examples=selected_examples_txt_metaprompt, key=key, options=options, _print=False, _model='gpt-4o')
print(guides['guide_urgency'])
This code generates step-by-step guides for different categories—like "categories," "urgency," and "sentiment"—from labeled examples in a dataset.

It creates tailored guides for distinguishing between categories, urgency, and sentiment in text data. It formats examples using a specific template, then sends these examples to a model for generating step-by-step instructions. The guides help users distinguish between these categories based on patterns in the provided examples.

Here's a more detailed explanation:

Template Definitions:

"example_template_metaprompt": Defines a template to format examples, specifying how to structure input and output within an example.
"prompt_get_guide": Outlines a prompt format to request the generation of a guide based on formatted examples. It also specifies the format and requirements for the guide, including making it a step-by-step instruction, accounting for possible incorrect labels, and avoiding explicit replication of the examples.
Guide Preparation:

The script iterates over three keys: "categories", "urgency", and "sentiment".
For each key, it retrieves relevant options from "option_lists".
Example Selection and Formatting: It formats examples from "dev_set" using the predefined template for each key, embedding the input message and corresponding ground truth.

Guide Generation:

It sends a formatted prompt along with the examples to a model (gpt-4o), requesting the generation of a guide for distinguishing between the specified options for each key.
It stores the generated guides in a dictionary (guides), with each guide associated with its respective key (for example, "guide_categories", "guide_urgency", "guide_sentiment").
This process ensures that comprehensive and accurate instruction guides are generated for different classification tasks, facilitating the correct categorization of text data.

The last line of the code prints the guide for urgency.

You can see the following output:

Contains guidelines for determining the urgency of issues, categorized into High Urgency, Medium Urgency, and Low Urgency, with specific criteria for each level.
You can see the guide describing three rules for each urgency category that can be used in a prompt.

We use the following code to utilize these guides in a prompt.

Python

Copy code

Switch to dark mode
prompt_12 = """Your task is to classify messages.
This is an explanation of `urgency` labels:
---
{{?guide_urgency}}
---
This is an explanation of `sentiment` labels:
---
{{?guide_sentiment}}
---
This is an explanation of `support` categories:
---
{{?guide_categories}}
---
Giving the following message:
---
{{?input}}
---
Extract and return a json with the following keys and values:
- "urgency" as one of {{?urgency}}
- "sentiment" as one of {{?sentiment}}
- "categories" list of the best matching support category tags from: {{?categories}}
Your complete message should be a valid json string that can be read directly and only contain the keys mentioned in the list above. Never enclose it in ```json...```, no newlines, no unnecessary whitespaces.
"""
f_12 = partial(send_request, prompt=prompt_12, **option_lists, **guides)
response = f_12(input=mail["message"])
The code prepares a prompt for classifying messages based on urgency, sentiment, and support categories by utilizing predefined guides generated through the metaprompt code. It then uses a partial function to send this prompt as a request with specific options and guides. Finally, it processes an email message to extract and return these classifications in a JSON format.

See the following video to see the output.

Let’s evaluate this prompt and its response using the following code:

Python

Copy code

Switch to dark mode

overall_result["metaprompting--llama3-70b"] = evalulation_full_dataset(test_set_small, f_12)
pretty_print_table(overall_result)
You can get the following output:

A table with evaluation metrics for three different models
Now, we see that accuracy for urgency is improved, however accuracy for other categories is similar or even worse in the case of sentiment.

Combining Metaprompting and Few-shot Prompting
We can combine metapromting and few-shot prompting using the following code:

Python

Copy code

Switch to dark mode

prompt_13 = """Your task is to classify messages.
Here are some examples:
---
{{?few_shot_examples}}
---
This is an explanation of `urgency` labels:
---
{{?guide_urgency}}
---
This is an explanation of `sentiment` labels:
---
{{?guide_sentiment}}
---
This is an explanation of `support` categories:
---
{{?guide_categories}}
---
Giving the following message:
---
{{?input}}
---
extract and return a json with the following keys and values:
- "urgency" as one of {{?urgency}}
- "sentiment" as one of {{?sentiment}}
- "categories" list of the best matching support category tags from: {{?categories}}
Your complete message should be a valid json string that can be read directly and only contain the keys mentioned in the list above. Never enclose it in ```json...```, no newlines, no unnecessary whitespaces.
"""

f_13 = partial(send_request, prompt=prompt_13, **option_lists, few_shot_examples=examples, **guides)

response = f_13(input=mail["message"])
This Python code creates a template prompt for a message classification task, specifying how to extract and return information about urgency, sentiment, and support categories in a JSON format. The code uses this prompt to configure a function, "f_13", to analyze a given input message and generate a structured JSON response. This ensures consistent and accurate message classification.

You can see that it's combining few examples with guides generate during metaprompting.

See the following video to see the output.

Let’s evaluate this prompt and its response using the following code:

Python

Copy code

Switch to dark mode

overall_result["metaprompting_and_few_shot--llama3-70b"] = evalulation_full_dataset(test_set_small, f_13)
pretty_print_table(overall_result)
You can get the following output:

A performance evaluation table for different models.
Now, we see that accuracy for almost all categories except urgency is improved. This prompt has good accuracy. However, it's a more expensive prompt needing more resources.

Note

You may get a slightly different response to the one shown here and in all the remaining responses of models shown in this learning journey.

When you execute the same prompt in your machine, an LLM produces varying outputs due to its probabilistic nature, temperature setting, and nondeterministic architecture, leading to different responses even with slight setting changes or internal state shifts.

Evaluation Summary
We need to consider the overall accuracy and quality of a model along with its cost and scale.

At times, smaller models and simpler techniques may give better results.

In the preceding output, we can see that few-shot gives optimal performance with less expensive prompt.

Let's recap what we have done to solve the business problem so far:

We created a basic prompt in SAP AI Launchpad using an open-source model.
We recreated the prompt using generative-ai-hub-sdk to scale the solution.
We created a baseline evaluation method for the simple prompt.
Finally, we used techniques like few shot and metaprompting to further enhance the prompts.
The results show improvement in the quality of prompt responses after implementing advanced techniques.
We'll study the costs associated with these techniques using other models in the next unit.



Exploring Different Large Language Models
Objectives

After completing this lesson, you will be able to:
Describe different models in generative AI hub
Use different models in generative AI hub
Solution Using Different Models
Continuing with the scenario discussed previously, we created basic prompts that assign urgency, sentiment, and categories to customer messages that can be used in software.

We used advanced prompting techniques to arrive at better prompts. We evaluated techniques and their combinations to analyze the results.

Let's now evaluate the results for different models.

Here are some reasons to use different models to solve a business problem:

Specialization: Different LLMs are often trained on varied datasets and optimized for specific tasks. For example, some models excel at text generation, while others specialize in image or speech recognition.
Performance: Combining multiple LLMs can enhance overall performance. One model handles natural language understanding, while another generates high-quality responses, creating a more effective team.
Cost Efficiency: Allocating specialized models to specific tasks can be more cost-effective than relying on a single, general model. This approach optimizes resource allocation and reduces waste.
Flexibility: Integrating different LLMs enables handling multimodal inputs and outputs, such as text, images, and audio, providing a more comprehensive solution.
Redundancy and Reliability: Having multiple models in place ensures that if one model fails or underperforms, others can step in, leading to more reliable outcomes and minimizing downtime.
Generative AI Hub Benefits
Generative AI hub provides access to various models in one place in a reliable and responsible manner for accelerating your innovation journey.

Generative AI hub provides the following benefits:

Flexible access to broad range of models: Accelerate AI development with flexible access to a broad range of models and compute capacity. Our extensive set of frontier AI models, infrastructure, and tooling helps you move faster.
Custom AI solutions: Build custom AI solutions and extend SAP applications by combining AI models with your unique data, creating powerful results.
Responsible AI development: Take control of your AI lifecycle and safeguard your data with SAP's trusted privacy and security policies, ensuring responsible AI development through centralized orchestration.
Flexible access to the broadest range of models and compute capacity implies that generative AI hub can help in the following ways:

Tailor: Leverage various model options from all leading providers in one platform to tailor AI to your needs.
Connect: Easily connect to any supported foundation model or bring your own model, giving you flexibility and control.
Switch: Switch seamlessly between models to find and upgrade to the best-suited technology for your needs, ensuring optimal results.
Optimize: Eliminate individual contracts and lock-in, ultimately boosting the ROI of your AI projects.
Deploy: Deploy your custom AI solutions to any cloud infrastructure, freeing you from lock-in and giving you greater flexibility.
Deliver: Simplify access, deployment, and management of the latest AI advancements to accelerate development.
Generative AI Hub Models’ Access
We have seen how different models can be configured in generative AI hub. We also saw how orchestration services and workflow can help in configuring different models using minimum code.

Let's summarize these benefits here:

You can configure models using the Configuration tab in SAP AI Launchpad. You can refer to the configuration steps in Unit 1. Some configured models are shown in the following screenshot. SAP AI Launchpad interface, specifically the Configurations section under ML Operations. It lists various AI models, their versions, and sources like azure-openai and gcp-vertexai.
You can see various foundation models from many vendors. You can also see various scenarios, such as aicore-opensource, gcp-vertexai, and azure-openai. You will learn more about scenarios and models in the next topic.

You can deploy models using the Deployments tab in SAP AI Launchpad. You can refer to the configuration steps in Unit 1 Some deployed models are shown in the following screenshot.SAP AI Launchpad interface showing the All Deployments section. Multiple AI models are listed, all in the RUNNING state, with details such as names, IDs, and timestamps.
You have also seen how to use orchestration to use a common code for different models. Refer to "send_request" function details in the helper functions in Unit 2 Lesson 3.
Generative AI Hub Models Usage
SAP AI Core manages the global AI scenario foundation-models, which grant access to generative AI models. We offer individual models as executable serving templates, allowing users to select the desired model by choosing the corresponding template.

The available executables in foundation-models global scenario include:

Azure OpenAI Service: Use the REST API to access OpenAI's powerful LLMs. This includes models such as gpt-4o, gpt-35, gpt-35-turbo, and others.
SAP AI Core: Work with open-source models hosted and accessed directly through SAP AI Core. This includes models such as mistralai--mixtral-8x7b-instruct-v01 and meta--llama3-70b-instruct.
GCP Vertex AI: Get access to Google's advanced PaLM 2 and Gemini models. This also includes models such gemini-1.5-pro and textembedding-gecko.
AWS Bedrock: Explore and use Foundation models from leading providers like Anthropic, Amazon, and more. This includes models such as anthropic--claude-3.5-sonnet and amazon--titan-embed-text.
Orchestration services are provided under the global AI scenario orchestration, which is managed by SAP AI Core. It allows the use of different generative AI models with a common code, configuration, and deployment.

You can learn more about the models and versions available within generative AI hub along other details, such as their data center, and when they will be outdated, through the SAP Note: 3437766 - Availability of Generative AI Models. It also explains the generative AI Token conversion rates for each model and gives information on rate limits.

See the following video to know how to use different models in SAP AI Launchpad for the Facility Solutions Company.

You can see results using different models. In the next lesson, we'll evaluate the results from different models.

Selecting the Suitable LLM
Objective

After completing this lesson, you will be able to evaluate different models using generative-ai-hub-sdk
Different Models in generative-AI-hub Code
Let's now evaluate different models for Facility solutions problem that we're solving.

mistralai--mixtral-8x7b-instruct-v01
We begin with mistralai--mixtral-8x7b-instruct-v01 and use the basic prompt. This model is an example of the cheapest open source, SAP hosted models available on generative AI hub.

Python

Copy code

Switch to dark mode
overall_result["basic--mixtral-8x7b"] = evalulation_full_dataset(test_set_small, f_8, _model='mistralai--mixtral-8x7b-instruct-v01')
pretty_print_table(overall_result) 
This code evaluates a dataset and prints the results. It calculates a specific model's performance on a small test set, storing the results under a key in the "overall_result" dictionary. The "pretty_print_table" function then formats and prints these results, making the evaluation data clear and easy to read.

You can have the following output and can see the results.

A table with performance metrics for different models. Metrics include valid JSON, correct categories, sentiment, and urgency. Models listed are llama3-70b, mixtral-8x7b, and few-shot variations.
Similarly, let's evaluate results using a combination of few-shot and metaprompting for the same model.

Python

Copy code

Switch to dark mode
overall_result["metaprompting_and_few_shot--mixtral-8x7b"] = evalulation_full_dataset(test_set_small, f_13, _model='mistralai--mixtral-8x7b-instruct-v01')
pretty_print_table(overall_result) 
You can have the following output:

A table with evaluation results for different models and prompting techniques. Metrics include valid JSON, correct categories, sentiment, and urgency, with scores for each model configuration.
You can see the evaluation results.

gpt-4o
We perform similar steps with gpt-4o. This model is an example of the best proprietary OpenAI models available on generative AI hub.

Python

Copy code

Switch to dark mode
overall_result["basic--gpt4o"] = evalulation_full_dataset(test_set_small, f_8, _model='gpt-4o')
pretty_print_table(overall_result)
A table with performance metrics for different models and configurations. Metrics include JSON validity, correct categories, sentiment, and urgency. Models include llama3, mixtral, and gpt4o.
You can see results for these outputs.

Similarly, let's evaluate results using a combination of few-shot and metaprompting for the same model.

Python

Copy code

Switch to dark mode
overall_result["metaprompting_and_few_shot--gpt4o"] = evalulation_full_dataset(test_set_small, f_13, _model='gpt-4o')
pretty_print_table(overall_result)
You can have the following output.

A table comparing different models (llama3-70b, mixtral-8x7b, gpt4o) on metrics: valid JSON, correct categories, sentiment, and urgency. All models have 100% valid JSON, with varying other scores.
You can see the evaluation results.

gemini-1.5-flash
We perform similar steps with gemini-1.5-flash. This model is the cheapest and fastest Google model available on generative AI hub.

Python

Copy code

Switch to dark mode
overall_result["basic--gemini-1.5-flash"] = evalulation_full_dataset(test_set_small, f_8, _model='gemini-1.5-flash')
pretty_print_table(overall_result)
You can have the following output:

A table of model performance metrics, including is_valid_json, correct_categories, correct_sentiment, and correct_urgency, for various models like llama3-70b, mixtral-8x7b, and gpt4o.
You can see results for these outputs.

Similarly, let's evaluate results using a combination of few-shot and metaprompting for the same model.

Python

Copy code

Switch to dark mode
overall_result["metaprompting_and_few_shot--gemini-1.5-flash"] = evalulation_full_dataset(test_set_small, f_13, _model='gemini-1.5-flash')
pretty_print_table(overall_result)
You can have the following output:

A table of model performance metrics, including is_valid_json, correct_categories, correct_sentiment, and correct_urgency, for various models like llama3-70b, mixtrxl-8x7b, and gpt4.
You can see the evaluation results.

Note

You may get a slightly different response to the one shown here and in all the remaining responses of models shown in this learning journey.

When you execute the same prompt in your machine, a LLM produces varying outputs due to its probabilistic nature, temperature setting, and non-deterministic architecture, leading to different responses even with slight setting changes or internal state shifts.

Evaluation Guidelines for Different Models
Pricing and rates play a crucial role in model selection within generative AI hub. Key considerations include:

Cost Efficiency: Smaller, more affordable models often provide excellent results at a lower cost. They offer a cost-effective solution that balances performance with budget constraints for many applications. Companies need to weigh the cost of using advanced models against the expected return on investment. Refer to SAP notes 3437766 - Availability of Generative AI Models and 3505347 - Orchestrationfor pricing details in generative AI hub.
Scalability: Subscription-based pricing models provide predictable costs and are easier to scale. This is where generative AI hub can support scalable AI development and deployment.
Performance vs. Cost: Although high-performing models come at a higher cost, smaller models can deliver comparable results for specific tasks. This makes them a viable option for organizations looking to optimize spending without compromising on quality. Organizations must evaluate whether the performance gains justify the additional expense by assessing the specific needs of the application and its business value.
Flexibility: Pricing models that allow for adjustments based on usage patterns help organizations optimize their spending. This is particularly important in dynamic environments where AI capability demands fluctuate.
Competitive Advantage: Strategic pricing can provide a competitive edge. Companies adopting innovative pricing strategies, such as outcome-based pricing, can attract more customers and drive higher adoption rates.
Businesses can make use of these considerations and take informed decisions about which generative AI models to deploy, achieving the best balance between cost and performance.

Evaluation Summary
We saw how we can use generative AI hub to solve a business problem and learned about features and options that generative AI hub offers to develop, deploy, and manage custom-built AI solutions.

Let's add to the recap what we have done to solve the business problem so far:

We created a basic prompt in SAP AI Launchpad using an open-source model.
We recreated the prompt using generative-ai-hub-sdk to scale the solution.
We created a baseline evaluation method for the simple prompt.
Finally, we used techniques like few-shot and metaprompting to further enhance the prompts.
We tested various models in generative AI hub using SAP AI Launchpad.
We evaluated various models for the problem using generative-ai-hub-sdk.
For example, in the Facility Solutions Company scenario, we can see through the evaluation that the few_shot--llama3-70b option gives a better result. Although some models give a better accuracy for certain fields, considering the cost, scalability, and performance the company can use the few-shot prompt using meta--llama3-70b-instruct to get a structured response in a format that can be used by other applications within the organization.

Facility solutions teams can leverage generative AI hub for categorizing customers mails and prioritize tasks to enhance customer service and overall experience.

With SAP’s generative AI hub, you can build your own solutions that enhance business applications with using LLMs programmatically.

...........................

4. Using Advanced AI Techniques with SAP’s Generative AI Hub/

...

Reviewing AI and Generative AI Hub Concepts
Objective

After completing this lesson, you will be able to recap fundamental concepts of AI and the generative AI hub to provide context for advanced techniques.
Introduction
In this lesson, we will revisit the essential concepts covered in the previous learning journey and prepare for the advanced techniques we will explore in this learning journey.

Quick Recap of the Topics
In the previous learning journey, you explored the fundamental concepts of Artificial Intelligence (AI) with a focus on SAP’s generative AI hub, equipping yourself with the knowledge to tackle advanced techniques and complex AI applications.

Understanding Generative AI and SAP AI Core
We began by exploring generative AI and SAP AI Core, focusing on the AI Foundation on SAP BTP. This foundation provides tools and services to tackle business challenges. The generative AI hub integrates AI capabilities into applications, enabling businesses to use AI for complex problem solving.

Leveraging the Power of LLMs Using SDK for Generative AI Hub
We learned to develop basic prompts for common queries and leveraged Large Language Models (LLMs) using the Software Development Kit (SDK). We explored the orchestration service, which manages complex AI workflows, including a harmonized API and streamlined code for accessing models. Evaluating prompts ensured their accuracy and relevance, especially for larger datasets.

Refining AI Responses Using Advanced Prompt Engineering Techniques
We delved into advanced prompt engineering techniques like few-shot prompting and metaprompting. These techniques improve the specificity, accuracy, and user experience of AI models by providing multiple examples and refining prompts.

Selecting Large Language Models in Generative AI Hub
We explored different LLMs available in the generative AI hub. We emphasized the importance of selecting the right model for specific tasks, balancing accuracy, performance, and cost-effectiveness.

Key Learnings Recap
AI Foundation on SAP BTP: The AI Foundation on SAP Business Technology Platform (BTP) provides a comprehensive set of tools and services designed to help businesses tackle various challenges. It includes capabilities for developing, deploying, and managing AI models, making it easier for organizations to integrate AI into their operations and drive innovation.
Generative AI hub: The generative AI hub is a pivotal component within SAP AI Core, playing a crucial role in integrating AI capabilities into different applications. It serves as a central platform for accessing and deploying (LLMs), enabling businesses to apply the power of generative AI to solve complex problems and enhance their operations.
Applications of Generative AI: Generative AI has diverse applications beyond traditional chatbots. It can be used to solve real-world problems in various business scenarios, such as automating customer service, generating content, and optimizing processes. Understanding these applications helps businesses identify opportunities to leverage generative AI for their specific needs.
Developing Prompts: Developing effective prompts is essential for leveraging the power of generative AI. This involves creating prompts that are concise, specific, and relevant to the business problem at hand. The process includes initial setup, creating prompts with assigned values, and refining them to improve accuracy and relevance.
Generative AI Hub SDK: The generative AI hub SDK provides practical tools for addressing business problems using LLMs. It includes examples of installing, configuring, and using the SDK, enabling businesses to develop and deploy AI solutions effectively. The SDK also supports advanced features like the orchestration service, which helps manage complex AI workflows.
Evaluating Prompts: Evaluating prompts is crucial for ensuring their accuracy and relevance, especially when dealing with larger datasets. This involves implementing evaluation functions and using practical examples to assess the performance of prompts. Effective evaluation helps businesses refine their AI solutions and achieve better results.
Advanced Prompt Engineering: Advanced prompt engineering techniques, such as few-shot prompting and metaprompting, are essential for improving the specificity, accuracy, and user experience of AI models. These techniques involve providing multiple examples to guide the model and crafting or refining prompts to generate more accurate and contextually relevant responses.
Selecting Models: Selecting the right LLM is critical for achieving the desired performance and cost-effectiveness. This involves evaluating different models available in the generative AI hub, comparing their performance, and considering factors like specialization, flexibility, and reliability. Choosing the right model helps businesses tailor their AI solutions to specific needs and achieve optimal results.
This comprehensive recap provided a solid foundation for understanding advanced techniques like grounding, preparing us to explore the complexities of advanced AI applications.

Explore Models in Generative AI Hub
The Model Library in the Generative AI Hub is an invaluable resource for selecting the right model. It provides comprehensive information on available models to aid in decision-making.

Catalog Mode: Explore all available models and their metadata. Use filters to refine your selection or search for a model by name. Each model's card offers detailed information, including data input types, cost, and metrics.

Leaderboard Mode: Access model scores across various benchmarks. Apply filters to narrow down your options, search by name, or reorder the list based on specific benchmarks. Deprecation notices are also available.

Chart Mode: View model scores in a chart format for easy comparison.

Model Cards: Navigate through tabs to see detailed information and the status of deployments containing the model. Refresh to get the latest updates.

See more details here.

Preparing for Advanced Techniques
In this learning journey, we will build on the foundational knowledge from the previous learning journey and delve into advanced AI techniques. The upcoming units and lessons will cover:

Orchestration: Exploring how the orchestration service can streamline AI workflows.
Vector Embeddings: Learning about vector embeddings and their applications in AI.
Embedding Models: Identifying embedded models to optimize AI responses.
Retrieval Augmented Generation (RAG): This technique involves using relevant documents to provide accurate, contextually appropriate responses. Document repositories, such as Microsoft SharePoint, are indexed, allowing the AI to retrieve and use specific data from these documents.
Document Grounding: Understanding and implementing document grounding to enhance AI applications. Grounding is also known as RAG, which is an important tool for specializing LLMs on domain knowledge without the need of retraining.
This lesson is a recap of the foundational concepts of AI and the generative AI hub. This knowledge is essential as we move forward in developing and refining advanced AI techniques for various business applications.

Identifying Business Scenarios for Advanced AI Techniques
Objective

After completing this lesson, you will be able to analyze and select business problems that can benefit from complex AI solutions.
Introduction
In this lesson, we will explore sophisticated AI methods to tackle business challenges.

We will identify business scenarios that can benefit from these techniques, ensuring seamless integration of AI components to enhance workflow efficiency, accuracy, and security.

We will also look at how SAP's generative AI hub enhances these solutions, improving efficiency, decision-making, and data security. By the end, you'll understand how these AI techniques can transform your business operations.

AI Solutions Using Advanced Techniques
In today's fast-paced business environment, applying advanced AI techniques can significantly enhance operational efficiency and decision-making.

AI solutions offer powerful tools to address complex business challenges across various industries, from manufacturing and healthcare to retail and finance.

By incorporating techniques such as grounding and contextual relevance, data masking, content filtering, prompt templating, harmonized API integration, and access to LLMs, businesses can enhance efficiency, improve decision-making, and ensure data security.

Here are several key areas where advanced AI techniques can make a substantial impact.

Improving Production Efficiency:
Manufacturing:
Problem: Inaccurate predictions and recommendations can lead to production delays and increased costs.
Solution: AI solutions use document grounding to add context to predictions, reducing errors and improving accuracy. This ensures better decision-making and efficient production processes.
Ensuring Data Privacy and Security:
Healthcare:
Problem: Exposure of patient data can lead to privacy breaches and non compliance with regulations.
Solution: Data masking and content filtering protect patient information while allowing AI models to analyze medical data securely, ensuring compliance with privacy regulations and improving patient care.
Enhancing Marketing Strategies:
Retail:
Problem: Inefficiency in generating and refining AI prompts can limit the effectiveness of marketing campaigns.
Solution: Prompt engineering allows safe and scalable experimentation, helping to refine prompts and improve AI model performance for better marketing strategies and personalized customer experiences.
Optimizing Operational Efficiency:
Utilities:
Problem: Difficulty in integrating various AI components and data sources can hinder operational efficiency.
Solution: Seamless integration with utility systems and data sources enables the development of custom AI solutions, enhancing operational efficiency and customer service.
Improving Financial Analysis:
Finance:
Problem: Limited access to diverse AI models restricts the ability to analyze financial data effectively.
Solution: Instant access to various LLMs allows financial institutions to choose the best model for each task, speeding up AI development and improving financial analysis and decision-making.
Streamlining Logistics Planning:
Logistics:
Problem: Complex AI solution development processes can delay logistics planning and execution.
Solution: Streamlined workflows and simple SDKs facilitate faster AI solution development, reducing time-to-market and enhancing logistics planning and execution.
Value of SAP's Generative AI Hub for Implementing Advanced AI Techniques
SAP's generative AI hub is a game-changer for businesses looking to integrate advanced AI capabilities into their operations. This platform not only facilitates the deployment of LLMs but also enhances document retrieval processes through document grounding. By providing practical tools and the orchestration service, the generative AI hub streamlines AI solution development and deployment.

The generative AI hub offers significant value for implementing advanced techniques such as grounding, data masking, content filtering, prompt templating, and harmonized API. Here are the key benefits:

Grounding and Contextual Relevance:
Problem: Inaccurate or irrelevant AI-generated responses.
Solution: The generative AI hub uses the document grounding service to incorporate context into AI responses, reducing hallucinations and errors, and improving adoption by end users.
Data Masking and Content Filtering:
Problem: Risk of exposing sensitive information during AI processing.
Solution: The hub provides tools for data masking (pseudonymization and anonymization) and content filtering, ensuring data privacy and security while enabling responsible AI development.
Prompt Templating and Engineering:
Problem: Inefficiency in generating and refining AI prompts.
Solution: Developers can experiment with prompt engineering in a self-contained playground, allowing safe and scalable innovation. The built-in prompt history and orchestration layer enhance control and transparency.
Harmonized API and Integration:
Problem: Difficulty in integrating various AI components and data sources.
Solution: The hub offers seamless integration with SAP applications and data sources, enabling the development of custom AI solutions and AI-powered extensions on the SAP Business Technology Platform (BTP).
Access to LLMs:
Problem: Limited access to diverse AI models for different tasks.
Solution: Instant access to a broad range of LLMs from different providers, such as GPT-4 by Azure OpenAI, allows for orchestration of multiple models, speeding up AI development and freeing up valuable developer time.
Streamlined Workflows and SDKs:
Problem: Complex and time-consuming AI solution development processes.
Solution: The hub supports streamlined workflows and simple SDKs in preferred programming languages, facilitating faster and easier AI solution development.
The value of the generative AI hub lies in its ability to simplify and enhance the use of these advanced AI techniques through several key features:

The Orchestration Service: The orchestration service streamlines the deployment and management of AI models by requiring only a single virtual deployment for access to all leading models. It reduces the complexity and effort needed to manage multiple models individually.
Low Code Approach: The hub supports a low code approach, making it accessible to users without deep technical expertise. This approach allows users to interact with AI core features more easily, facilitating broader adoption, and utilization of AI capabilities.
Advanced Techniques: The grounding module, an advanced AI technique, is packaged in an easy-to-use manner through the orchestration service. This enables users to apply complex AI functionalities without needing extensive technical knowledge or without writing huge code.
Identify the Facility Management Scenario
Facility Solutions Company is a premier provider of comprehensive facility management, maintenance, and cleaning services tailored for both residential and commercial properties. Their mission is to ensure that your environment remains safe, efficient, and impeccably maintained, allowing you to focus on your core activities without the hassle of facility upkeep.

Their target markets are luxury residential complexes, apartments, and individual homes. They also serve commercial properties like office buildings, retail spaces, industrial facilities, and educational institutions.

Business Problem:
Facility Solutions Company faces a significant challenge in managing the thousands of e-mails that they receive daily, which include customer requests, complaints, and other messages. The current process involves manually transferring data from e-mails to internal applications, categorizing, and then prioritizing these tasks. This manual process is not only time-consuming but also prone to errors, leading to delays in addressing customer needs.

Existing Solution:
In the previous learning journey, the company successfully addressed the problem of extracting relevant information from customer e-mails accurately. By creating basic prompts and refining them using prompt engineering techniques, they were able to query the generative AI for specific tasks and achieve better responses. This helped in categorizing and prioritizing customer requests and complaints more efficiently.

New Challenges and Solutions:
Despite these improvements, the company still faces several challenges that must be addressed to further streamline their e-mail management process.

One of the key challenges is ensuring that the extracted information from e-mails is grounded in a structured format for user-defined applications. This is crucial for accurate categorization and prioritization. For instance, when a customer reports a maintenance issue, the company must ground the details of the issue in their internal systems to ensure it is addressed promptly. By using document grounding techniques in the generative AI hub, they can achieve this by grounding the extracted information in SAP HANA vector databases.

Also, understanding the context and semantics of customer e-mails is crucial for making informed decisions. For instance, when a customer expresses dissatisfaction with a service, the company must understand the context to address the issue effectively. By using embedding models from the generative AI hub, the company can enhance the categorization and prioritization process. These models help in understanding the context and semantics of the e-mails, enabling the company to provide timely and appropriate responses to customer requests and complaints.

Finally, Facility management needs to ensure that the solutions using the generative AI hub follow ethical practices for deploying AI-driven solutions. The data stored for best decision-making must be anonymized, and filtered for harmful content, which ensures that sensitive information is masked, and inappropriate content is filtered out. This can lead to complex workflows. By using the orchestration service in the generative AI hub, the company can integrate diverse AI components, ensure data privacy, and improve accuracy through document grounding, and manage AI-driven business processes.

By addressing these challenges, Facility Solutions Company aims to further streamline their e-mail categorization and prioritization process, reduce manual effort, and improve the overall efficiency and accuracy of their facility management services.

We will see how the generative AI hub can implement these solutions in the next few units.

Conclusion
In this lesson, we saw how advanced AI techniques help businesses solve specific challenges and improve operations. SAP's generative AI hub further enhances these benefits by integrating powerful AI capabilities into business processes. Embracing these technologies can drive innovation, efficiency, and growth in your organization.

We expanded upon the scenario for Facility Solutions Company, by integrating vector embeddings and document grounding to enhance efficiency and accuracy.

After completing this lesson, you are ready to apply advanced AI techniques in developing and refining AI solutions for diverse business applications.

Exploring the Role of the Orchestration Service
Objective

After completing this lesson, you will be able to explain how the orchestration service can enhance AI workflows, connecting business logic with AI capabilities.
Introduction
AI is revolutionizing the way that we approach complex business processes. The generative AI hub is at the forefront of this transformation by integrating various AI components into a unified system. One of the key enablers of this seamless integration is the orchestration service.

In this lesson, we will explore the need for the orchestration service, and how this service enhances AI workflows within the generative AI hub, ensuring efficiency, accuracy, and security.

Need for Using the Orchestration Service in Generative AI Hub
The orchestration service is crucial for solving business problems, especially in the context of generative AI, as it provides a structured and efficient way to manage and integrate various AI components. Here are some key points highlighting the need for the orchestration service:

Coordination and Management
Problem: Coordinating and managing the deployment, integration, and interaction of multiple AI models and components within a system can be complex and inefficient.
Solution: The orchestration service helps in coordinating and managing these processes, ensuring that different AI models work together seamlessly, optimizing overall performance and efficiency.
Streamlining and Automating Processes
Problem: Manual interventions in the end-to-end lifecycle of AI applications can be time-consuming and prone to errors.
Solution: The orchestration service streamlines and automates processes such as: data flow management, model execution, and resource utilization, leading to faster and more reliable AI operations.
Grounding Capabilities
Problem: Creating accurate and impactful AI solutions often requires custom data connections, which can be cumbersome and resource intensive.
Solution: Orchestration modules include grounding capabilities that enrich AI requests with relevant business context, reducing the need for custom data connections and using existing business data.
Content Filtering and Data Masking
Problem: Maintaining data privacy and security in AI applications is challenging, especially when dealing with sensitive information.
Solution: The orchestration service provides modules for content filtering and data masking, ensuring that sensitive information is protected, and AI applications adhere to ethical standards.
Harmonized API
Problem: Managing multiple APIs for different orchestration modules can be complex and inefficient.
Solution: A harmonized API across all models simplifies the integration and management of various orchestration modules, making it easier for businesses to implement and manage AI solutions.
Modular Orchestration Framework
Problem: Designing powerful AI workflows can be challenging due to the need to connect diverse components like data pipelines, AI models, and prebuilt modules.
Solution: A modular orchestration framework allows businesses to design powerful AI workflows by connecting these diverse components, reducing maintenance efforts, and enabling businesses to focus on innovation.
Recap: Introduction to the Orchestration Service
The orchestration service refers to the coordination and management of various AI components within the generative AI hub. It ensures seamless integration and efficient operation of LLMs, automates workflows, manages dependencies, and optimizes resource utilization, enabling robust and scalable AI-driven solutions for complex business problems.

In the previous learning journey, Using generative-AI-hub-SDK to interact with the orchestration service we explored the essential role of the orchestration service within the SAP generative AI hub.

Now, we will explore the modules of orchestration and their implementation with an example.

Use the Orchestration Service for Enhancing Workflows
The orchestration service acts as the backbone of AI workflows, managing the flow of data and tasks between different AI components. In the context of the generative AI hub, the orchestration service provides a structured and efficient way to integrate and manage AI capabilities.

It discusses enhancing AI workflows using orchestration services. Key elements include AI workflow enhancement, key features like module integration and data privacy, ensuring accuracy and context via document grounding and contextual integration, and development simplification through uniform APIs and workflow efficiency. Each section is visually represented by icons.
Key Features of the Orchestration Service
Module Integration: The orchestration service facilitates the integration of diverse modules such as templating, LLMs, data masking, and content filtering. This modular approach allows for the creation of comprehensive and automated AI-driven workflows, enhancing the overall capabilities of AI solutions.

Content Filtering and Data Masking: To ensure data privacy and security, the orchestration service incorporates content filtering and data masking. Inbound and outbound content is filtered based on classification services or models, while personal data is masked before processing and optionally unmasked afterward. This ensures compliance with data protection regulations and maintains user trust.

Simplified Configuration: The orchestration service allows for easy configuration at the service request or deployment level. This flexibility enables businesses to customize workflows to meet their specific needs, simplifying the process of building, and managing AI workflows.

Enhancing Context and Accuracy through Document Grounding
Leveraging Document Grounding: Document grounding enriches AI responses by incorporating relevant contextual business data from specific documents. The orchestration service efficiently retrieves and uses this data, minimizing inaccuracies, and enhancing the relevance of AI outputs.

Contextual Data Integration: When AI queries are processed, the orchestration service ensures that responses are grounded in real, relevant data, improving the accuracy and trustworthiness of AI-generated content.

API Harmonization for Seamless Integration
Uniform API Access: The orchestration layer provides a harmonized API across all models, facilitating seamless integration and interoperability between different AI components. This uniform access simplifies development and deployment, reducing the complexity of managing multiple APIs.

Streamlined Development: Developers can use the harmonized API to create and deploy AI workflows more efficiently. This streamlined approach enables faster time-to-market for AI solutions, driving business innovation.

Applications and Benefits
The following video summarizes key features of the orchestration service in the generative AI hub.

We have seen the implementation of harmonized code in the previous learning journey. We used a common code to access various models.

In coming lessons and units, we will implement other features such as data masking, grounding, and so on.

Conclusion
The orchestration service in the generative AI hub plays a crucial role in enhancing AI workflows. By integrating diverse AI components, ensuring data privacy, and improving accuracy through document grounding, the orchestration service provides a robust framework for managing AI-driven business processes. As AI continues to evolve, the role of the orchestration service becomes increasingly important in delivering efficient, secure, and contextually relevant AI solutions.

Implementing the Orchestration Service to Enhance Workflows
Objective

After completing this lesson, you will be able to implement the orchestration service to enhance AI workflows to implement AI solutions more effectively.
Implementing the Orchestration Service to Enhance Workflows
Let’s see how we can use the orchestration service modules such as data masking and content filtering.

Before accessing the orchestration service, you must configure the service.

Setting up the Orchestration Service
The following video shows how to setup and consume orchestration service modules.

Use the Orchestration Service
Access to the Orchestration Service
Navigate to the landing page for the orchestration service through Generative AI Hub → Orchestration. SAP AI Launchpad displays the following workflow to which you can use to build your own orchestration pipeline.

Screenshot of a user interface showing the Orchestration workflow in progress within the SAP AI Core Administration Tool, with options like Grounding, Templating, Data Masking, Input Filtering, Model Configuration, and Output Filtering.

A basic orchestration scenario allows you to combine different modules into a pipeline that can be executable. Within the pipeline, the response from one module will be used as the input for the next module.

Edit the Orchestration Workflow
You can change the orchestration workflow by showing or hiding modules through the Edit button and toggling the switch for the modules required. You can only hide and show optional modules.

User interface of orchestration workflow editor with options for grounding, templating, data masking, input filtering, model configuration, and output filtering. Left sidebar has navigation options.

Note

The templating and model configuration modules are mandatory to build an orchestration pipeline.
Toggle JSON Orchestration Workflow
You can toggle the orchestration workflow to upload or download your own orchestration workflow in JSON format through the Workflow/JSON button. From there, you can upload your edited JSON file (max file limit: 200KB) or download the JSON file of the workflow orchestration with the Upload and Download buttons respectively.

A dashboard interface showing an Orchestration workflow configuration script in JSON format with edit, upload, and download options. The sidebar lists different workspace modules and settings.
An example of an orchestration workflow in JSON format is provided next.

JSON

Copy code

Switch to dark mode
{
    "module_configurations": {
        "llm_module_config": {
            "model_name": "gpt-35-turbo",
            "model_params": {},
            "model_version": "0613"
        },
        "templating_module_config": {
            "template": [],
            "defaults": {}
        },
        "filtering_module_config": {
            "input": {
                "filters": [
                    {
                        "type": "azure_content_safety",
                        "config": {
                            "Hate": 2,
                            "SelfHarm": 2,
                            "Sexual": 2,
                            "Violence": 2
                        }
                    }
                ]
            },
            "output": {
                "filters": [
                    {
                        "type": "azure_content_safety",
                        "config": {
                            "Hate": 2,
                            "SelfHarm": 2,
                            "Sexual": 2,
                            "Violence": 2
                        }
                    }
                ]
            }
        }
    }
}
Consume the Orchestration Service
Once you have deployed the orchestration service, it’s time to consume the deployment.

The following video shows how to consume this service and use advanced features such as data masking and content filtering.

Use generative-AI-hub SDK
You can perform these tasks using generative-AI-hub SDK. Refer to the following tutorials for more details:

Consumption of GenAI models Using Orchestration - A Beginner's Guide
Leveraging Orchestration Capabilities to Enhance Responses
You can switch to the Gen AI SDK options in these tutorials to navigate through the code to implement modules such as data masking and content filtering.

Recent updates for generative-AI-hub SDK
Here are the latest updates for the generative AI hub SDK that learners should be aware of to implement advanced AI techniques:

Version 4.3.1:

Enhanced Model Access: The SDK now supports models from OpenAI, Amazon, and Google, as well as integration with LangChain and the orchestration service.
Installation Options: Users can install the SDK with support for all models using:
Python

Copy code

Switch to dark mode
pip install "generative-ai-hub-sdk[all]"
"
For specific models, use:

Python

Copy code

Switch to dark mode
pip install "generative-ai-hub-sdk[google, amazon]"
Key updates: Enhanced model access, prompt registry templates, streaming support, debug logging, dependency updates, new model support, API logging.

These updates enhance the functionality and usability of the generative AI hub SDK, enabling learners to implement advanced AI techniques more effectively. For detailed release notes and further information, refer to the official documentation.

For details, use the following references:

generative-ai-hub-sdk - PyPI
Release Notes | Generative AI Hub SDK v4.3.1 - SAP Online Help
You can also explore the following SDKs for Javascript and Java:

SAP Cloud SDK for AI: Javascript SDK for SAP AI Core, SAP Generative AI Hub, and Orchestration Service.
SAP Cloud SDK for AI (for Java): Java SDK to leverage the generative AI hub to make use of templating, grounding, data masking, content filtering, and more.
Best Practices for Incorporating the Orchestration Service to Streamline AI Functionalities in Applications
Design Powerful AI Workflows
Connect diverse components such as data pipelines, AI models, and prebuilt modules.
Ensure seamless integration to focus on innovation rather than integration challenges.
Manage Data in Context
Provide an accurate understanding of data and its relevance.
Incorporate data security measures to protect sensitive information.
Use pattern recognition to identify and fix problems in master data.
Utilize Knowledge Graphs
Support orchestration, integration, and organization of data from different sources.
Build connections between entities within data sources for better visibility and process building.
Provide explanations of underlying systems to data scientists and engineers.
Leverage Developer-Focused Tooling
Use powerful SDKs and client libraries to enhance applications with generative AI.
Build custom AI solutions faster and easier with developer-focused tools.
Implement Retrieval-Augmented Generation (RAG)
Use RAG as an AI framework to source the right data at the right time.
Ensure that LLMs are built upon the most accurate, up-to-date information.
Advantages of Using the Orchestration Service
Provider Agnostic: Easily access LLMs from various providers through a unified interface. This allows for effortless testing and comparison of models by simply switching a config parameter, eliminating the need for new deployments.
Simplified Integration: Streamline the integration of LLMs into applications, promoting efficient and cost-effective development.
Expandability: Begin with essential features and add modules as needed, ensuring a smooth learning curve.
These advantages help in coordinating and managing the deployment, integration, and interaction of multiple AI models and components within a system efficiently. It also helps in avoiding manual interventions in the end-to-end lifecycle of AI applications.

Considerations Before Using the Orchestration Service
Function Calling Availability: Currently, structured output aspects are not available. This means that while the orchestration service can facilitate seamless integration and management of different LLMs, it does not yet provide the capability to call specific functions within those models directly. For instance, you cannot directly instruct the model to execute certain tasks based on structured commands or parameters. This limitation requires developers to implement additional layers or alternative methods to achieve function-specific outputs. However, ongoing developments and updates may soon introduce this feature, enhancing the overall versatility and functionality of the orchestration framework.
Open-Source Framework Support: While we support, some open frameworks, such as LangChain, not all open-source frameworks are supported. Extending support or exploring alternative methods may be required.
Conclusion
Incorporating the orchestration service into applications is essential for streamlining AI functionalities.

You saw how we can use the orchestration service to create streamline flows with ease.

By following best practices, organizations can focus on innovation, ensure data accuracy and security, and build powerful AI solutions efficiently.

Understanding Document Grounding in Generative AI Hub
Objective

After completing this lesson, you will be able to define document grounding, SAP HANA vector engine, and embedding models.
Vector Embeddings
Introduction
Let's understand some basic concepts and features before using advanced techniques like document grounding. We will first understand the significance of the SAP HANA vector engine and vector embeddings. We will see how the generative AI hub simplifies the usage of these techniques.

Vector Embeddings
A vector is a list of numerical float values with magnitude and direction.

In generative AI, it represents an object such as a book, car, or customer record by describing the object itself, its attributes, or its characteristics for comparison.

An embedding is a number within the vector that represents data by capturing meaningful information, semantic relationships, or contextual characteristics.

A table showing text-to-embedding pairs. In blue text are: tea [75, 75], coffee [80, 60], milk [130, 80]. In green text are: dog [290, 90], puppy [240, 100], wolf [300, 100]. In orange text is congress [400, 40].
Vector embeddings in the context of the SAP HANA vector engine refer to numerical representations of objects such as text, images, or audio. The model responsible for converting text to embeddings is called the Text Embeddings model. These embeddings are stored and managed within SAP HANA Cloud's vector engine, which is part of its multimodel processing capabilities.

The vector engine allows for efficient storage, retrieval, and analysis of high-dimensional vectors, enabling advanced applications like semantic search and RAG. This integration supports combining vector embeddings with other data types, facilitating intelligent data applications, and automated decision-making.

SAP HANA Vector Engine
The SAP HANA Vector Engine is a feature of SAP HANA Cloud that enables the storage, processing, and analysis of high-dimensional vectors, such as text embeddings, alongside other types of business data. This engine is a part of SAP HANA Cloud's multimodel processing capabilities, which allow for the integration of various data types, including relational, graph, spatial, and document data.

Key functionalities of the SAP HANA Vector Engine include:

Storing and managing vector embeddings, which are numerical representations of objects like text, images, or audio.
Enabling advanced applications such as semantic search and RAG, which combines LLMs with private business data.
Supporting efficient vector searches using SQL, with functions like L2DISTANCE() and COSINE_SIMILARITY(). L2DISTANCE() and COSINE_SIMILARITY() are vector functions available in the SAP HANA Vector Engine.
L2DISTANCE(): This function calculates the Euclidean distance between two vectors. It is commonly used to measure the straight-line distance between points in a high-dimensional space, which is useful for various applications such as clustering and nearest neighbor search.

COSINE_SIMILARITY(): This function computes the cosine similarity between two vectors. Cosine similarity measures the cosine of the angle between two vectors, which indicates how similar the vectors are in terms of their direction. It is widely used in text analysis and information retrieval to determine the similarity between documents or text embeddings.

A scatter plot with points labeled tea, coffee, milk, puppy, dog, wolf, and congress. Displays Euclidean distance and cosine of the angle between vectors puppy, dog, and other points.
These functions enable efficient vector searches and can be combined with other SQL operations within SAP HANA Cloud.

Facilitating context-aware responses and automated decision-making by applying the semantic meaning captured in vector representations.
The vector engine enhances the ability of intelligent data applications to provide detailed, context-aware responses and improves the overall efficiency and scalability of data processing within SAP HANA Cloud.

Grounding in Generative AI Hub
Document grounding is a process that combines generative LLMs with advanced information retrieval techniques to improve the quality and accuracy of responses. It achieves this without the time, complexity, and expense of training or fine-tuning an LLM with company-specific data. Instead, it uses a customer’s own knowledge sources (such as HR policy manuals) to supplement the LLM's internal representation of information, making the models more accurate and reliable.

Diagram of Generative AI workflow showing data ingestion, orchestration, and retrieval. Integrates SAP HANA Cloud vector engine for search and embedding, processing document store content.
Here is how document grounding in generative AI hub uses embedding models and the SAP HANA Cloud vector engine to enhance the contextual relevance and accuracy of AI responses. The system comprises several key components:

Document Store
Houses various document types (for example, PDF, text files) which serve as the knowledge base.

Generative AI Hub
This central hub manages the processing and retrieval of information. It consists of three main stages:

Data Ingestion: Incoming documents undergo preprocessing, chunking (breaking into smaller segments), and embedding (converting text into numerical vectors).
Orchestration: This stage handles the overall workflow, including the grounding process and access to Large Language Models (LLMs).
Retrieval: This stage retrieves relevant information using embedding search and federated search (searching across multiple data sources). Query embeddings are generated to facilitate similarity search.
SAP HANA Cloud Vector Engine
This component provides the underlying infrastructure for similarity search based on the generated embeddings and stores user-provided content.

Embedding & Similarity Search
Connects the retrieval stage with both SAP and non-SAP content sources based on similarity between query and document embeddings.

In essence, the system takes documents, processes them into a searchable format, and uses a combination of embedding and federated search techniques to retrieve relevant information based on user queries, facilitating grounded text generation by LLMs.

Summary
In summary, grounding in the generative AI hub uses the SAP HANA vector engine to enhance the contextual relevance of AI responses. The generative AI hub serves as an abstraction layer to access a wide range of LLMs from various providers. The SAP HANA Cloud vector engine stores "embeddings" of unstructured data, which are numerical representations of objects, such as text, images, or audio, in high-dimensional vectors. These embeddings are used for semantic data retrieval, enabling advanced text search and similarity queries.

This helps organizations to achieve more accurate, contextually relevant AI responses, driving better decision-making and innovation.

Implementing Document Grounding in the Orchestration Service
Objective

After completing this lesson, you will be able to demonstrate the practical application of Document Grounding within the generative AI hub.
Implementation of Document Grounding in generative AI hub
The grounding capability is now available in the orchestration module of the generative AI hub. It provides specialized data retrieval through vector databases, grounding the retrieval process using external and context-relevant data. Grounding combines generative AI capabilities with the capacity to use real-time, precise data to improve decision-making and business operations for specific AI-driven solutions. This data supplements the natural language processing capabilities of pretrained models trained on general material. The Pipeline API is proxied through the SAP AI Core generative AI hub and incorporates vector stores, such as the managed SAP HANA database.

Screenshot of SAP AI Launchpad showing Orchestration page. The Build Orchestration Workflow progress bar highlights Grounding, displaying input and output variables, and selected sources settings.
The generative AI hub supports document grounding through several key features and processes:

Access to LLMs: The generative AI hub provides instant access to a range of LLMs from different providers, such as GPT-4 by Azure OpenAI and OpenSource meta-lama. This broad access allows the orchestration of multiple models to enhance AI capabilities.
Integration with SAP AI Launchpad: Users can execute prompts in the AI Launchpad, showcasing how generative AI can assist with business processes. The SAP AI Core infrastructure supports text template generation based on applications while ensuring data security.
Improved Context and Accuracy: By grounding AI responses in customers' specific documents (like HR policies), the generative AI hub enhances the context and accuracy of generated content. It minimizes the chance of hallucinations and reduces ambiguity.
Document Indexing: Unstructured and semi-structured data from documents are preprocessed, split into chunks, and converted into embeddings using LLMs. These embeddings are stored in the SAP HANA Vector Database for efficient querying, which aids in grounding AI responses in real, relevant data.
These features ensure that generative AI can provide reliable, transparent, and contextually accurate responses by using the customer's own document repositories.

Enhance Content Generation with Document Grounding in Generative AI Hub
In this lesson, you will learn to use the Document Grounding module in the Orchestration Service to generate content with the RAG approach. The Document Grounding module aligns input questions with relevant documents by retrieving them from a knowledge base and using them to produce high-quality responses. This knowledge base can consist of documents stored in a SharePoint folder, an Elasticsearch engine, or a data repository containing vectors.

You perform the following steps:

Create the knowledge base with relevant documents.
Configure the Document Grounding module in the Orchestration Service.
Generate content using the RAG approach based on the knowledge base.
Prerequisites
Install the Generative AI Hub SDK using the command:
Python

Copy code

Switch to dark mode
%pip install generative-ai-hub-sdk
Set the credentials for the Generative AI Hub SDK.
Detailed Steps
Step 1: Create a Vector Knowledge Base
Prepare your knowledge base before using the Grounding module in the orchestration pipeline.
The generative AI hub provides several options for users to prepare their knowledge base data:
Upload documents to a supported data repository, then run the data pipeline to vectorize the documents. For more details, refer to the Pipeline API.
Use the Vector API to directly provide chunks of the document. For additional information, see the Vector API.
Grounding Module Options
Choose one of the following options to use grounding:

Option 1: Upload Documents to Supported Data Repository and Run Data Pipeline
The pipeline collects documents and segments the data into chunks.
It generates embeddings, which are multidimensional representations of textual information, and stores them efficiently in a vector database.
The process involves the following steps:

Perform Initial One-Time Administrative Steps: Create a resource group and a generic secret for grounding. For more information, see:
Create a Resource Group for Grounding
Grounding Generic Secrets for Microsoft SharePoint
Prepare Vector Knowledge Base: Configure the Pipeline API to read unstructured data from data repositories and store it in a vector database. Use the Pipeline API to:
Read unstructured documents from various data repositories. Break the data into chunks and create embeddings.
Store the multidimensional representations of the textual information in the vector database.
Provide a repository ID to access the data.
For more information, see Preparing Data Using the Pipeline API.

Option 2: Provide Chunks of Documents via Vector API Directly
Provide chunks of data directly and store them using the Vector API. The process involves the following steps:

Perform Initial One-Time Administrative Steps:Create a Resource Group for Grounding.
Prepare Vector Knowledge Base: Provide chunks of information directly and store data in the vector database using the Vector API. Use the Vector API to:
Create collections.
Create documents by directly using the chunks of data provided by users.
Store data in the vector database.
Assign repository IDs to access the data.
For more information, see Preparing Data Using the Vector API.
Configure Grounding Module in the Orchestration: In the orchestration pipeline, you add configuration for the grounding requests:
Create a grounding request configuration in the orchestration pipeline using the repository IDs and filters.
Run the orchestration pipeline and check that the response refers to the user data. For more information, see Using the Grounding Module.
A detailed setup is also described here to prepare a knowledge base and verify it.

Step 2: Configure the Document Grounding Module
Now, you must define the configuration for the Document Grounding module, including setting up filters, and specifying the data repositories.

Python

Copy code

Switch to dark mode
orchestration_service_url = <your url code from deployment of the orchestration service>
You must have at least one orchestration-compatible deployment for a generative AI model running. For more information, see and Create a Deployment for Orchestration in SAP AI Core.

Next, you must import all relevant libraries. See the code in the code repository here.

Python

Copy code

Switch to dark mode
# Set up the Orchestration Service
aicore_client = get_proxy_client().ai_core_client
orchestration_service = OrchestrationService(api_url=orchestration_service_url)
llm = LLM(
    name="gpt-4o",
    parameters={
        'temperature': 0.0,
    }
)
template = Template(
            messages=[
                SystemMessage("""Facility Solutions Company provides services to luxury residential complexes, apartments,
                individual homes, and commercial properties such as office buildings, retail spaces, industrial facilities, and educational institutions.
                Customers are encouraged to reach out with maintenance requests, service deficiencies, follow-ups, or any issues they need by email.
                """),
                UserMessage("""You are a helpful assistant for any queries for answering questions.
                Answer the request by providing relevant answers that fit to the request.
                Request: {{ ?user_query }}
                Context:{{ ?grounding_response }}
                """),
            ]
        )
This Python code sets up an orchestration service crucial for handling complex tasks. It initializes an AI core client and configures an orchestration service with a given URL. The code then sets up an LLM with specific parameters to ensure consistent responses. Lastly, it creates a message template to aid in answering customer inquiries efficiently and effectively.

Next, we set up grounding services.

Python

Copy code

Switch to dark mode
# Set up Document Grounding
filters = [DocumentGroundingFilter(id="vector",
                                   data_repositories=["<add your data repository ID>"],  ,
                                   search_config=GroundingFilterSearch(max_chunk_count=15),
                                   data_repository_type=DataRepositoryType.VECTOR.value
                                   )
]

grounding_config = GroundingModule(
            type="document_grounding_service",
            config=DocumentGrounding(input_params=["user_query"], output_param="grounding_response", filters=filters)
        )

config = OrchestrationConfig(
    template=template,
    llm=llm,
    grounding=grounding_config
)
This Python code sets up and configures a document grounding service. It defines filters for document repositories and specifies search parameters. The grounding configuration is then integrated into an orchestration system, which uses templates and a language model to process and respond to user queries. This setup ensures efficient and accurate document retrieval based on user inputs.

Step 3: Generate Context-Relevant Answers
Run the orchestration service with the configured settings to generate answers based on user queries.

Python

Copy code

Switch to dark mode
response = orchestration_service.run(config=config,
                            template_values=[
                                TemplateValue("user_query", "List the issues that are reported by customers."),
                            ])
print(response.orchestration_result.choices[0].message.content)
This Python code sends a request to an orchestration service to run a specific configuration. It includes a template value with a user query asking to list customer-reported issues. After running the service, it prints the response from the orchestration result, specifically the message content of the first choice. This helps automate and fetch data on customer issues efficiently.

You get the following output:

List of customer-reported issues including noise, landscaping, window cleaning, AC malfunction, janitorial, elevator, heating, electrical, plumbing, water damage, roofing, pest control, and cleaning.

You can see that the list of issues reported by customer, which is grounded in mails that customers provided.

You can see the documents that were retrieved for the context from the data repository.

Python

Copy code

Switch to dark mode
print(response.module_results.grounding.data['grounding_result'])
This code extracts and displays the value of 'grounding_result' from a nested data structure within the 'response' object. This specific piece of data could be critical for understanding the outcome of a grounding module, making the code essential for debugging or analysis.

The output lists all the relevant mails used for response earlier, providing a deep insight into the context of the grounding technique. You can see this output in the repository.

Conclusion
In this lesson, you have successfully achieved the following tasks:

Created a Vector Knowledge Base: Uploaded and vectorized documents to form a searchable knowledge base.
Configured the Document Grounding Module: Set up the module to retrieve relevant documents based on user queries.
Generated Context-Relevant Answers: Used the Orchestration Service to produce accurate and contextually relevant responses.
You can enhance the accuracy and relevance of AI-generated content by using document grounding in generative AI hub, addressing common challenges in AI-driven content generation.

Summarize Key Learnings
Throughout this learning journey, you have explored and applied advanced AI techniques, including document grounding, vector embeddings, data masking, content filtering, to enhance business applications using SAP's generative AI hub. These techniques collectively offer significant benefits that can transform how businesses operate and solve complex problems.

The Orchestration service streamlines AI workflows by integrating various AI components, such as templating, content filtering, and data masking. This not only improves the efficiency and accuracy of AI-driven processes but also ensures data privacy and security, enabling businesses to manage complex AI workflows with ease.

Vector embeddings enhance the ability to analyze and retrieve information by representing data in high-dimensional vectors. This allows for advanced applications like semantic search and RAG, which improve the quality and relevance of AI-generated content.

The SAP HANA Cloud vector engine stores "embeddings" of unstructured data, which are numerical representations of objects, such as text, images, or audio, in high-dimensional vectors. These embeddings are used for semantic data retrieval, enabling advanced text search and similarity queries.

Document grounding ensures that AI responses are contextually relevant and accurate by using real-time data from your own knowledge sources. This minimizes the risk of misinformation and enhances decision-making processes by providing precise and reliable insights.

Document grounding in generative AI hub leverages embedding models and the SAP HANA Cloud vector engine to enhance the contextual relevance and accuracy of AI responses.

The generative AI hub serves as an abstraction layer to access a wide range of LLMs from various providers.

By mastering these techniques, you are now equipped to implement AI solutions that drive operational efficiency, reduce costs, and enhance customer experiences. It enables you to tackle business challenges with innovative AI-driven approaches, ultimately leading to improved productivity and strategic growth.