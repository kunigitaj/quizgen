{"custom_id": "topicmap_0001", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5", "input": [{"role": "system", "content": "You are an expert instructional designer.\nFrom ordered chunks of a single document, produce a compact topic map that fully covers the material.\n\nReturn ONLY valid JSON with:\n{\n  \"schema_version\": \"1.0\",\n  \"units\": [\n    {\n      \"unit_id\": \"u1\",\n      \"title\": \"Unit Title\",\n      \"topics\": [\n        {\"topic_id\": \"u1_t1_slug\", \"title\": \"Topic Title\", \"summary\": \"1-2 sentences\",\n         \"chunk_span\": [start_idx, end_idx]}\n      ]\n    }\n  ]\n}\n\nHARD CONSTRAINTS (must all be satisfied):\n- You will be given N (total_chunks) and the valid chunk indices 0..N-1 in the user message.\n- The UNION of all topic chunk_span ranges MUST cover EVERY index in 0..N-1 with no gaps and no overlaps.\n- chunk_span uses inclusive integer indices and MUST stay within [0, N-1].\n- Respect semantic boundaries: headings and ellipsis separators have been preserved in chunking; avoid splitting a topic across unrelated sections.\n- Titles must be concise and specific; prefer 6–15 topics for a ~3,000-line text; add more units if needed.\n- If a chunk contains only boilerplate (e.g., “Objective”, “Note”, “Demo”), roll it into the nearest relevant topic so coverage remains continuous.\n"}, {"role": "user", "content": "Create the topic map from these CHUNKS (index + first lines shown).\n\nTOTAL_CHUNKS (N): 61\nVALID INDICES: 0..60\nEXPLICIT INDEX LIST: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60]\n\nGuidance:\n- These chunks are aligned to lesson/section boundaries using '...' (minor breaks) and '......' (unit ends).\n- Group related chunks into coherent topics; keep each topic's span contiguous.\n- Prefer unit titles like \"Unit 1 – Preparing the Modeling Environment\" and topic titles like \"Getting Started with BAS\", \"Development Spaces\", etc.\n\nCHUNKS PREVIEW:\n[{\"index\": 0, \"text\": \"Developing Data Models with SAP HANA Cloud\\nUnit 1\\nPreparing the Modeling Environment\\nAfter completing this unit, you will be able to:\\n\\nDescribe SAP Business Application Studio and how it is used for development in SAP HANA Cloud.\\nCreate a Development Space for your modeling content in SAP Business Application Studio.\\nCreate a modeling project in SAP Business Application Studio.\\nImport a Project into your Workspace of SAP Business Application Studio.\\nDescribe the layout and features of SAP Business Application Studio that are relevant to modeling.\"}, {\"index\": 1, \"text\": \"Development Spaces in SAP Business Application Studio\\nIn SAP Business Application Studio, each developer has their own environment. This environment is made up of one or more Development Spaces.\\n\\nBe careful not to confuse development spaces with Cloud Foundry spaces.\\nWhen you create a development space, you will be asked to choose tools, known as extensions, that you require. For modeling in SAP HANA Cloud, it is essential that you include the SAP HANA Calculation View Editor extension in your development space. This is a key tool that you will use.\\n\\nWatch the video, Configuring a Development Space, following this figure to learn about configuring a Development Space and choosing extensions.\\n\\nManaging Development Spaces\\nThe landing page of SAP Business Application Studio contains the list of Development Spaces. You may have access to more than one. This list Includes the following:\"}, {\"index\": 2, \"text\": \"Importing an Existing Project in SAP Business Application Studio\\nObjective\\n\\nAfter completing this lesson, you will be able to import a Project into your Workspace of SAP Business Application Studio.\\nImport a Project\\nSharing Development Content using Git\\nGit is the preferred method for sharing development content in an SAP HANA Cloud project. Sharing means importing and exporting content with other developers.\\n\\nGit is used to manage the source code in an application development environment. It provides tools for developers who need to collaborate on projects and work in parallel. Git provides sophisticated features to manage the source code, especially in large projects where multiple team members need to work together and share objects without overwriting each others' work.\\n\"}, {\"index\": 3, \"text\": \"Unit 2\\nCreating Calculation Views\\nAfter completing this unit, you will be able to:\\n\\nUnderstand modeling terminology of SAP HANA Cloud so that you are ready to begin modeling.\\nCreate a dimension calculation view using the graphical calculation view editor.\\nCreate a cube calculation view using the graphical calculation view editor.\\nCreate a time-based dimension calculation view using the graphical calculation view editor.\\nUnderstand which data sources are supported by calculation views.\\nCheck the output of a calculation view to ensure correct results are generated.\"}, {\"index\": 4, \"text\": \"Creating Dimension Calculation Views\\nObjective\\n\\nAfter completing this lesson, you will be able to create a dimension calculation view using the graphical calculation view editor.\\nDimension Calculation Views\\nPurpose of a Dimension Calculation View\\nA dimension calculation view is used to expose master data from source tables. You can combine multiple data sources, define filters, calculate additional attributes, and create hierarchies to provide a meaningful view of master data. A dimension calculation view is a highly-reusable, centralized object that provides consistent data. You would typically define a dimension calculation view and share it with other colleagues who would consume it in their own calculation views.\\n\\nAn example of a dimension view for customer that accesses columns from different database tables. ID, Name, and Country are accessed from the Customer Table. Contact Frequency is accessed from the Contact Preference Table.\\nDimension calculation views are only built from attributes. They do not include measures. Every column from the data source is treated as an attribute. This means that any numerical column in the source record, such as price or salary, is treated as an attribute and not a measure. This means when you query a dimension calculation view, a complete list of every single individual value in the source is produced in the output. Even the numerical values such as quantities are listed individually and not aggregated. If you want to sum values then you need a cube calculation view where it is possible to define measures.\"}, {\"index\": 5, \"text\": \"Creating Cube Calculation Views\\nObjective\\n\\nAfter completing this lesson, you will be able to create a cube calculation view using the graphical calculation view editor.\\nCube Calculation Views\\nWhen you would like to create a calculation view that includes measures, you use a calculation view of the following type: cube.\\n\\nAn example of a cube calculation view for orders that accesses columns from different database tables. Order ID, Customer ID, and Quantity Sold are accessed from the Sales Table. Quantity Returned is accessed from the Returns Table.\\nBy default, all the measures in this type of calculation view will always be aggregated by the attributes requested by the query. Consequently, even though the calculation view may be able to provide many attributes, the measures are always automatically aggregated only by the attributes that were requested by the query.\\n\"}, {\"index\": 6, \"text\": \"Creating Time-Based Dimension Calculation Views\\nObjective\\n\\nAfter completing this lesson, you will be able to create a time-based dimension calculation view using the graphical calculation view editor.\\nTime-Based Dimension Calculation Views\\nThere are two types of dimension calculation view: Standard and Time.\\n\\nYou use time-based dimension calculation views to automatically generate various date-related attributes from a base date. For example, from the base date dd-mm-yy, the time-based dimension calculation view automatically provides the following:\\n\\nThe number of the day in the week. For example, Wednesday = day 3\"}, {\"index\": 7, \"text\": \"Checking the Output of a Calculation View\\nObjective\\n\\nAfter completing this lesson, you will be able to check the output of a calculation view to ensure correct results are generated.\\nChecking the Output of a Calculation View\\nWhen you are building a calculation view, SAP Business Application Studio allows you to use a feature called Data Preview to check the results.\\n\\nThere are two ways to display a data preview:\\n\\nRight-click the top node of an open calculation view and choose Data Preview.\"}, {\"index\": 8, \"text\": \"Working with Common Features of Calculation Views\\nObjective\\n\\nAfter completing this lesson, you will be able to describe features that are common to all types of calculation view.\\nDefining Column Semantics\\nEvery calculation view has a Semantics node. You do not add this, it is already present and will always be the top node (regardless of the type of calculation view). In this node, you assign semantics to each column in order to define its behavior and meaning. This is important information used by consuming clients so that they are able to display and process the columns appropriately.\\n\\nScreen capture of details of the Semantics node of a calculation view. As Semantics, you can specify that a field is Quantity with Unit Of Measure or Amount with Currency Code or Date. For the reason to do so, see the following text.\\nOne of the most important and mandatory settings for each column is its column type. You can choose between attribute or measure. This defines the basic role of the column.\\n\"}, {\"index\": 9, \"text\": \"Top View Node\\nObjective\\n\\nAfter completing this lesson, you will be able to describe the function of the top view node.\\nThe Top View Node\\nTop View Node\\nWhen you create a new calculation view, there are always two nodes that are automatically provided.\\n\\nThe very top node is the Semantic node. This node is used to define enrichments to the final results set, such as adding currency information to measures, or apply masking rules to cover up sensitive parts of a data. This is a mandatory node and no additional nodes can be added on top.\\n\"}, {\"index\": 10, \"text\": \"Working with Aggregation Nodes\\nObjective\\n\\nAfter completing this lesson, you will be able to aggregate measures using the aggregation node.\\nAggregation Node\\nThe purpose of an Aggregation node is to apply aggregate functions to measures based on one or several attributes. The node generates a result set that is grouped by the selected attribute columns and computes the selected measure with the specified function, such as SUM, MIN, AVG, and so on.\\n\\nScreen capture of an Aggregation node of a calculation view. In this example, details on the Columns tab define aggregation function SUM for the measure Amount_Total and MAX for the measure Largest_Amount.\\nBy default, the granularity of the aggregation is defined by the attribute columns that are mapped to the output of the aggregation node. In the example, you will get one aggregate row for each country/order pair, showing two measures: the total amount of each order, and the biggest line item of each order.\\n\"}, {\"index\": 11, \"text\": \"Knowledge quiz\\nIt's time to put what you've learned to the test, get 2 right to pass this unit.\\n\\n1.\\nWhen is the calculated column computed in an aggregation node?\\nChoose the correct answer.\\n\\nBefore aggregation\\n\\nAfter aggregation\"}, {\"index\": 12, \"text\": \"Unit 4\\nJoining Data Sources in Calculation Views\\nAfter completing this unit, you will be able to:\\n\\nImplement the different join node types to combine data sources.\\nSpecify how a filter applies to a join.\\nOptimize joins to improve calculation view performance.\\nUsing Join Nodes\\nFiltering on Join Nodes\\nOptimizing Joins\"}, {\"index\": 13, \"text\": \"Optimizing Joins\\nObjective\\n\\nAfter completing this lesson, you will be able to optimize joins to improve calculation view performance.\\nJoin Optimization\\nSet Join Cardinalities\\nWhen you define a join, you can optionally set the cardinality to describe the relationship between the two data sources, that is, 1..1, 1..n, n..1, and n..m.\\n\\nBased on the join cardinality setting, some joins can be omitted to improve run-time performance, but a wrong setting can lead to unexpected results. Therefore, you should always set a correct join cardinality. Follow a cardinality proposal where applicable (only tables as sources) and ensure continued correctness of the setting.\\nWe recommend that you always set join cardinalities so the optimizer can decide if joins can be omitted at runtime based on the query that is being sent to the calculation view.\"}, {\"index\": 14, \"text\": \"Knowledge quiz\\nIt's time to put what you've learned to the test, get 5 right to pass this unit.\\n\\n1.\\nWhat do you implement to allow data analysis at different levels of granularity with the same calculation view?\\nChoose the correct answer.\\n\\nNon-equi join\\n\\nReferential join\"}, {\"index\": 15, \"text\": \"Working with the Union Node\\nObjective\\n\\nAfter completing this lesson, you will be able to combine data from different sources using the union node.\\nUnion Node\\nA Union node is used to combine two or more data sets to a common set of columns.\\n\\nScreen capture of the definition of a union node that combines actual and plan data. If you want to combine multiple result sets with identical structures into a single result set, you can use a union node. A mapping of the sources to the target is required and allows you to adapt structural differences. This can be done via a drag-and-drop interface.\\nDepending on how different the column names are in the data source, you can map the columns automatically by name (columns with identical names will be mapped automatically) or define the mapping manually.\\n\"}, {\"index\": 16, \"text\": \"Implementing Union Pruning\\nObjective\\n\\nAfter completing this lesson, you will be able to implement union pruning to improve performance of calculation views.\\nUnion Pruning\\nWhat is Union Pruning?\\nThere are opportunities to significantly improve the performance of unions by implementing union pruning. There are three approaches to implementing pruning rules in unions. Two of these approaches are based on column values and one is based on column names. We will first cover union pruning using values.\\n\\nConstant Pruning\\nFor each data source in a union node, you can define an additional column and fill it with a constant value to explicitly tag the meaning of the data source. For example, you could assign the constant valueplan to the data source that represents plan data and assign the constant value actual to the data source that represents actual data. If a query filter - where clause - does not match the constant value for a specific source of a union, then the source is ignored. This is called explicit pruning. Explicit pruning helps performance by avoiding access to data sources that are not needed by the query.\"}, {\"index\": 17, \"text\": \"Knowledge quiz\\nIt's time to put what you've learned to the test, get 2 right to pass this unit.\\n\\n1.\\nWhich of the following are union pruning approaches?\\nThere are three correct answers.\\n\\nExplicit pruning using constants\\n\\nColumn-based pruning\"}, {\"index\": 18, \"text\": \"Implementing Rank Nodes\\nObjective\\n\\nAfter completing this lesson, you will be able to configure a rank node to identify the top or bottom values of a data set.\\nRank Node\\nThe purpose of the Rank node is to enable the selection, within a data set, of the top or bottom 1, 2, ... n values for a defined measure, and to output these measures together with the corresponding attributes and, if needed, other measures.\\n\\nFor example, with a Rank node, you can easily build a subset of data that gives you the five products that generated the biggest revenue (considering the measure GROSS_AMOUNT) in each country. The country, in this example, defines a Logical Partition of the source data set.\\n\\n\"}, {\"index\": 19, \"text\": \"Knowledge quiz\\nIt's time to put what you've learned to the test, get 1 right to pass this unit.\\n\\n1.\\nWhy might you implement a rank node?\\nThere are two correct answers.\\n\\nTo identify the best selling products in each region for each of the last 5 years.\\n\\nTo identify the training courses that represent the bottom 10%, based on bookings per year.\"}, {\"index\": 20, \"text\": \"Generating Calculated Columns\\nObjective\\n\\nAfter completing this lesson, you will be able to generate a calculated column.\\nCalculated Columns\\nIt is possible to create additional calculated columns in any type of calculation view. Calculated columns are generated on the fly at runtime and do not persist the result.\\n\\nAn example of a calculated column might be as follows: you have two columns containing the first and last name of the customer, but you would like to combine the first and last name in a single column. You can do this by creating a calculated column based on a string function. Alternatively, you could use an 'If / then / else' expression to check the value of a column and decide what to do if it is empty, perhaps by providing a fixed value 'Other' if it is empty.\\n\\nA calculated column that replaces a 'NULL' value with the text 'Other'. First click the '+ 'icon, then enter general properties, such as column name 'DIVISION_2' and data type 'VARCHAR(50)', and in the lower part an expression, such as 'IF (isnull(DIVISION),'Other',DIVISION)'.\"}, {\"index\": 21, \"text\": \"Filtering Data\\nObjective\\n\\nAfter completing this lesson, you will be able to implement a filter to restrict data.\\nUsing Filter Operations\\nFiltering data is frequently required when analyzing data to reduce the result set to make it more meaningful to the business user. Typically, you might want to retrieve the sales details for a particular country or region, for a particular range of products, or specific customers (new customers, customers who have not ordered any product for more than one year). These three examples relate to filtering based on attributes.\\n\\nOn the other hand, you might want to filter data based on measures. For example, you might want to list only the sales orders for which the total amount exceeds a threshold.\\n\\nThe two approaches can also be combined, for example, when you want to retrieve the list of US customers who have created more than 10 orders during the past month. In this case, filtering the data by country (US) can be done as early as possible in the data flow, but you must compute the total number of orders per customer (for last month) before applying the threshold (10).\"}, {\"index\": 22, \"text\": \"Implementing Currency Conversion\\nObjective\\n\\nAfter completing this lesson, you will be able to describe how to implement currency conversion.\\nCurrency Conversion in SAP HANA Cloud\\nMost organizations operate in multiple currencies but will usually want to report in a single currency. When we load data to SAP HANA Cloud database the individual transactions are often stored in mixed currencies. SAP HANA Cloud calculation views can convert currencies during runtime.\\n\\nThe concept of currency conversion. Source data contains a measure amount with different currency values (EUR, USD, GBP). In the Semantics node, define conversion rules. The output contains only EUR currency value with calculated corresponding amount values.\\nEven if the reporting tool is able to convert currencies, you should always consider implementing currency conversion in the calculation view to ensure high performance as the conversion in SAP HANA Cloud is processed completely in-memory. Additionally, by defining the conversion rules in the calculation view you ensure consistency across reports that use the same calculation view as you define the rules only once and not in each report.\\n\"}, {\"index\": 23, \"text\": \"Knowledge quiz\\nIt's time to put what you've learned to the test, get 3 right to pass this unit.\\n\\n1.\\nA calculated column can generate an attribute or a measure?\\nChoose the correct answer.\\n\\nTrue\\n\\nFalse\"}, {\"index\": 24, \"text\": \"Implementing Variables\\nObjective\\n\\nAfter completing this lesson, you will be able to implement variables to filter data by attributes.\\nVariables\\nYou define a variable in a calculation view when you want to pass dynamic values to filter attribute columns.\\n\\nWhen a calculation view that contains a variable is called by a front-end reporting tool, the calculation view passes a request to the front-end tool to provide a value (usually using a user prompt) to complete a WHERE clause. The WHERE clause is added to the query that runs on top of the calculation view. For example, if the user was prompted to choose a country and the user chose 'JP', the SQL that runs on the calculation view looks like this:\\n\\nSELECT <columns> FROM <calculation_view_name> WHERE ((\\\"COUNTRY\\\" = 'JP')) GROUP BY <group_by columns>;\"}, {\"index\": 25, \"text\": \"Implementing Input Parameters\\nObjective\\n\\nAfter completing this lesson, you will be able to define input parameters to pass dynamic values to a calculation view.\\nInput Parameters\\nAn input parameter is similar to a variable in that it supports to passing of dynamic values to a calculation view at runtime. But whereas a variable has only one purpose, that is to provide a dynamic filter based on attributes, an input parameter can fulfill additional requirements.\\n\\nInput parameters can be used in different places to provide dynamic values, in particular, calculated columns, restricted columns and filter expressions. Here are a few examples:\\n\\nCalculated Column\"}, {\"index\": 26, \"text\": \"Mapping Variables and Input Parameters\\nObjective\\n\\nAfter completing this lesson, you will be able to map variables and input parameters.\\nMapping of Variables and Input Parameters\\nParameter Mapping\\nIn some cases, your calculation view needs to pass information provided by the user, via a variable or input parameter, to another object. This could be another calculation view or even a procedure or function.\\n\\nThis is called parameter mapping and is an important feature of SAP HANA calculation view modeling.\\n\"}, {\"index\": 27, \"text\": \"Knowledge quiz\\nIt's time to put what you've learned to the test, get 3 right to pass this unit.\\n\\n1.\\nWhat must you do if you want to use the IP_COUNTRY input parameter value to filter the value help of the IP_CITY input parameter?\\nChoose the correct answer.\\n\\nCreate a value help view for the IP_COUNTRY parameter, listing countries with a filter on Country using an input parameter. Then map this input parameter to the IP_COUNTRY parameter.\\n\\nCreate a value help view for the IP_CITY parameter, listing cities with a filter on Country using an input parameter. Then map this input parameter to the IP_CITY parameter.\"}, {\"index\": 28, \"text\": \"Modeling Hierarchies\\nObjective\\n\\nAfter completing this lesson, you will be able to define a hierarchy to organize data for efficient navigation.\\nHierarchy\\nHierarchies play a key role in data modeling as they provide an easy-to-use navigation aid when drilling through lots of data. As the user clicks on the hierarchy node, the node is expanded to offer even more lower level values. Typically, measures are accumulated from the lower level to the upper levels. A lot of business data is organized by hierarchy so this is a very popular topic for modeler.\\n\\nExample of a hierarchy - juice and water is under beverage, burgers and soup is under food, beverage and food is under catering.\\nSAP HANA Cloud calculation views support two types of hierarchy: Level and Parent-Child.\\n\"}, {\"index\": 29, \"text\": \"Knowledge quiz\\nIt's time to put what you've learned to the test, get 1 right to pass this unit.\\n\\n1.\\nWhat are two types of modeled hierarchy that you can create using calculation views?\\nThere are two correct answers.\\n\\nParent-Child\\n\\nReferential\"}, {\"index\": 30, \"text\": \"Implementing SQL in Calculation Views\\nObjective\\n\\nAfter completing this lesson, you will be able to implement SQL in a calculation view.\\nSQL Expressions in Calculation Views\\nExpressions in calculation views are written in SQL. You write expressions in the following:\\n\\nCalculated columns\\n\\nFilters\"}, {\"index\": 31, \"text\": \"Working with SQLScript\\nObjective\\n\\nAfter completing this lesson, you will be able to describe the additional features provided by SQLScript compared to standard SQL.\\nSQLScript Capabilities\\nSQLScript is a database language developed by SAP. It is based on standard SQL but adds additional capabilities to exploit the advanced features of SAP HANA Cloud.\\n\\nHere are some of the key features of SQL Script:\\n\\nSQLScript extends ANSI-92 SQL. Just like other database vendors extend SQL in their databases, SAP HANA extends SQL using SQLScript.\"}, {\"index\": 32, \"text\": \"Creating and Using Functions\\nObjective\\n\\nAfter completing this lesson, you will be able to describe how functions can be consumed by calculation views.\\nIntroducing Functions\\nFunctions allow developers to define a reusable block of data processing logic using the powerful SQLScript language to generate a result as either a single value, or a complete data set. For a modeler, functions are interesting because they can be used in calculation views to provide custom logic when the standard features of graphical calculation view editor are insufficient.\\n\\nFunctions can consume input parameters defined in calculation views. This means that the same function can be reused in different calculation views.\\n\\nSAP HANA Cloud supports scalar and table functions. Table and scalar functions are created as source files in a project folder. The source file has the extension .hdbfunction The same extension is used for table and scalar functions.\"}, {\"index\": 33, \"text\": \"Knowledge quiz\\nIt's time to put what you've learned to the test, get 5 right to pass this unit.\\n\\n1.\\nWhy is knowledge of SQL important to an SAP HANA modeler?\\nThere are two correct answers.\\n\\nSo that they can develop calculation views using SQL instead of using the graphical editor\\n\\nSo that they can develop custom logic using an SQL function and consume this within a calculation view\"}, {\"index\": 34, \"text\": \"Implement Recommended Modeling Practices\\nObjective\\n\\nAfter completing this lesson, you will be able to implement recommended modeling practices.\\nUnderstand the Instantiation Process\\nBefore we dive into the detail of how to improve the calculation view performance, it is helpful to learn a little about the way a calculation view responds to a query that calls it.\\n\\nA schema of the optimization process for queries on stored models. First, the calculation engine instantiates a run-time model for the query, then optimizes it, and finally the SQL optimizer takes over. For details, refer to the following text.\\nThe calculation engine pre-optimizes queries before they are worked on by the SQL optimizer.\\n\"}, {\"index\": 35, \"text\": \"Implement Best Practices in calculation view nodes\\nObjective\\n\\nAfter completing this lesson, you will be able to implement best practices in calculation view nodes.\\nBest Practices in Calculation View Nodes\\nIn this lesson, we will cover recommendations and best practices for nodes in your calculation views.\\n\\nColumn Pruning\\nRecommendation for column mapping and pruning. Verify that the intermediate sets are as small as possible. Therefore, investigate which attributes are requested on each operations and why. For more details, refer to the following text.\\nMake sure that you do not map columns that are not needed. This might sound like a fairly obvious recommendation, but it is very common for developers to simply map all columns without considering if and why there are used. They are often added 'just in case' they are needed later. Too many attributes can lead to very large, granular data sets especially when only aggregated data is needed.\"}, {\"index\": 36, \"text\": \"Validating calculation views with Performance Analysis mode\\nObjective\\n\\nAfter completing this lesson, you will be able to validate calculation views with the Performance Analysis mode.\\nIntroducing the Performance Analysis Mode\\nWhen you are developing a calculation view, the calculation view editor is able to highlight settings that might lead to poor performance.\\n\\nTo enable this, you use the Performance Analysis mode. This tool is launched from inside the calculation view editor in the toolbar (the icon looks like a speedometer). You do not need to activate or build a calculation view in order to use this tool. The basic idea is that you are able to react at design time to warnings and other information relating to your choices, which might lead to poor performance so that you can consider alternative approaches.\\n\\nPerformance Analysis – Detailed Information\"}, {\"index\": 37, \"text\": \"Analyzing executions with the SQL Analyzer\\nObjective\\n\\nAfter completing this lesson, you will be able to analyze executions with the SQL Analyzer.\\nIntroducing the SQL Analyzer\\nWhat is the Purpose of the SQL Analyzer?\\nAlthough your calculation view may appear well designed, ultimately it is the performance of the calculation view that really counts. You have seen how to display the generated SQL in a calculation view using the Debug Query mode. While this is helpful to see how filters are pushed down and data is pruned, this does not tell us how the SQL actually performs. We really need to know the answers to the following questions:\\n\\nWhat was the total execution time?\\n\"}, {\"index\": 38, \"text\": \"Knowledge quiz\\nIt's time to put what you've learned to the test, get 3 right to pass this unit.\\n\\n1.\\nTo work with calculation view Debug Query Mode, you first need to deploy the calculation view?\\nChoose the correct answer.\\n\\nTrue\\n\\nFalse\"}, {\"index\": 39, \"text\": \"Partitioning Tables\\nObjective\\n\\nAfter completing this lesson, you will be able to define partitions to improve calculation view runtime.\\nPartitioning Tables\\nTable Partitioning\\nData in column store tables is separated into individual columns to support high performance on queries and also for better memory management. However, it is also possible to subdivide the rows of column tables into separate blocks of data. We call this table partitioning. If column separation is vertical subdivision, think of table partitioning as horizontal subdivision.\\n\\nA table containing three columns: Year, Country and Quantity. It contains rows for three different years (2020,2021 and 2022). Three partitions are displayed, containing each a subset of the table rows, depending on the Year value.\\nNote\"}, {\"index\": 40, \"text\": \"Implementing Static Cache to Improve Performance\\nObjective\\n\\nAfter completing this lesson, you will be able to improve calculation view performance with static cache.\\nCaching View Results\\nComplex calculation views place a heavy load on the system. To reduce this, it's possible to cache the results of a calculation view in order to speed up the time needed to get the results when executing a query against the view. This feature is useful when you have complex scenarios or large amounts of data so that the results don't have to be recalculated each time the query is executed. The benefits aren't just the reduced time for the results to appear for the user of the query, but it also means that CPU and memory consumption is reduced, leading to better performance for other tasks.\\n\\nOverview of the static cache principles. A first execution of Query A at 9:10AM will fetch the data from the database and store the result in the cache, taking a certain amount of time. A subsequent execution at 10:30AM will only have to read the data from the cache, thus taking much less time.\\nIf the cache can't be used, due to a query requesting a different data set than what is held in cache, a query doesn't fail. It means that the query will need complete processing.\\n\"}, {\"index\": 41, \"text\": \"Creating Snapshots\\nObjective\\n\\nAfter completing this lesson, you will be able to define snapshots queries on a calculation views to store its results.\\nCalculation View Snapshots\\nSometimes, storing calculation view results isn't just for performance. You could need to take pictures of your data at different points in time (snapshots). This could be done to store historical values. Or you could need performance but not on real-time data, just on a latest snapshot of the system taken in the morning, for example.\\n\\nThe Snapshot Process\\nLet's look at how the snapshot feature works.\\n\"}, {\"index\": 42, \"text\": \"Defining MDS Cubes\\nObjective\\n\\nAfter completing this lesson, you will be able to define MDS Cubes for a Calculation View.\\nDefining MDS Cubes for a Calculation View\\nConsider you want to prepare a model in SAP Analytics Cloud (SAC) for a few business users. This is based on the SAP HANA Calculation View technology. One business user needs to evaluate net sales amount and gross sales amount by business partner, distribution channel, and month. Other business users want to see the same information, but sometimes they need details by product, distribution channel, and date for these measures. Instead of creating two different calculation views, it is easier to create a single view with all these drill-down options. However, the more complex the calculation view becomes, the more processing resources will be needed. This is especially true when remote data access is involved. To improve the performance, you could allow caching for this view. However, the first dialogue would then take a long time because the cache is not yet filled. Additionally, it might not be possible to serve such different drill-down queries with the static cache. It's better to store pre-processed data in a format that can serve all these complex drill-down queries and allows to quickly answer these most frequently required drilldown situations. For this purpose, calculation views offer an additional property called multidimensional services cube (MDS cube).\\n\\nScreen captures of the steps to create an MDS cube: Create or open a Calculation View. Create a new MDS Cube. Open its details and add attributes and measures. Deploy the view.\\nAn MDS Cube is defined for a specific underlying calculation view and represents a submodel of this view, this means, a subset of its measures together with a subset of its attributes. When you define an MDS cube as the basis of a model in SAP Analytics Cloud, you must include all fields into the model that are required in your SAC model.\\n\"}, {\"index\": 43, \"text\": \"Knowledge quiz\\nIt's time to put what you've learned to the test, get 3 right to pass this unit.\\n\\n1.\\nWhich is the frontend tool for which you profit from MDS cube definition?\\nChoose the correct answer.\\n\\nSAP Analytics Cloud (via InA interface)\\n\\nSAP Lumira (via Javascript)\"}, {\"index\": 44, \"text\": \"Productivity Aids for Efficient Calculation View Development\\nObjective\\n\\nAfter completing this lesson, you will be able to use features to increase productivity of calculation view development.\\nFeatures to Enhance Productivity of Calculation View Design\\nCalculation views offer a lot of flexibility, in particular the possibility to have a large number of nodes. As calculation views become more complex, maintenance can be challenging. The SAP Business Application Studio provides functionality to manage nodes and semantics. The following features are available:\\n\\nFeature\\tPurpose\\nSwitching Node Types\\tSwitch one node type with another without losing the link to the data source or the upper and lower nodes.\\nReplacing a Data Source\\tReplace one node source with another without losing the output columns, calculated columns, and so on, throughout all of the upper nodes.\"}, {\"index\": 45, \"text\": \"Working with Modeling Content in a Project\\nObjective\\n\\nAfter completing this lesson, you will be able to audit calculation views using provided tools.\\nAuditing Dependencies Between Calculation Views\\nTwo features are available in SAP Business Application Studio to analyze modeling content within a project. These are:\\n\\nData lineage\\n\\nImpact analysis\"}, {\"index\": 46, \"text\": \"Project Structure\\nObjective\\n\\nAfter completing this lesson, you will be able to explain the structure of a project.\\nWorkspace, Projects, Modules, and Folders\\nA workspace in SAP Business Application Studio is a file structure where you can work on one or several projects. Each user has their own workspace, and can create additional ones if needed.\\n\\nThe workspace of a user can contain as many projects as needed, but each project must have a different name. Practically, this is done by storing the content of different projects into different root folders. It's possible to open a single project/folder at a time, or to create a multiroot workspace.\\n\\nRelationship of Workspace, Projects, and Modules. A user's workspace contains projects. A project contains modules with folders and objects.\"}, {\"index\": 47, \"text\": \"Deploying Models\\nObjective\\n\\nAfter completing this lesson, you will be able to deploy modeling content to the runtime environment.\\nThe SAP HANA Deployment Infrastructure\\nThe SAP HANA Deployment Infrastructure (HDI) is a set of services that allows the deployment of database objects into isolated HDI containers.\\n\\nHDI containers are simply database schemas, but they include additional features such as a dedicated owner of the container plus metadata to manage their database objects. Think of a container as a smart schema.\\n\\nThe objects included in the containers are defined in the HDB modules of your project, using source files and during deployment, the runtime objects are created and added to the container.\"}, {\"index\": 48, \"text\": \"Manage modeling content\\nObjective\\n\\nAfter completing this lesson, you will be able to manage modeling content.\\nImport and Export of Modeling Content\\nSAP Business Application Studio offers import and export features within the development space.\\n\\nThese features can be used to keep several versions of your modeling content, and re-import all or part of this content if needed. They're also useful for quick sandboxing, moving content from one development space to another.\\n\\nAdditionally, import/export can be used to share in a simple way your modeling content with another developer. For example, when a series of synonyms to external database objects have been defined in your project, you can easily share the synonyms definitions with another developer by sending the relevant design-time files for synonyms.\"}, {\"index\": 49, \"text\": \"Knowledge quiz\\nIt's time to put what you've learned to the test, get 3 right to pass this unit.\\n\\n1.\\nA runtime object in a container must always have a corresponding source file?\\nChoose the correct answer.\\n\\nTrue\\n\\nFalse\"}, {\"index\": 50, \"text\": \"Enabling Access to External Data\\nObjective\\n\\nAfter completing this lesson, you will be able to set up access to external data.\\nExternal Data Access Setup\\nThe SAP HANA Deployment Infrastructure (HDI) relies on a strong isolation of containers and a schema-free definition of all the modeling artifacts.\\n\\nThe isolation principle: By default, the Object Owner user of one container has access to all objects of his container, but no access to external data, neiter to another containers nor to a classic schama.\\nBy default, a container has full access to all of the database objects it contains (tables, calculation views, and so on), but has no access at all to other database schemas, whether it’s another container's schema or a classic database schema.\\n\"}, {\"index\": 51, \"text\": \"Using Git to Manage Source Code\\nObjective\\n\\nAfter completing this lesson, you will be able to manage source files using Git.\\nOverview of Git\\nWhy Should You Use a Version Control System?\\nIt's very likely that there will be multiple developers working on the same modeling project. The developers need to work together, and this means not overwriting each other's work, making sure they use the latest version of any file, and also being able to return to a previous version of any file if errors are found. To handle these requirements a source code version control system is needed.\\n\\nThe source code version control system needs to be able to handle parallel developments. Let's take a look at a common scenario:\\n\"}, {\"index\": 52, \"text\": \"Deploying an Application\\nObjective\\n\\nAfter completing this lesson, you will be able to manage the lifecycle of a project.\\nApplication Deployment\\nYou've already learned how to manage some important stages of a modeling project’s lifecycle.\\n\\nCreate a project and define key settings.\\n\\nImport and export modeling content.\"}, {\"index\": 53, \"text\": \"Defining Analytic Privileges\\nObjective\\n\\nAfter completing this lesson, you will be able to define analytic privileges.\\nAnalytic Privileges\\nAnalytic privileges are used to enable data access in calculation views by filtering the data based on the values of one or more attributes.\\n\\nThe rationale for analytic privileges is to allow the use of calculation views by different users who might not be allowed to see the same data. This approach means we don't have to create copies of the same calculation views with different filters defined.\\n\\nFor example, different regional sales managers who are only allowed to see sales data for their regions could use the same calculation view, but they would have different analytic privileges assigned to their user ID that specify the regions they're supposed to see.\"}, {\"index\": 54, \"text\": \"Defining Roles\\nObjective\\n\\nAfter completing this lesson, you will be able to create a design-time role to secure data access.\\nDesign-Time Roles\\nSecurity Concepts\\nBefore we cover roles, there are some important concepts that you must understand.\\n\\nA database user can be the owner of database objects. A role is a collection of privileges and can be granted to a user or another role. A privilege enables specific operations on one or several objects.\\nIn SAP HANA Cloud, we define roles and users, and assign privileges. Privileges can be assigned to users directly. They can also be assigned to users indirectly by using roles.\"}, {\"index\": 55, \"text\": \"Masking Sensitive Data\\nObjective\\n\\nAfter completing this lesson, you will be able to restrict access to columns containing sensitive data.\\nColumn Masking\\nWhen calculation views might expose sensitive data to the end user, it's possible to define a mask on the corresponding column(s) so that only authorized users will be allowed to see the actual data. For other users, the mask will be applied in order to hide part or all of the column when previewing the calculation view data or querying it with a front-end tool.\\n\\nThis slide shows how to mask a column.\\nDefining a Mask Expression\\nThis slide shows how to define a mask expression.\"}, {\"index\": 56, \"text\": \"Overview of Knowledge Graphs\\nObjective\\n\\nAfter completing this lesson, you will be able to describe the concept Knowledge Graph.\\nKnowledge Graphs\\nWhen we ask a virtual assistant about tomorrow’s weather forecast or use Google to search for the latest news on climate change, knowledge graphs serve as the foundation for today’s cutting-edge information systems. In addition, knowledge graphs have the potential to elucidate, evaluate, and substantiate information produced by deep learning models such as Chat-GPT and other large language models. Knowledge graphs have a wide range of applications, including improving search results, answering questions, providing recommendations, and developing explainable AI systems.\\n\\nA knowledge graph is a structured representation of facts about the world, describing entities and their interrelations, organized in a graph format. Here are some key aspects of a knowledge graph:\\n\\nAspect\\tDefinition\"}, {\"index\": 57, \"text\": \"SAP HANA Cloud Knowledge Graph Engine\\nObjective\\n\\nAfter completing this lesson, you will be able to discover the SAP HANA Cloud Knowledge Graph Engine.\\nSAP HANA Database Knowledge Graph Engine\\nThe SAP HANA database includes a sophisticated Knowledge Graph Engine designed to facilitate advanced analytics and graph processing.\\n\\nA diagram showing components of the SAP HANA Knowledge Graph Enging and how it interacts with the SAP HANA Cloud Index Server, Session Management and SAP HANA clients.\\nThe SAP HANA database Knowledge Graph Engine is a component within the SAP HANA in-memory database platform designed to support advanced graph processing and analysis. It extends SAP HANA Cloud's capabilities to handle complex, interconnected data structures, making it well-suited for a variety of modern analytical tasks.\\n\"}, {\"index\": 58, \"text\": \"Getting Started\\nInstalling and setting up the SAP HANA Cloud Knowledge Graph Engine involves several steps, and since SAP HANA Cloud is a managed service, many of the traditional installation steps are abstracted away.\\n\\nHere are the prerequisites to get you started with the Knowledge Graph Engine on SAP HANA Cloud:\\n\\nPrerequisites\\tTask\\nSAP HANA Cloud Instance\\nEnsure you have an active SAP HANA Cloud instance.\\n\\nSAP HANA Cloud Cockpit\"}, {\"index\": 59, \"text\": \"Use Cases and Demo\\nObjective\\n\\nAfter completing this lesson, you will be able to present use cases with the power of SAP HANA Cloud KGE.\\nUse Cases\\nSAP HANA Cloud Knowledge Graph Engine provides a robust platform for managing and leveraging comprehensive knowledge graphs, enabling advanced analytics, data integration, and knowledge-driven applications. Here are several use cases where SAP HANA Cloud Knowledge Graph Engine can be effectively applied:\\n\\nMaster Data Management\\nCustomer 360 View: Integrate data from various systems to create a unified customer profile, enabling better customer segmentation, personalized marketing, and improved customer service.\\n\"}, {\"index\": 60, \"text\": \"Knowledge quiz\\nIt's time to put what you've learned to the test, get 5 right to pass this unit.\\n\\n1.\\nIt is correct to say that the case study presented \\\"Ontology of a University\\\" is a Knowledge Graph model?\\nChoose the correct answer.\\n\\nTrue\\n\\nFalse\"}]\n\nReturn ONLY the JSON object described by the system message (no extra text)."}], "text": {"format": {"type": "json_object"}, "verbosity": "low"}, "max_output_tokens": 6000, "reasoning": {"effort": "minimal"}}}
