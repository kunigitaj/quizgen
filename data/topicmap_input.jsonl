{"custom_id": "topicmap_0001", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5", "input": [{"role": "system", "content": "You are an expert instructional designer.\nFrom ordered chunks of a single document, produce a compact topic map that fully covers the material.\n\nReturn ONLY valid JSON with:\n{\n  \"schema_version\": \"1.0\",\n  \"units\": [\n    {\n      \"unit_id\": \"u1\",\n      \"title\": \"Unit Title\",\n      \"topics\": [\n        {\"topic_id\": \"u1_t1_slug\", \"title\": \"Topic Title\", \"summary\": \"1-2 sentences\",\n         \"chunk_span\": [start_idx, end_idx]}\n      ]\n    }\n  ]\n}\n\nHARD CONSTRAINTS (must all be satisfied):\n- You will be given N (total_chunks) and the valid chunk indices 0..N-1 in the user message.\n- The UNION of all topic chunk_span ranges MUST cover EVERY index in 0..N-1 with **no gaps and no overlaps**.\n- chunk_span uses inclusive integer indices and MUST stay within [0, N-1].\n- If some chunks are minor/boilerplate, still include them in a small topic (e.g., “Misc/Glue”) so nothing is uncovered.\n- Prefer 6–15 topics per unit for a ~3,000-line text; add more units if needed to achieve complete coverage.\n- Titles should be concise and specific.\n"}, {"role": "user", "content": "Create the topic map from these CHUNKS (index + first lines shown).\n\nTOTAL_CHUNKS (N): 74\nVALID INDICES: 0..73\nEXPLICIT INDEX LIST: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73]\n\nCHUNKS PREVIEW:\n[{\"index\": 0, \"text\": \"1. Learning how to use the SAP AI Core service on SAP Business Technology Platform\\n\\n..\\n\\nConfiguring SAP AI Core and SAP AI Launchpad\\nObjectives\\n\\nAfter completing this lesson, you will be able to:\\nIllustrate the process involved in working with SAP AI Core and SAP AI Launchpad\\nConnect SAP AI Core and SAP AI Launchpad to user's docker and GitHub repositories\"}, {\"index\": 1, \"text\": \"inference, so that models can make predictions.\\n\\nTo ease the shipment of new AI scenarios, the service offers continuous delivery capabilities and ensures tenant isolation with multi-tenancy. To safeguard your costs and to scale on demand, you only pay for the resources you used but can tap into scalability and increased performance powered by GPUs.\\n\\nAll of the functionality comes out of the box and is managed for you while retaining openness to any AI framework so that you can ship your AI scenario easily.\\n\\nTo connect to your business applications as well as to operate your AI scenarios in AI Launchpad, SAP AI Core provides the standardized AI interface, AI API, which provides a common framework for consuming and operating your AI scenarios.\\n\\nDuring development your AI scenarios, you are supported with development tooling, such as the SAP AI Core SDK, and full flexibility in the choice of storage for your data and models.\\n\"}, {\"index\": 2, \"text\": \"mline the AI lifecycle management of their AI scenarios SAP AI Launchpad. Benefit from uniform model status as well as uniform training and deployment, regardless of the underlying technology. As such, SAP AI Launchpad will not only increase transparency but will also enable to drive innovation by re-using AI content.\\n\\nMonitor and continuously improve model performance\\nCustomers benefit from simplified model retraining to continuously improve their model performance. They can also productize existing training models of supported AI runtimes or trigger jobs and deploy models directly using SAP AI Launchpad. The different resource groups (tenant) are displayed in SAP AI Launchpad.\\n\\nAI Core Configuration\\nWatch the following video to learn about AI Core Configuration.\\n\\nResource Groups\\nThe SAP tenant concept provides a separation of data and workspaces of customers in an SAP system.\"}, {\"index\": 3, \"text\": \"synchronizing your content.\\n\\nManaging Your Git Repository | SAP Help Portal\\n\\nManage Resource Groups.\\n\\nA resource group represents a unique workspace environment where users can create or add entities such as configurations, executions, deployments, and artifacts.\\n\\nManaging Resource Groups | SAP Help Portal\\n\"}, {\"index\": 4, \"text\": \"e group.\\n\\nNote\\n\\nAlthough the segregation of assets using Res.grps is valid, the important point here is to provide different datasets that are injected into the same training pipeline in different runs, however, resulting in different models.\\n\\nThis can be done periodically, or by shuffling one large dataset into different variants to get the best generalizing model.\\n\\nAI Core: Create a Configuration\\nAs mentioned before, you can use SAP AI Core to design their own training pipeline and specify it into a training template, which is used to define an executable object in SAP AI Core. For the sake of convenience, it falls under the umbrella of a specific scenario that acts as an additional namespace and groups all the executables together that you need to solve your specific business challenge. The executables, together with the input dataset, are the key components that you need to bind together to create a training configuration and then to train an ML model in SAP AI Core.\"}, {\"index\": 5, \"text\": \"this lesson, you will be able to deploy an ML model in SAP AI Core and use the deployment URL for inferencing\\nModel Deployment in SAP AI Core\\nAfter a model is trained, that is, it has learned (hidden) patterns from the provided dataset, the model needs to be deployed. By deploying the model, one can send new data to the model and get a \\\"prediction\\\" for the given data record. This is also called Model Serving or Inferencing.\\n\\nA Kubernetes cluster can be configured to provide both CPU (cheap) and GPU containers for Model Serving.\\n\\nIn addition, a high number of requests can be sent to the Model Server at the same time. In order to process these \\\"inference\\\" requests in a timely fashion, Kubernetes allows to scale the Model Server \\\"on demand\\\".\\n\\nHere we have 2 cases:\\n\"}, {\"index\": 6, \"text\": \"application, and so on.\\n\\nBuild a House Price Predictor with SAP AI Core\\nSteps\\nBuild a House Price Predictor with SAP AI Core | Tutorials for SAP Developers\\n\\n..........................\\n\\n2. Navigating Large Language Models fundamentals and techniques for your use case\\n\"}, {\"index\": 7, \"text\": \"Dependency: Excessive reliance on LLMs can make a business vulnerable if the system stops working or delivers inaccurate results.\\nScalability: LLM can handle increasing amounts of work and interactions due to its deep learning capabilities.\\tTechnical Complexity: Implementing, fine-tuning, and maintaining LLMs requires technical expertise and resources. Mismanagement could lead to unexpected issues and costs.\\nFurther Reading\\nAI Ethics at SAP\\nSAP believes that artificial intelligence (AI) has the potential to unlock all kinds of opportunities for businesses, governments, and society. However, AI also has the potential to create economic, political, and social challenges. Many people are concerned about the impact of AI on their lives as well as the associated uncertainty and risks. SAP acknowledges AI’s profound impact on decisions, fairness, transparency, privacy, and human dignity. For this reason, the development, deployment, use, and sale of AI systems at SAP is governed by a clear, ethical set of rules that takes into account technological advancements and the evolving regulatory landscape.\\n\\nIn this free online course, you’ll learn the importance and principles of AI ethics, how SAP set up an ethics governance framework, and which rules SAP employees need to follow during the development, deployment, use, and sale of AI systems.\\n\\n​SAP Generative AI Cybersecurity Strategy​​ Learn about Generative AI (GenAI) and how it offers tremendous potential to improve the way the world conducts business, but also introduces new and complex security challenges, and how SAP is addressing both its use and its challenges.\\n\"}, {\"index\": 8, \"text\": \"be integrated directly into applications, products, or services. Businesses can directly input their unique data into the AI model and receive tailored responses or forecasts, without having to learn how to design and train an AI model.\\n\\nYou can see models and scenarios supported in the generative AI hub here Generative AI Hub | SAP Help Portal\\n\\nSAP Business AI Approach\\nSee the video to understand how SAP Business AI works to make your business applications more intelligent.\\n\\n\\n\\nStarting From Ideation to Productization\"}, {\"index\": 9, \"text\": \"viewing SAP's AI strategy, accessing learning resources, and experimenting with the technology. For more information, see https://www.sap.com/products/artificial-intelligence.html.\\nCustomer Collaboration: Engaging with customers to explore potential use cases is key. This step is essential for Product Management & Product Owners who need to understand the customer's needs to ideate effectively. For more information, see Design-Led Development Process.\\n2. Validation Phase\\nThe validation phase assesses the feasibility, desirability, and viability of implementing LLMs. This involves determining whether the problems at hand are suitable for LLMs, based on their capability to understand or generate natural language. There are clear \\\"exclusion criteria\\\" for tasks not suitable for LLMs, such as calculations, and compares the efficacy of LLMs against traditional machine learning techniques for certain problems. Cost-benefit analysis and customer data access are crucial factors in this phase, with specific emphasis on compliance with data protection policies when using third-party LLMs.\\n\\nIn the Validation phase, the aim is to refine the ideas into viable use cases and assess their feasibility:\\n\\nUse Case Iteration: This includes clearly defining the business problem and solution, and iterating on the value proposition through customer feedback sessions. Ensure that you follow ethical and data protections principles for your use case (mandatory for use cases with personal and/or personally identifiable information (PII) data).\\nTechnical Evaluation: Data Engineers & Software Architects ensure access to data for evaluation, test various models, and follow architectural guidelines defined for their use case.\\nCommercialization Evaluation: Product Management & Product Owners should review pricing and commercialization and conduct cost-benefit analysis to understand the financial viability.\"}, {\"index\": 10, \"text\": \"duct is continuously refined based on real-world usage data and customer feedback, ensuring that it remains effective and relevant.\\nThis structured approach ensures that each phase of product development is thorough, collaborative, and geared toward creating robust, market-ready generative AI products that adhere to SAP's standards for software development and operations life cycle.\\n\\nIntegrating LLMs into Business Applications\\nObjective\\n\\nAfter completing this lesson, you will be able to integrate LLMs into business applications\\nIntegration of LLMs into Business Applications\\nIntegrating Large Language Models (LLMs) into business applications using SAP’s methodical approach involves several steps. Some of these steps can be:\\n\"}, {\"index\": 11, \"text\": \"LLM Use Cases\\nObjective\\n\\nAfter completing this lesson, you will be able to identify the appropriate optimization technique for different LLM use cases\\nReasons Why LLMs Can Underperform on Your Use Case\\nLarge Language Models (LLMs) can have limitations and it's advised that you explore advanced techniques to maximize the performance of LLMs for your specific use case.\\n\\nIn this unit, you will identify the challenges for maximizing LLM performance, and then learn about practical strategies that drive the efficiency and effectiveness of these AI models.\\n\\nChallenges of Maximizing LLM Performance on Use Cases\"}, {\"index\": 12, \"text\": \"nation?\\\"\\n\\nPrompt 2: \\\"What happens during the seedling stage?\\\"\\n\\nPrompt 3: \\\"How does the plant develop leaves?\\\"\\n\\nPrompt 4: \\\"What is the role of sunlight in plant growth?\\\"\\n\\nThis CoT approach ensures that a complex process is broken down into easy-to-understand parts, making it simpler to grasp the overall concept.\\n\"}, {\"index\": 13, \"text\": \"or your use case. RAG or Retrieval Augmented Generation can help in improving these responses, making them more precise and reliable for your use case.\\n\\nSee the video to identify the RAG architecture.\\n\\nSAP HANA Vector Engine\\nTo handle complex and unstructured vector data efficiently in enterprise environments, you can use SAP HANA vector engine.\\n\\nAn SAP HANA vector engine is a component of the SAP HANA database designed to handle complex and unstructured vector data, such as embeddings used in machine learning and AI applications. It allows for the storage, analysis, and processing of vector data, enabling the development of intelligent data applications and adding more context in scenarios involving generative AI.\\n\\nThe HANA vector engine is optimized for executing vectorized operations on large datasets, allowing for efficient parallel processing.\"}, {\"index\": 14, \"text\": \"I with intelligent data apps and insights.\\n\\nSAP HANA Cloud vector engine guide is available at SAP HANA Cloud, SAP HANA Database Vector Engine Guide.\\n\\nIntroducing Fine-Tuning\\nObjective\\n\\nAfter completing this lesson, you will be able to identify the need for fine-tuning\\nFine-Tuning Large Language Models (LLMs)\\nIn the optimization journey, you can fine-tune Large Language Models (LLMs) to improve their performance for your use case.\"}, {\"index\": 15, \"text\": \"anage LLM interactions, and interpret LLM output. There are various tools available for LLMs, ranging from simple text editors to complex programming frameworks. The tools used depend on the specific needs of the user. For example, a developer who uses an LLM to generate code might use a text editor to create prompts and a debugging tool to inspect LLM output.\\n\\nScenario Using the Agent, Functions, and Tool Interaction\\nConsider a scenario where a user wants to generate a marketing report. They can use an agent to create a prompt for the LLM that specifies the desired content and format for the report. The agent can then use a function to access a company's data warehouse and retrieve relevant data for the report. Finally, the LLM can generate the report based on the data and the user's prompt.\\n\\nIn this example, the agent acts as a bridge between the user and the LLM, ensuring that the user's request is understood and the LLM's output is relevant and actionable. The function provides the LLM with access to the necessary data, while the LLM does the heavy lifting of processing the data and generating the report. By effectively using agents, functions, and tools, users can streamline their LLM workflows, improve the quality of their results, and gain a deeper understanding of the capabilities of these powerful tools.\\n\\nEvaluating and Testing Your LLM Use Case\\nObjective\\n\"}, {\"index\": 16, \"text\": \"nderstudy for Gisting Evaluation (ROUGE):\\nROUGE evaluates the quality of text summarization systems. It measures the overlap between system-generated summaries and reference summaries.\\nClassification Accuracy:\\nThis metric measures the proportion of correctly predicted instances out of the total. For example, if an LLM classifies customer reviews as positive or negative sentiment, accuracy tells us how often it predicts correctly.\\nPrecision and Recall:\\nThese metrics are essential for binary classification tasks.\\nPrecision quantifies how many of the predicted positive instances are positive. It helps avoid false positives.\\nRecall (also known as sensitivity) measures how many actual positive instances were correctly predicted. It helps avoid false negatives.\\nF1 Score:\\nOften used for text classification tasks, the F1 score balances precision (correctly predicted positive instances) and recall (actual positive instances).\"}, {\"index\": 17, \"text\": \"e on domain-specific data: Pretrain LLMs further on relevant industry or company data like customer support tickets, legal contracts, and others. This enhances accuracy on business terminology.\\n\\nIncrementally update training data: Add new product specifications, policy documents, and so on, to continually fine-tune LLMs to maintain performance as new data comes in.\\n\\nTest with production workloads: Evaluate LLMs with real customer queries, transactions, and others instead of synthetic data to measure production readiness.\\n\\nSet rigorous quality metrics: Define quantitative KPIs like accuracy, latency, explanation capability to benchmark model performance.\\n\\nMonitor and address errors: Log model failures during inference to identify areas of improvement and feedback into the next training iteration.\\n\"}, {\"index\": 18, \"text\": \"is is invaluable for business-critical AI where continuity in performance rigor is essential.\\n\\nSecure Your LLM Applications\\nYou also need to evaluate the safety and security of your LLM applications and protect them against potential risks. Monitor and enhance security measures over time to safeguard your LLM applications. Detect and prevent critical security threats like hallucinations, jailbreaks, and data leakage. Explore real-world scenarios to prepare for potential risks and vulnerabilities.\\n\\nSummary\\nThe video summarizes the model testing and evaluation.\\n\\nFurther Reading\\nThis blog post discusses a use case involving multiple AI agents in the context of SAP: Multi AI Agents use case. SAP Maintenance Notification creation.\"}, {\"index\": 19, \"text\": \"n models. SAP embeds AI Foundation across its portfolio.\\n\\nThe following video describes AI Foundation on SAP BTP.\\n\\nGenerative AI Hub\\nNeed for Generative AI Hub\\nThe growing landscape of LLMs necessitates a generative AI hub that will help to facilitate systematic and tool-supported model selection tailored to diverse use cases. Generative AI hub, part of SAP AI Core and SAP AI Launchpad on SAP Business Technology Platform, consolidates trusted LLM and foundation model access, grounded on business and context data, and LLM exploration into a single entity. This streamlines innovation, ensures compliance, and offers versatility, benefiting both SAP's internal needs and its broader ecosystem of partners and customers.\\n\\nGenerative AI Hub in SAP AI Core\\nThe generative AI hub, within SAP AI Core, is crucial in boosting business AI capabilities, acting as the main hub for including generative AI into AI tasks in both SAP AI Core and SAP AI Launchpad.\"}, {\"index\": 20, \"text\": \"ion: AI orchestration refers to the process of coordinating and managing the deployment, integration, and interaction of various AI components within a system or workflow. This includes orchestrating the execution of multiple AI models, managing data flow, and optimizing the utilization of computational resources.\\n\\nAI orchestration aims to streamline and automate the end-to-end life cycle of AI applications. It ensures the efficient collaboration of different AI models, services, and infrastructure components, leading to improved overall performance, scalability, and responsiveness of AI systems.\\n\\nCoordinating and managing AI compute workflows and scheduling, content moderation, data masking, agent deployment, grounding capabilities, and inference engines to ensure seamless and efficient operation of AI systems.\\n\\nGovernance: Implementing policies and procedures to manage the development, deployment, and operation of AI systems in compliance with regulatory and organizational standards. This includes logs for tracking and auditing purposes, metering, monitoring, multi-tenancy, CaaS flow, as well as roles and responsibilities.\\n\\nAdaptability (Adaptation): With custom AI model it's crucial to constantly adapt, whether you need new better models, different model architecture, or simply exchange the underlying dataset.\\n\"}, {\"index\": 21, \"text\": \"into an application:\\n\\nSet up a BTP global account:\\n\\nYou need an access to a global BTP account.\\nSee the following tutorial here for detailed process: Setup your Global Account of your SAP BTP Enterprise Account\\nProvision SAP AI Core from the SAP BTP cockpit:\\n\\nProvision SAP AI Core from the SAP BTP cockpit within the SAP Business Technology Platform. This process generates a service key, which includes the necessary URLs and credentials to access your SAP AI Core instance.\\nSee the following documentation for this process: Intial setup for SAP AI core\"}, {\"index\": 22, \"text\": \"ion models (FMs) in generative AI hub. You can start experimenting with deployed models in SAP AI Launchpad. However, these FMs are not limited to just human to human interaction, they have a wider use case. Let’s explore that in this lesson.\\n\\nGenerative AI, particularly FMs , has opened a wide range of potential applications that go beyond the traditional chatbot paradigm. While many people still associate generative AI with applications that follow a specific schema – human input, AI processing, and output for human consumption – the possibilities extend far beyond this.\\n\\nOne exciting application of generative AI is its ability to produce outputs solely based on software, without requiring human instruction. For instance, it can automatically generate reports, create explanations, or translate complex information into easily understandable formats for human readers. This opens opportunities for automating various tasks and making information more accessible.\\n\\nOn the other hand, generative AI can also be used to interpret human instructions and control software systems without necessarily producing output for human consumption. This allows for the development of intelligent interfaces and control systems that can understand and execute human commands effectively.\\n\\nFurthermore, generative AI can be employed in complex applications where both input and output are handled by software, without any direct human interaction. These LLM-based applications have the potential to automate intricate processes, optimize systems, and solve problems in ways that were previously unimaginable.\\n\"}, {\"index\": 23, \"text\": \"oduct information and generate consistent, detailed, and engaging product descriptions that can be used across various sales channels.\\n\\nValue Proposition for generative AI hub in this Problem: The generative AI hub provides the company with access to advanced AI models that can understand the context and nuances of their products. By generating high-quality content, the company can ensure brand consistency, improve customer satisfaction, and potentially increase sales due to clearer communication. Moreover, the automation of this process frees up valuable time for employees to focus on more strategic tasks.\\n\\nIn this learning journey, we will take a scenario and then provide a detailed solution of a business problem in the next lesson and units.\\n\\nDeveloping Basic Prompts for Common Queries\\nObjectives\\n\\nAfter completing this lesson, you will be able to:\"}, {\"index\": 24, \"text\": \"Python\\n\\nCopy code\\n\\nSwitch to dark mode\\nGiving the following message:\\n---\\nSubject: Urgent HVAC System Repair Needed\\n\\nDear Support Team,\"}, {\"index\": 25, \"text\": \"iding for the past few years. I have always appreciated the meticulous care and attention your team provides in maintaining our facilities.\\n\\nHowever, I am currently facing a pressing issue with the HVAC system in my apartment. Over the past few days, the system has been malfunctioning, resulting in inconsistent temperatures and, at times, complete shutdowns. Given the current weather conditions, this has become quite unbearable and is affecting my daily routine significantly.\\n\\nI have attempted to troubleshoot the problem by resetting the system and checking the thermostat settings, but these efforts have not yielded any improvement. The situation seems to be beyond my control and requires professional intervention.\\n\\nI kindly request that a repair team be dispatched immediately to address this urgent issue. The urgency of the matter cannot be overstated, as it is impacting not only my comfort but also my ability to carry out daily activities effectively.\\n\\nThank you for your prompt attention to this matter. I look forward to your swift response and resolution.\\n\"}, {\"index\": 26, \"text\": \"rry out daily activities effectively.\\n\\nThank you for your prompt attention to this matter. I look forward to your swift response and resolution.\\n\\nBest regards,\\n[Sender]\\n---\\nExtract and return a json with the following keys and values:\\n- \\\"urgency\\\" as one of `low`, `medium`, `high`\\n- \\\"sentiment\\\" as one of `positive`, `neutral`, `negative`\"}, {\"index\": 27, \"text\": \"g that can be read directly and only contain the keys mentioned in the list above. Never enclose it in ```json...```, no newlines, no unnecessary whitespaces.\\nThis prompt gives clear instruction to provide a clean format without any quotes or whitespaces.\\n\\nThe following screenshot shows version 4 of the prompt:\\n\\nA Prompt Editor interface from a Generative AI tool. The interface includes sections for the prompt, response, and metadata. The prompt instructs the AI to extract and return a JSON object with keys urgency and sentiment based on specified values. The response box contains the JSON output: {urgency:high,sentiment:neutral}. The model used is meta-llama3-70b-instruct. There are also options to save, create new, and select prompts, as well as a section for metadata tags and notes.\\nYou can see that the JSON output is now clear. This is ready for software consumption.\\n\\n5. Simple Categories Based on Business Functions\\nWe have associated urgency and sentiment to a mail. However, we also need more tags for each message to categorize them for business needs. We can start with a simple prompt:\"}, {\"index\": 28, \"text\": \"ues like urgency. Let’s align them to values defined in the company. Aligning the output to business needs is a key step in addressing a business problem.\\n\\nPython\\n\\nCopy code\\n\\nSwitch to dark mode\\nGiving the following message:\\n---\\nSubject: Urgent HVAC System Repair Needed\"}, {\"index\": 29, \"text\": \", and I am reaching out to you from [Residential Complex Name], where I have been residing for the past few years. I have always appreciated the meticulous care and attention your team provides in maintaining our facilities.\\n\\nHowever, I am currently facing a pressing issue with the HVAC system in my apartment. Over the past few days, the system has been malfunctioning, resulting in inconsistent temperatures and, at times, complete shutdowns. Given the current weather conditions, this has become quite unbearable and is affecting my daily routine significantly.\\n\\nI have attempted to troubleshoot the problem by resetting the system and checking the thermostat settings, but these efforts have not yielded any improvement. The situation seems to be beyond my control and requires professional intervention.\\n\\nI kindly request that a repair team be dispatched immediately to address this urgent issue. The urgency of the matter cannot be overstated, as it is impacting not only my comfort but also my ability to carry out daily activities effectively.\\n\\nThank you for your prompt attention to this matter. I look forward to your swift response and resolution.\\n\"}, {\"index\": 30, \"text\": \"lex Name], where I have been residing for the past few years. I have always appreciated the meticulous care and attention your team provides in maintaining our facilities.\\n\\nHowever, I am currently facing a pressing issue with the HVAC system in my apartment. Over the past few days, the system has been malfunctioning, resulting in inconsistent temperatures and, at times, complete shutdowns. Given the current weather conditions, this has become quite unbearable and is affecting my daily routine significantly.\\n\\nI have attempted to troubleshoot the problem by resetting the system and checking the thermostat settings, but these efforts have not yielded any improvement. The situation seems to be beyond my control and requires professional intervention.\\n\\nI kindly request that a repair team be dispatched immediately to address this urgent issue. The urgency of the matter cannot be overstated, as it is impacting not only my comfort but also my ability to carry out daily activities effectively.\\n\\nThank you for your prompt attention to this matter. I look forward to your swift response and resolution.\\n\"}, {\"index\": 31, \"text\": \"ves\\n\\nAfter completing this lesson, you will be able to:\\nIdentify the need of using SDKs for LLMs\\nDescribe generative AI hub SDK\\nSoftware Development Kits (SDKs) for LLMs\\nAn SDK is a collection of tools, libraries, and documentation that developers use to create software applications for specific platforms or frameworks. SDKs typically include APIs, sample code, and other resources to help developers build and integrate their own applications with the platform or framework.\\n\\nBusiness Problem\\nConsider the problem that we were solving using the generative AI hub.\"}, {\"index\": 32, \"text\": \"ve properly deployed, configured, and provisioned generative AI hub in SAP BTP.\\n\\nHere are some key points:\\n\\nInstallation: You can install the SDK using the following pip command:\\n\\nPython\\n\\nCopy code\\n\"}, {\"index\": 33, \"text\": \"n-40b-instruct\\\" model from the OpenAI library. It asks for a short response about the answer to the ultimate question of life and sets parameters for response length and randomness. Finally, it prints the generated completion.\\n\\nChat Completion:\\n\\nPython\\n\\nCopy code\\n\\nSwitch to dark mode\\nfrom gen_ai_hub.proxy.native.openai import chat\"}, {\"index\": 34, \"text\": \"RAG) .\\nYou need a service for coordinating and managing the deployment, integration, and interaction of various AI components.\\n\\nIn the Facility solutions company scenario in this learning journey, you've seen that you need to manually update the model each time for each prompt. You'll see that even in generative-AI-hub-SDK code, you need to write different functions for each model.\\n\\nThis can be an erroneous and time-consuming process leading to complex and redundant code and workflows.\\n\\nThis is where orchestration services can be helpful.\\n\\nAI orchestration is the process of coordinating and managing how various AI components are deployed, integrated, and interact within a system or workflow.\"}, {\"index\": 35, \"text\": \"tion response.\\nYou'll see an implementation in the next lesson to use multiple models in a seamless manner.\\n\\nUsing Orchestration Service\\nYou need to perform the following steps to use Orchestration service:\\n\\nGet an Auth Token for Orchestration\\nCreate a Deployment for Orchestration\\nConsume Orchestration\\nYou can refer to the detailed steps here.\"}, {\"index\": 36, \"text\": \"code\\n\\nSwitch to dark mode\\nfrom gen_ai_hub.orchestration.service import OrchestrationService\\n\\norchestration_service = OrchestrationService(api_url=YOUR_API_URL, config=config)\\nresult = orchestration_service.run(template_values=[\\n    TemplateValue(name=\\\"text\\\", value=\\\"The Orchestration Service is working!\\\")\\n])\\nprint(result.orchestration_result.choices[0].message.content)\"}, {\"index\": 37, \"text\": \"deployment and either retrieves it or creates a new one, coordinating configurations and ensuring smooth execution within a specified timeout. Refer to the detailed code in the repository here.and the relevant readme file.\\n\\nIn case of any errors or issues while executing the code, you may refer some of the troubleshooting tips.\\n\\nThe following function sets up and sends requests to an AI orchestration service.\\n\\nPython\\n\\nCopy code\\n\"}, {\"index\": 38, \"text\": \"-\\n{{?input}}\\n---\\nYour task is to extract\\n- urgency\\n- sentiment\\n\\\"\\\"\\\"\\n\\nf_1 = partial(send_request, prompt=prompt_1)\\n\"}, {\"index\": 39, \"text\": \"tral.\\nYou get a response. You can see that the urgency and sentiment of the mail is generated.\\n\\nYou can see that the values for urgency and sentiment are based on assigned values.\\n\\nGenerate JSON output: A software requires input in a specific format, and JSON is chosen for this purpose. JSON is a suitable choice because it's a lightweight, language-independent, and easy-to-parse format that enables efficient data exchange and simplifies development.\\nWe use the following code:\\n\\nPython\\n\"}, {\"index\": 40, \"text\": \"hem for business needs. We can start with a simple code.\\nPython\\n\\nCopy code\\n\\nSwitch to dark mode\\nprompt_5 = \\\"\\\"\\\"Giving the following message:\\n---\\n{{?input}}\\n---\"}, {\"index\": 41, \"text\": \"uses a predefined function, \\\"send_request\\\", with the constructed prompt and some options to generate the necessary response from the given input message. This ensures consistency and accuracy in data extraction.\\n\\nThe following screenshot shows the output of this code.\\n\\nA prompt asking to extract support categories from a message about an urgent HVAC repair. The response is a JSON object with categories: emergency_repair_services, facility_management_issues, and routine_maintenance_requests.\\nYou can see categories in JSON output that can be processed in a software.\\n\\nCombining all the steps: We have used generative-ai-hub-sdk to arrive at proper values of urgency, sentiment, and categories in JSON format step-by-step. Now, let's combine all these steps into a single prompt.\\nPython\\n\"}, {\"index\": 42, \"text\": \"cations that will directly impact customer experience.\\n\\nYou need automated and consistent evaluation of prompts across various scenarios, ensuring reliability and efficiency.\\n\\nFor this you need to create custom evaluation functions using generative-ai-hub-sdk.\\n\\nUsing an SDK for evaluating prompts offers the following benefits:\\n\\nReliable Testing: SDKs automate testing of prompts across various scenarios, ensuring consistent and efficient results.\\nMeasuring Performance: They provide objective metrics such as relevance, coherence, and fluency to quantitatively assess response quality.\"}, {\"index\": 43, \"text\": \"This code evaluates the predictions made by a function processing an email message. It uses a provided extraction function to analyze the email's content and compares the results against predefined ground truth data, checking for valid JSON, correct categories, sentiment, and urgency. This ensures that the extraction function performs accurately and consistently.\\n\\nThe last sentence evaluates the combined prompt function that we developed in the previous lesson.\\n\\nYou can get the following outputs:\\n\\nA prompt asking for a JSON response based on an urgent HVAC repair request email. The response JSON includes urgency, sentiment, and categories. The correct response is also shown.\\nYou can see that the evaluation shows that all predictions are correct except sentiment.\\n\\nWe'll implement an evaluation function for a large number of mails to support large-scale evaluations, making it easier to test prompts on extensive datasets.\"}, {\"index\": 44, \"text\": \"ficient processing and clear presentation of results.\\n\\nImplement the final function to the final combined function.\\nPython\\n\\nCopy code\\n\\nSwitch to dark mode\\noverall_result[\\\"basic--llama3-70b\\\"] = evalulation_full_dataset(test_set_small, f_8)\\npretty_print_table(overall_result)\"}, {\"index\": 45, \"text\": \"ith minimal examples and allows the model to produce relevant responses based on the provided example.\\n\\nFew-Shot Prompting\\nFew-shot prompting builds on one-shot prompting by providing multiple examples to guide the model. This approach is particularly effective for complex tasks, as it helps the model understand nuances and generate more accurate responses.\\n\\nMetaprompting\\nMetaprompting involves crafting or refining prompts to effectively guide AI model responses. This technique uses a prompt to generate or complete subsequent prompts, streamlining the process. For instance, we employ a metaprompt to produce a guide for classification tasks based on an example. The goal is to let the LLM instruct itself and condense the information from multiple examples into concise instructions.\\n\\nOften, combining multiple techniques can also lead to better results.\\n\"}, {\"index\": 46, \"text\": \"ed using \\\"random.seed(42)\\\" to ensure that the random sampling of the examples is reproducible. This helps in maintaining consistency in experiments and evaluations.\\nSampling Examples: The variable \\\"k\\\" is set to 3, indicating the number of examples to sample from the \\\"dev_set\\\" dataset. The \\\"random.sample(dev_set, k)\\\" function selects three random examples from the development set.\\nFormatting Examples: The selected examples are formatted into a template \\\"example_template\\\". Each example includes the input message and the expected output in JSON format. This formatted string is then joined using \\\"\\\\n---\\\\n\\\" to create a cohesive set of examples.\\nPartial Function Application: The \\\"partial\\\" function is used to bind the generated prompt and examples to the \\\"send_request\\\" function, creating a function \\\"f_10\\\" that can be called with just the input message. This streamlines the process of sending requests to the model with the necessary context.\\nSending Request and Evaluating: The script sends the request using \\\"f_10(input=mail[\\\"message\\\"])\\\" with the input message from \\\"mail[\\\"message\\\"]\\\". The result is stored and evaluated against a small test dataset \\\"test_set_small\\\". The evaluation results are stored in \\\"overall_result[\\\"few_shot--llama3-70b\\\"]\\\".\\nOutput Display: Finally, the \\\"pretty_print_table(overall_result)\\\" function is used to display the evaluation results in a formatted table, making it easier to interpret the results.\\nYou can get the following output prompts:\\n\\nA prompt for extracting and categorizing messages, along with an example message and its categorized output. The example message is an inquiry about training programs for an in-house maintenance team. The output categorizes the message under general_inquiries and training_and_support_requests, with a neutral sentiment and low urgency.\\nYou can see an example prompt here:\"}, {\"index\": 47, \"text\": \"rompt_get_guide = \\\"\\\"\\\"Here are some example:\\n---\\n{{?examples}}\\n---\\nUse the examples above to come up with a guide on how to distinguish between {{?options}} {{?key}}.\\nUse the following format:\\n```\\n### **<category 1>**\\n- <instruction 1>\\n- <instruction 2>\"}, {\"index\": 48, \"text\": \"e prints the guide for urgency.\\n\\nYou can see the following output:\\n\\nContains guidelines for determining the urgency of issues, categorized into High Urgency, Medium Urgency, and Low Urgency, with specific criteria for each level.\\nYou can see the guide describing three rules for each urgency category that can be used in a prompt.\\n\\nWe use the following code to utilize these guides in a prompt.\\n\\nPython\"}, {\"index\": 49, \"text\": \"json string that can be read directly and only contain the keys mentioned in the list above. Never enclose it in ```json...```, no newlines, no unnecessary whitespaces.\\n\\\"\\\"\\\"\\n\\nf_13 = partial(send_request, prompt=prompt_13, **option_lists, few_shot_examples=examples, **guides)\\n\\nresponse = f_13(input=mail[\\\"message\\\"])\\nThis Python code creates a template prompt for a message classification task, specifying how to extract and return information about urgency, sentiment, and support categories in a JSON format. The code uses this prompt to configure a function, \\\"f_13\\\", to analyze a given input message and generate a structured JSON response. This ensures consistent and accurate message classification.\\n\\nYou can see that it's combining few examples with guides generate during metaprompting.\\n\"}, {\"index\": 50, \"text\": \"at can be used in software.\\n\\nWe used advanced prompting techniques to arrive at better prompts. We evaluated techniques and their combinations to analyze the results.\\n\\nLet's now evaluate the results for different models.\\n\\nHere are some reasons to use different models to solve a business problem:\\n\\nSpecialization: Different LLMs are often trained on varied datasets and optimized for specific tasks. For example, some models excel at text generation, while others specialize in image or speech recognition.\\nPerformance: Combining multiple LLMs can enhance overall performance. One model handles natural language understanding, while another generates high-quality responses, creating a more effective team.\"}, {\"index\": 51, \"text\": \"AI hub. We also saw how orchestration services and workflow can help in configuring different models using minimum code.\\n\\nLet's summarize these benefits here:\\n\\nYou can configure models using the Configuration tab in SAP AI Launchpad. You can refer to the configuration steps in Unit 1. Some configured models are shown in the following screenshot. SAP AI Launchpad interface, specifically the Configurations section under ML Operations. It lists various AI models, their versions, and sources like azure-openai and gcp-vertexai.\\nYou can see various foundation models from many vendors. You can also see various scenarios, such as aicore-opensource, gcp-vertexai, and azure-openai. You will learn more about scenarios and models in the next topic.\\n\\nYou can deploy models using the Deployments tab in SAP AI Launchpad. You can refer to the configuration steps in Unit 1 Some deployed models are shown in the following screenshot.SAP AI Launchpad interface showing the All Deployments section. Multiple AI models are listed, all in the RUNNING state, with details such as names, IDs, and timestamps.\\nYou have also seen how to use orchestration to use a common code for different models. Refer to \\\"send_request\\\" function details in the helper functions in Unit 2 Lesson 3.\\nGenerative AI Hub Models Usage\"}, {\"index\": 52, \"text\": \"fferent models. In the next lesson, we'll evaluate the results from different models.\\n\\nSelecting the Suitable LLM\\nObjective\\n\\nAfter completing this lesson, you will be able to evaluate different models using generative-ai-hub-sdk\\nDifferent Models in generative-AI-hub Code\\nLet's now evaluate different models for Facility solutions problem that we're solving.\\n\\nmistralai--mixtral-8x7b-instruct-v01\"}, {\"index\": 53, \"text\": \"N, with varying other scores.\\nYou can see the evaluation results.\\n\\ngemini-1.5-flash\\nWe perform similar steps with gemini-1.5-flash. This model is the cheapest and fastest Google model available on generative AI hub.\\n\\nPython\\n\\nCopy code\\n\"}, {\"index\": 54, \"text\": \"rtant in dynamic environments where AI capability demands fluctuate.\\nCompetitive Advantage: Strategic pricing can provide a competitive edge. Companies adopting innovative pricing strategies, such as outcome-based pricing, can attract more customers and drive higher adoption rates.\\nBusinesses can make use of these considerations and take informed decisions about which generative AI models to deploy, achieving the best balance between cost and performance.\\n\\nEvaluation Summary\\nWe saw how we can use generative AI hub to solve a business problem and learned about features and options that generative AI hub offers to develop, deploy, and manage custom-built AI solutions.\\n\\nLet's add to the recap what we have done to solve the business problem so far:\\n\\nWe created a basic prompt in SAP AI Launchpad using an open-source model.\"}, {\"index\": 55, \"text\": \"businesses to use AI for complex problem solving.\\n\\nLeveraging the Power of LLMs Using SDK for Generative AI Hub\\nWe learned to develop basic prompts for common queries and leveraged Large Language Models (LLMs) using the Software Development Kit (SDK). We explored the orchestration service, which manages complex AI workflows, including a harmonized API and streamlined code for accessing models. Evaluating prompts ensured their accuracy and relevance, especially for larger datasets.\\n\\nRefining AI Responses Using Advanced Prompt Engineering Techniques\\nWe delved into advanced prompt engineering techniques like few-shot prompting and metaprompting. These techniques improve the specificity, accuracy, and user experience of AI models by providing multiple examples and refining prompts.\\n\\nSelecting Large Language Models in Generative AI Hub\\nWe explored different LLMs available in the generative AI hub. We emphasized the importance of selecting the right model for specific tasks, balancing accuracy, performance, and cost-effectiveness.\"}, {\"index\": 56, \"text\": \"his involves implementing evaluation functions and using practical examples to assess the performance of prompts. Effective evaluation helps businesses refine their AI solutions and achieve better results.\\nAdvanced Prompt Engineering: Advanced prompt engineering techniques, such as few-shot prompting and metaprompting, are essential for improving the specificity, accuracy, and user experience of AI models. These techniques involve providing multiple examples to guide the model and crafting or refining prompts to generate more accurate and contextually relevant responses.\\nSelecting Models: Selecting the right LLM is critical for achieving the desired performance and cost-effectiveness. This involves evaluating different models available in the generative AI hub, comparing their performance, and considering factors like specialization, flexibility, and reliability. Choosing the right model helps businesses tailor their AI solutions to specific needs and achieve optimal results.\\nThis comprehensive recap provided a solid foundation for understanding advanced techniques like grounding, preparing us to explore the complexities of advanced AI applications.\\n\\nExplore Models in Generative AI Hub\\nThe Model Library in the Generative AI Hub is an invaluable resource for selecting the right model. It provides comprehensive information on available models to aid in decision-making.\\n\\nCatalog Mode: Explore all available models and their metadata. Use filters to refine your selection or search for a model by name. Each model's card offers detailed information, including data input types, cost, and metrics.\\n\"}, {\"index\": 57, \"text\": \"domain knowledge without the need of retraining.\\nThis lesson is a recap of the foundational concepts of AI and the generative AI hub. This knowledge is essential as we move forward in developing and refining advanced AI techniques for various business applications.\\n\\nIdentifying Business Scenarios for Advanced AI Techniques\\nObjective\\n\\nAfter completing this lesson, you will be able to analyze and select business problems that can benefit from complex AI solutions.\\nIntroduction\\nIn this lesson, we will explore sophisticated AI methods to tackle business challenges.\\n\"}, {\"index\": 58, \"text\": \"rational efficiency and customer service.\\nImproving Financial Analysis:\\nFinance:\\nProblem: Limited access to diverse AI models restricts the ability to analyze financial data effectively.\\nSolution: Instant access to various LLMs allows financial institutions to choose the best model for each task, speeding up AI development and improving financial analysis and decision-making.\\nStreamlining Logistics Planning:\\nLogistics:\\nProblem: Complex AI solution development processes can delay logistics planning and execution.\\nSolution: Streamlined workflows and simple SDKs facilitate faster AI solution development, reducing time-to-market and enhancing logistics planning and execution.\\nValue of SAP's Generative AI Hub for Implementing Advanced AI Techniques\"}, {\"index\": 59, \"text\": \"s and simple SDKs in preferred programming languages, facilitating faster and easier AI solution development.\\nThe value of the generative AI hub lies in its ability to simplify and enhance the use of these advanced AI techniques through several key features:\\n\\nThe Orchestration Service: The orchestration service streamlines the deployment and management of AI models by requiring only a single virtual deployment for access to all leading models. It reduces the complexity and effort needed to manage multiple models individually.\\nLow Code Approach: The hub supports a low code approach, making it accessible to users without deep technical expertise. This approach allows users to interact with AI core features more easily, facilitating broader adoption, and utilization of AI capabilities.\\nAdvanced Techniques: The grounding module, an advanced AI technique, is packaged in an easy-to-use manner through the orchestration service. This enables users to apply complex AI functionalities without needing extensive technical knowledge or without writing huge code.\\nIdentify the Facility Management Scenario\\nFacility Solutions Company is a premier provider of comprehensive facility management, maintenance, and cleaning services tailored for both residential and commercial properties. Their mission is to ensure that your environment remains safe, efficient, and impeccably maintained, allowing you to focus on your core activities without the hassle of facility upkeep.\\n\\nTheir target markets are luxury residential complexes, apartments, and individual homes. They also serve commercial properties like office buildings, retail spaces, industrial facilities, and educational institutions.\"}, {\"index\": 60, \"text\": \"nce, when a customer reports a maintenance issue, the company must ground the details of the issue in their internal systems to ensure it is addressed promptly. By using document grounding techniques in the generative AI hub, they can achieve this by grounding the extracted information in SAP HANA vector databases.\\n\\nAlso, understanding the context and semantics of customer e-mails is crucial for making informed decisions. For instance, when a customer expresses dissatisfaction with a service, the company must understand the context to address the issue effectively. By using embedding models from the generative AI hub, the company can enhance the categorization and prioritization process. These models help in understanding the context and semantics of the e-mails, enabling the company to provide timely and appropriate responses to customer requests and complaints.\\n\\nFinally, Facility management needs to ensure that the solutions using the generative AI hub follow ethical practices for deploying AI-driven solutions. The data stored for best decision-making must be anonymized, and filtered for harmful content, which ensures that sensitive information is masked, and inappropriate content is filtered out. This can lead to complex workflows. By using the orchestration service in the generative AI hub, the company can integrate diverse AI components, ensure data privacy, and improve accuracy through document grounding, and manage AI-driven business processes.\\n\\nBy addressing these challenges, Facility Solutions Company aims to further streamline their e-mail categorization and prioritization process, reduce manual effort, and improve the overall efficiency and accuracy of their facility management services.\\n\\nWe will see how the generative AI hub can implement these solutions in the next few units.\\n\"}, {\"index\": 61, \"text\": \"esson, we will explore the need for the orchestration service, and how this service enhances AI workflows within the generative AI hub, ensuring efficiency, accuracy, and security.\\n\\nNeed for Using the Orchestration Service in Generative AI Hub\\nThe orchestration service is crucial for solving business problems, especially in the context of generative AI, as it provides a structured and efficient way to manage and integrate various AI components. Here are some key points highlighting the need for the orchestration service:\\n\\nCoordination and Management\\nProblem: Coordinating and managing the deployment, integration, and interaction of multiple AI models and components within a system can be complex and inefficient.\\nSolution: The orchestration service helps in coordinating and managing these processes, ensuring that different AI models work together seamlessly, optimizing overall performance and efficiency.\\nStreamlining and Automating Processes\\nProblem: Manual interventions in the end-to-end lifecycle of AI applications can be time-consuming and prone to errors.\"}, {\"index\": 62, \"text\": \"resource utilization, enabling robust and scalable AI-driven solutions for complex business problems.\\n\\nIn the previous learning journey, Using generative-AI-hub-SDK to interact with the orchestration service we explored the essential role of the orchestration service within the SAP generative AI hub.\\n\\nNow, we will explore the modules of orchestration and their implementation with an example.\\n\\nUse the Orchestration Service for Enhancing Workflows\\nThe orchestration service acts as the backbone of AI workflows, managing the flow of data and tasks between different AI components. In the context of the generative AI hub, the orchestration service provides a structured and efficient way to integrate and manage AI capabilities.\\n\\nIt discusses enhancing AI workflows using orchestration services. Key elements include AI workflow enhancement, key features like module integration and data privacy, ensuring accuracy and context via document grounding and contextual integration, and development simplification through uniform APIs and workflow efficiency. Each section is visually represented by icons.\"}, {\"index\": 63, \"text\": \"aging multiple APIs.\\n\\nStreamlined Development: Developers can use the harmonized API to create and deploy AI workflows more efficiently. This streamlined approach enables faster time-to-market for AI solutions, driving business innovation.\\n\\nApplications and Benefits\\nThe following video summarizes key features of the orchestration service in the generative AI hub.\\n\\nWe have seen the implementation of harmonized code in the previous learning journey. We used a common code to access various models.\\n\\nIn coming lessons and units, we will implement other features such as data masking, grounding, and so on.\"}, {\"index\": 64, \"text\": \"tion workflow to upload or download your own orchestration workflow in JSON format through the Workflow/JSON button. From there, you can upload your edited JSON file (max file limit: 200KB) or download the JSON file of the workflow orchestration with the Upload and Download buttons respectively.\\n\\nA dashboard interface showing an Orchestration workflow configuration script in JSON format with edit, upload, and download options. The sidebar lists different workspace modules and settings.\\nAn example of an orchestration workflow in JSON format is provided next.\\n\\nJSON\\n\\nCopy code\\n\\nSwitch to dark mode\"}, {\"index\": 65, \"text\": \"mode\\npip install \\\"generative-ai-hub-sdk[google, amazon]\\\"\\nKey updates: Enhanced model access, prompt registry templates, streaming support, debug logging, dependency updates, new model support, API logging.\\n\\nThese updates enhance the functionality and usability of the generative AI hub SDK, enabling learners to implement advanced AI techniques more effectively. For detailed release notes and further information, refer to the official documentation.\\n\\nFor details, use the following references:\\n\\ngenerative-ai-hub-sdk - PyPI\\nRelease Notes | Generative AI Hub SDK v4.3.1 - SAP Online Help\"}, {\"index\": 66, \"text\": \"ut aspects are not available. This means that while the orchestration service can facilitate seamless integration and management of different LLMs, it does not yet provide the capability to call specific functions within those models directly. For instance, you cannot directly instruct the model to execute certain tasks based on structured commands or parameters. This limitation requires developers to implement additional layers or alternative methods to achieve function-specific outputs. However, ongoing developments and updates may soon introduce this feature, enhancing the overall versatility and functionality of the orchestration framework.\\nOpen-Source Framework Support: While we support, some open frameworks, such as LangChain, not all open-source frameworks are supported. Extending support or exploring alternative methods may be required.\\nConclusion\\nIncorporating the orchestration service into applications is essential for streamlining AI functionalities.\\n\\nYou saw how we can use the orchestration service to create streamline flows with ease.\\n\\nBy following best practices, organizations can focus on innovation, ensure data accuracy and security, and build powerful AI solutions efficiently.\\n\\nUnderstanding Document Grounding in Generative AI Hub\"}, {\"index\": 67, \"text\": \"ions, and automated decision-making.\\n\\nSAP HANA Vector Engine\\nThe SAP HANA Vector Engine is a feature of SAP HANA Cloud that enables the storage, processing, and analysis of high-dimensional vectors, such as text embeddings, alongside other types of business data. This engine is a part of SAP HANA Cloud's multimodel processing capabilities, which allow for the integration of various data types, including relational, graph, spatial, and document data.\\n\\nKey functionalities of the SAP HANA Vector Engine include:\\n\\nStoring and managing vector embeddings, which are numerical representations of objects like text, images, or audio.\\nEnabling advanced applications such as semantic search and RAG, which combines LLMs with private business data.\\nSupporting efficient vector searches using SQL, with functions like L2DISTANCE() and COSINE_SIMILARITY(). L2DISTANCE() and COSINE_SIMILARITY() are vector functions available in the SAP HANA Vector Engine.\"}, {\"index\": 68, \"text\": \"nd the SAP HANA Cloud vector engine to enhance the contextual relevance and accuracy of AI responses. The system comprises several key components:\\n\\nDocument Store\\nHouses various document types (for example, PDF, text files) which serve as the knowledge base.\\n\\nGenerative AI Hub\\nThis central hub manages the processing and retrieval of information. It consists of three main stages:\\n\\nData Ingestion: Incoming documents undergo preprocessing, chunking (breaking into smaller segments), and embedding (converting text into numerical vectors).\\nOrchestration: This stage handles the overall workflow, including the grounding process and access to Large Language Models (LLMs).\"}, {\"index\": 69, \"text\": \"tive AI hub and incorporates vector stores, such as the managed SAP HANA database.\\n\\nScreenshot of SAP AI Launchpad showing Orchestration page. The Build Orchestration Workflow progress bar highlights Grounding, displaying input and output variables, and selected sources settings.\\nThe generative AI hub supports document grounding through several key features and processes:\\n\\nAccess to LLMs: The generative AI hub provides instant access to a range of LLMs from different providers, such as GPT-4 by Azure OpenAI and OpenSource meta-lama. This broad access allows the orchestration of multiple models to enhance AI capabilities.\\nIntegration with SAP AI Launchpad: Users can execute prompts in the AI Launchpad, showcasing how generative AI can assist with business processes. The SAP AI Core infrastructure supports text template generation based on applications while ensuring data security.\\nImproved Context and Accuracy: By grounding AI responses in customers' specific documents (like HR policies), the generative AI hub enhances the context and accuracy of generated content. It minimizes the chance of hallucinations and reduces ambiguity.\\nDocument Indexing: Unstructured and semi-structured data from documents are preprocessed, split into chunks, and converted into embeddings using LLMs. These embeddings are stored in the SAP HANA Vector Database for efficient querying, which aids in grounding AI responses in real, relevant data.\\nThese features ensure that generative AI can provide reliable, transparent, and contextually accurate responses by using the customer's own document repositories.\"}, {\"index\": 70, \"text\": \"or additional information, see the Vector API.\\nGrounding Module Options\\nChoose one of the following options to use grounding:\\n\\nOption 1: Upload Documents to Supported Data Repository and Run Data Pipeline\\nThe pipeline collects documents and segments the data into chunks.\\nIt generates embeddings, which are multidimensional representations of textual information, and stores them efficiently in a vector database.\\nThe process involves the following steps:\\n\\nPerform Initial One-Time Administrative Steps: Create a resource group and a generic secret for grounding. For more information, see:\"}, {\"index\": 71, \"text\": \"ice = OrchestrationService(api_url=orchestration_service_url)\\nllm = LLM(\\n    name=\\\"gpt-4o\\\",\\n    parameters={\\n        'temperature': 0.0,\\n    }\\n)\\ntemplate = Template(\\n            messages=[\\n                SystemMessage(\\\"\\\"\\\"Facility Solutions Company provides services to luxury residential complexes, apartments,\"}, {\"index\": 72, \"text\": \"n code sends a request to an orchestration service to run a specific configuration. It includes a template value with a user query asking to list customer-reported issues. After running the service, it prints the response from the orchestration result, specifically the message content of the first choice. This helps automate and fetch data on customer issues efficiently.\\n\\nYou get the following output:\\n\\nList of customer-reported issues including noise, landscaping, window cleaning, AC malfunction, janitorial, elevator, heating, electrical, plumbing, water damage, roofing, pest control, and cleaning.\\n\\nYou can see that the list of issues reported by customer, which is grounded in mails that customers provided.\\n\\nYou can see the documents that were retrieved for the context from the data repository.\\n\"}, {\"index\": 73, \"text\": \"structured data, which are numerical representations of objects, such as text, images, or audio, in high-dimensional vectors. These embeddings are used for semantic data retrieval, enabling advanced text search and similarity queries.\\n\\nDocument grounding ensures that AI responses are contextually relevant and accurate by using real-time data from your own knowledge sources. This minimizes the risk of misinformation and enhances decision-making processes by providing precise and reliable insights.\\n\\nDocument grounding in generative AI hub leverages embedding models and the SAP HANA Cloud vector engine to enhance the contextual relevance and accuracy of AI responses.\\n\\nThe generative AI hub serves as an abstraction layer to access a wide range of LLMs from various providers.\\n\\nBy mastering these techniques, you are now equipped to implement AI solutions that drive operational efficiency, reduce costs, and enhance customer experiences. It enables you to tackle business challenges with innovative AI-driven approaches, ultimately leading to improved productivity and strategic growth.\"}]\n\nReturn ONLY the JSON object described by the system message (no extra text)."}], "text": {"format": {"type": "json_object"}, "verbosity": "low"}, "max_output_tokens": 6000, "reasoning": {"effort": "minimal"}}}
